Aktualisiert am: 2025-11-05 (spät abends)

Kurzfassung (Status & To-dos)
- Status: PDF→Bild-Rendering, Bildvorverarbeitung, minimale Orchestrierung und Dev-Hook nach Submission sind implementiert und durch Tests abgedeckt. DB-Repo unterstützt jetzt den Zwischenstatus `extracted` inklusive Speicherung der `page_keys` in `analysis_json`.
- Worker-Integration (fertiger Minimalpfad): Worker akzeptiert Submissions mit Status `pending` und `extracted`, ruft Vision und Feedback auf, markiert bei Erfolg `completed` und entfernt den Job. Fehlerpfade (Vision/Feedback transient → Retry; Vision/Feedback permanent → `failed`) sind durch Tests abgedeckt.
- Noch offen: Vision-Auswertung (Ollama) + Feedback-Persistenz, SSR-Einbindung (Verlauf mit Thumbnails/Text) und eine produktionsreife Hintergrundausführung.
- PDF-Einreichung: Upload funktioniert. In Dev stößt der Hook die Verarbeitung an, wenn `STORAGE_VERIFY_ROOT` gesetzt ist; persistierte Seiten werden erfasst und die Submission als `extracted` markiert.

Implementierungsstand (TDD, minimaler Slice)
- PDF→Bilder Rendering (sequentiell, bounded memory): implementiert in `backend/vision/pdf_renderer.py:1` mit Defaults 300 DPI, Grayscale, Annotationen, `page_limit` und optionalem `preprocess`-Hook. Tests: `backend/tests/test_vision_pdf_pipeline.py:1` (Happy Path, page_limit, Fehlerfälle, Hook).
- Bildvorverarbeitung: implementiert in `backend/vision/image_preprocess.py:1` (Grayscale → optional Median-Denoise → Equalize → optional Otsu-Binarize). Tests: `backend/tests/test_vision_image_preprocess.py:1`.
- Orchestrierung: `process_pdf_bytes(pdf_bytes)` in `backend/vision/pipeline.py:1`, verbindet Renderer mit Vorverarbeitung (denoise+equalize an, binarize aus). Test: `backend/tests/test_vision_pipeline_usecase.py:1`.
- Vision (minimaler Adapter): `backend/vision/vision_adapter.py:1` definiert `VisionClient` und `extract_text_from_pages(...)` (Markdown mit `## Page N`). Test: `backend/tests/test_vision_adapter_integration.py:1`.
- Dev-only Verarbeitungs-Hook nach Submission: In `backend/web/routes/learning.py:~610` wird nach erfolgreicher PDF-Submission (202) bei gesetztem `STORAGE_VERIFY_ROOT` das PDF aus dem Dev-Verzeichnis gelesen und die Pipeline best-effort angestoßen. Neu: `_dev_try_process_pdf(...)` rendert und persistiert Seiten als PNG unter `submissions/.../derived/{submission}/page_0001.png` und markiert (dev) `extracted`, wenn ein Repo-Port mit `mark_extracted` vorhanden ist. Tests: `backend/tests/test_learning_pdf_processing_hook.py:1`, `backend/tests/test_learning_pdf_persist_in_dev.py:1`.

- Persistenz-Port (neu): `backend/storage/ports.py:1` (`BinaryWriteStorage.put_object(...)`).
- Seitenpersistenz: `backend/vision/persistence.py:1` (`persist_rendered_pages(...)`) speichert Pages und ruft `repo.mark_extracted(...)`). Test: `backend/tests/test_vision_persistence_storage.py:1`.
- Supabase-Adapter: `backend/teaching/storage_supabase.py:1` implementiert `put_object(...)` (Key-Normalisierung, Content-Type-Weitergabe). Tests: `backend/tests/test_supabase_storage_adapter.py:1`.

DB/Repo-Erweiterung (neu)
- Migration: `supabase/migrations/20251105190000_learning_submissions_add_extracted_status.sql` erweitert den `analysis_status`-Check um `extracted`.
- Repo: `backend/learning/repo_db.py:mark_extracted(...)` setzt `analysis_status='extracted'` und merged `{"page_keys": [...]}` in `analysis_json`.
- Tests: `backend/tests/test_learning_repo_mark_extracted.py:1` (DB-gestützt, prüft Statuswechsel und JSON-Inhalt).

Verifikation (gezielte Tests)
- `backend/tests/test_vision_pdf_pipeline.py` — Rendert Seiten, respektiert `page_limit`, behandelt Fehler, Hook wird aufgerufen.
- `backend/tests/test_vision_image_preprocess.py` — Denoise/Equalize/Binarize wirken wie erwartet.
- `backend/tests/test_vision_pipeline_usecase.py` — Orchestrierung übergibt `preprocess` korrekt an Renderer.
- `backend/tests/test_learning_pdf_processing_hook.py` — Dev-Hook wird nach PDF-Submission ausgelöst (mit gemockten externen Abhängigkeiten).
- `backend/tests/test_supabase_storage_adapter.py` — Presign/Head/Delete sowie `put_object`-Schreibpfad abgedeckt.
- `backend/tests/test_vision_adapter_integration.py` — Vision-Adapter wird pro Seite aufgerufen; Markdown-Text und Metadaten entstehen.
- `backend/tests/test_learning_worker_vision_integration.py` — Worker (ohne echte DB): Erfolgsfall mit Status `extracted` sowie Vision-Fehler (transient → Retry, permanent → failed).
- `backend/tests/test_learning_worker_jobs.py` — Worker (mit echter DB): End-to-end für `pending → completed`, Retry-Backoff und permanente Fehler in Vision/Feedback.

Schnellstart (lokal testen)
- `.venv/bin/pytest -q \
    backend/tests/test_vision_pdf_pipeline.py \
    backend/tests/test_vision_image_preprocess.py \
    backend/tests/test_vision_pipeline_usecase.py \
    backend/tests/test_vision_adapter_integration.py \
    backend/tests/test_vision_persistence_storage.py \
    backend/tests/test_learning_pdf_processing_hook.py \
    backend/tests/test_supabase_storage_adapter.py \
    backend/tests/test_learning_worker_vision_integration.py`

Bekannter Umfang, der noch fehlt
- Vision-Adapter (Ollama) in produktionsreif: echtes Client-Handling (Timeouts, Chunking), robuste Fehlerklassifikation; Feedback (DSPy) finalisieren.
- SSR/Fragments: Anzeige der neuen Artefakte (Vorschau, extrahierter Text) im Verlauf; UI-Tests dafür; Polling am Status `completed` sauber beenden.
- Supabase-Schreibpfad in produktiver Worker-Ausführung verifizieren (Content-Type, Key-Layout, Berechtigungen).
- Background-Ausführung/Infra: robuste Queue/Worker-Deployment-Strategie (Restart, Metriken, Alerts).

Nächste Schritte (konkret)
1) Persistenz (TDD) – abgeschlossen (erste Stufe):
   - Status: Port + Seitenpersistenz + Repo-Update + Tests umgesetzt; `extracted`-Status verfügbar.
2) Vision (TDD, nächste Inkremente):
   - Integration: Adapter in Worker-Flow einspeisen; Repo über SECURITY DEFINER-Helper oder Port `mark_completed(...)` aktualisieren (Status `completed`).
   - Tests: Erfolgs-/Fehlerpfade (inkl. Retry) im Worker; Persistenz von `text_md` und Analyse-Metadaten.
3) UI/SSR (TDD):
   - Tests: History-Fragment zeigt Extrakt/Thumbnails; kein Polling wenn `completed`.
   - Implementierung: Erweiterung des Fragments/Render-Hilfen.
4) Background-Ausführung (Prod):
   - Entwurf eines minimalen Queue/Worker-Interfaces; spätere Umsetzung in eigenem Prozess/Service.
5) Dokumentation:
   - Docstrings an Integrationspunkten (Berechtigungen/Verhalten), kurze Architektur-Notiz; Plan fortlaufend pflegen.

—

Empfohlene Bibliothek: Für das Rendern von PDFs in Bilder ist pypdfium2 (PDFium) die erste Wahl. Diese Python-Bindings nutzen Googles PDFium-Engine, welche liberal lizenziert (BSD-3-Klausel) ist und nicht dem strengen Copyleft unterliegt[1][2]. Pypdfium2 bietet vorgebaute Wheel-Pakete für gängige Plattformen, was die Installation in Docker stark vereinfacht[3][4]. Alternativen: PyMuPDF (mit MuPDF) ist technisch ebenfalls leistungsfähig, birgt aber eine AGPL-Lizenz[5], die bei nicht vollständig quelloffenen Projekten problematisch sein kann. Poppler-basierte Lösungen (z.B. pdf2image mit Poppler/GraphicsMagick) sind ebenfalls verbreitet, erfordern jedoch systemweite Abhängigkeiten (Poppler-Bibliotheken) und sind GPL-lizenziert. PDFium/pypdfium2 kombiniert hohe Zuverlässigkeit (PDFium ist die Rendering-Engine von Chrome, bewährt für Bilder, Annotationen und Formulare) mit unkomplizierter Installation und FOSS-Lizenzkonformität, daher der Primärvorschlag.
DPI/Skalierung: Eine Auflösung von 300 DPI pro Seite ist ein bewährter Standard, um Text klar lesbar zu machen und OCR/Visionsmodelle optimal zu versorgen[6]. 300dpi bietet in der Praxis einen guten Kompromiss zwischen Lesbarkeit und Dateigröße[7] – es ist branchenüblich, da niedrigere Auflösungen (z.B. 150–200 DPI) oft zu Fehlinterpretationen führen (B vs. 8 Verwechslungen, ausgelöschte Markierungen)[6]. In speziellen Fällen (sehr kleine Schrift, <8pt) kann sogar höher skaliert werden (400–600 DPI), jedoch steigt dann Speicher- und CPU-Bedarf unverhältnismäßig[8]. Als Default empfehlen wir 300 DPI. Falls Ressourcenengpässe auftreten (z.B. extrem viele Seiten), kann adaptiv auf ~200 DPI reduziert werden – aber grundsätzlich gilt: Lesbarkeit vor Dateigröße.
Große PDFs (viele Seiten): Bei umfangreichen Dokumenten sollte streamingweise pro Seite gearbeitet werden, um Speicher zu sparen. Die PDF-Bibliothek (pypdfium2) erlaubt das sequentielle Rendern jeder Seite; wir öffnen das PDF einmal und iterieren seitenweise, anstatt alles in den RAM zu laden. Nach jeder Seite werden temporäre Objekte sofort freigegeben (z.B. bitmap/Pixmap löschen oder per Kontextmanager arbeiten), damit der Speicherverbrauch konstant bleibt. Zur Sicherheit setzen wir ein Seitenlimit (z.B. 100 Seiten) und brechen mit Hinweis ab, falls überschritten – das schützt vor DoS durch gigantische Uploads. Für jeden Seiten-Render kann ein Timeout etabliert werden (z.B. 5s pro Seite), um Hänger bei korrupten PDFs zu vermeiden. Seitensortierung ist im Normalfall sequentiell wie im PDF-Index; wir behalten diese Reihenfolge bei, außer es gibt Metadaten (z.B. PageLabels) die eine alternative Sortierung nahelegen – in der Regel nicht nötig für Schüler-PDFs. Relevante Metadaten wie Seitenanzahl und ggf. Originalformat können wir im Output-Objekt vermerken. Annotationen/Formulare: PDFium rendert standardmäßig auch Annotationen und Formularfelder (ggf. mit Flag FPDF_ANNOT), sodass z.B. Lehrer-Markierungen oder ausgefüllte Felder im Bild erscheinen[9]. Das ist wichtig, da solche Informationen oft für die Bewertung relevant sind. Insgesamt gewährleistet die PDFium-Lösung Robustheit für vielfältige Schul-PDFs (eingescannte Arbeitsblätter, kommentierte PDFs, Formulare). Sollte PDFium in seltenen Fällen ein Dokument nicht laden können (korruptes PDF), kann als Fallback ein alternativer Renderer (z.B. Poppler via pdf2image) versucht werden – allerdings planen wir diese Komplexität erst bei Bedarf (YAGNI-Prinzip).
Konkrete Defaults: Primäre Library: pypdfium2 (PDFium). DPI: 300. Farbmodus: Grayscale (falls Library-API verfügbar, sonst nachträglich konvertieren). Render-Flags: inkl. Annotationen (z.B. PDFium FPDF_RENDER_FLAG.ANNOT). Seitenlimit: 100 (konfigurierbar). Timeout: 5s/Seite (als grober Startwert).
2. Bildvorverarbeitung
Pipeline-Überblick: Wir setzen auf eine minimale, risikoarme Vorverarbeitung in folgender Reihenfolge: Grayscale → optional Deskew → leichtes Denoising → Kontrastverstärkung (CLAHE) → sanfte Binarisierung. Diese Schritte sind konservativ gewählt, um die Lesbarkeit deutlich zu erhöhen, ohne Artefakte einzuführen.
    • Grayscale: Wir wandeln Eingabebilder konsequent in Graustufen um. Farbige Dokumente verlieren dabei zwar ihre Farbinformation, aber für Texterkennung ist Helligkeit/Kontrast entscheidend, nicht Farbe. Außerdem reduziert ein Kanal den Datenumfang erheblich. Sollte Farbinformation doch relevant sein (z.B. farbige Diagramme), könnten wir in Zukunft pro Seite heuristisch entscheiden – vorerst aber Standard: Graustufen.
    • Deskew (Entzerrung): Leicht schräg eingescanntes oder fotografiertes Dokumente können durch Rotation geraderichtet werden. Wir implementieren eine optionale Deskew-Heuristik: z.B. Rand- oder Textlinien per Hough-Transformation detektieren und den Bildwinkel korrigieren. Da viele Vision-Modelle moderate Rotation tolerieren[10], behandeln wir nur grobe Schräglagen (>2–3°) automatisch. Falls unsere Detektion unsicher ist (Gefahr von Überkorrektur), verzichten wir lieber auf Deskew – Korrektheit vor Perfektion. Für v1 kann Deskew auch komplett weggelassen werden, da es komplexer ist; dies wäre eine spätere Verbesserung, wenn nötig.
    • Denoising: Um kleine Scan-Artefakte (Staub, Punkte) zu entfernen, setzen wir ein mildes Rauschfilter ein. Bewährt ist ein Median-Blur mit kleinem Kernel (3x3), der kleine Punkte glättet, ohne Kanten stark anzutasten[11]. Alternativ oder ergänzend kann ein bilateraler Filter genutzt werden, der Rauschen entfernt und Kanten schont[11] – letzterer ist aber deutlich langsamer, daher zunächst Medianfilter. Wir wählen so schwache Parameter, dass Textdetails nicht verschwinden (z.B. Kernel 3).
    • Kontrastverstärkung (CLAHE): Der Kern unserer Pipeline ist Contrast Limited Adaptive Histogram Equalization. CLAHE erhöht lokal den Kontrast, damit Text gegenüber Hintergrund hervorgehoben wird[12]. Gerade ungleichmäßig beleuchtete Scans (Schatten, Flecken) profitieren enorm: Helle Bereiche werden nicht überbelichtet, dunkle werden aufgehellt, da CLAHE blockweise arbeitet und Verstärkung begrenzt (Clip Limit). Wir empfehlen OpenCV CLAHE mit Parametern: clipLimit ~2.0 (moderate Kontrastbegrenzung) und tileGridSize ~8x8 (Standardwert) als Startpunkt. Diese Werte funktionieren in vielen Fällen gut; feine Schrift könnte von etwas höherem ClipLimit (3.0) profitieren, aber zu hoch birgt Rauschen.
    • Binarisierung: Abschließend kann eine adaptive Schwellenwert-Binarisierung angewandt werden, um den Text in reinem Schwarz-Weiß zu erhalten[13][14]. Adaptive Verfahren (z.B. OpenCV adaptiveThreshold) analysieren lokale Nachbarschaften (wir z.B. Blockgröße ~25–51 px, Offset ~10–20)[15], um auch bei Hintergrundvariationen brauchbare Ergebnisse zu liefern. Wir wählen den cv2.ADAPTIVE_THRESH_MEAN_C oder Gauß-Variante und erzeugen ein binäres Bild (Pixel 0 oder 255)[13]. Sanft bedeutet: Wir wenden Thresholding nur an, wenn es einen klaren Vorteil bringt. Oft liefert schon das CLAHE-verbesserte Graustufenbild ausreichend Kontrast. Eine Heuristik: Wenn das Histogramm des CLAHE-Bildes immer noch viel Grautöne (zwischen 0 und 255) enthält oder Hintergrundschattierungen erkennbar sind, dann adaptive Binarisierung anwenden. Ist das Bild dagegen ohnehin sehr kontrastreich (z.B. bereits schwarz-weißer Scan), könnte Thresholding redundant oder sogar schädlich sein (Verlust von feinen grauen Details, z.B. Bleistift). Daher: Heuristik zum Auslassen: Wir messen z.B. den Anteil nahezu reiner Schwarz/Weiß-Pixel nach CLAHE – ist er hoch (d.h. Bild schon quasi binär), überspringen wir das Thresholding. Andernfalls führen wir es aus, um den letzten Rest an Rauschen zu eliminieren.
Robustheit für Varianten: Farb-Dokumente (etwa farbige Diagramme, Marker) werden in Graustufen umgewandelt – farbliche Hervorhebungen (z.B. Marker) erscheinen dann als Grauflächen. CLAHE plus evtl. Threshold kann solche Hintergründe oft entfernen oder abschwächen (z.B. gelbe Marker werden hellgrau und beim Threshold ggf. weiß)[16][17]. Das verbessert die Lesbarkeit des Textes darunter, birgt aber das Risiko, dass farbcodierte Informationen verloren gehen. Da im Bildungskontext Text im Vordergrund steht, akzeptieren wir das. Handschrift/Scribbles: Hellere Bleistift-Schrift profitiert stark von CLAHE (Kontrastgewinn). Allerdings kann striktes Binarisieren filigrane Bleistiftstriche entfernen, wenn der Schwellenwert unglücklich ist. Hier hilft das Adaptive-Verfahren, da es lokale Unterschiede berücksichtigt. Wir testen mit typischen Lehrer-Korrekturen in Rotstift und Schüler-Handschriften: Ziel ist, dass nichts Relevantes verschwindet. Notfalls wird Preprocessing für solche Seiten deaktiviert (Erkennung z.B. anhand von sehr niedrigem Kontrast – Indikator für evtl. Bleistift – um binarization zu skippen). Kleine Schriftgrößen: Hier ist vor allem die DPI entscheidend (300dpi oder mehr)[8]. Unsere Pipeline (ohne zu aggressive Filter) sollte kleine Buchstaben nicht verwischen. Wir vermeiden starke Weichzeichnung; stattdessen lieber leichtes Schärfen, falls nötig (derzeit nicht vorgesehen, aber z.B. ein unsharp mask könnte optional helfen, falls Texte sehr verwaschen sind).
Bibliotheken: Wir setzen OpenCV (cv2) für die Bildverarbeitung ein. OpenCV ist hochoptimiert in C/C++ und bietet alle benötigten Funktionen out-of-the-box (Median Blur, CLAHE, adaptiveThreshold, Rotation)[11][18]. Zudem ist OpenCV FOSS (Apache 2.0) und in Python einfach nutzbar. Pillow (PIL) bleibt im Einsatz für simple Operationen (Bild laden/speichern, evtl. Grayscale-Konvertierung), aber viele fortgeschrittene Filter (CLAHE, adaptives Threshold) gibt es dort nicht direkt. Pillow könnte via Plugins oder eigene Implementierung vieles lösen, ist aber tendenziell langsamer (Python-Schleifen) und weniger umfangreich. Daher: OpenCV als Haupt-Werkzeug, Pillow ergänzend (z.B. für initiales Lesen falls nötig, oder Endkonvertierung zu PNG/JPEG). Sollte OpenCV in Docker-Größe oder Installation ein Hindernis sein, könnten wir als Fallback auf reines Pillow + numpy ausweichen (z.B. Histogramm-Equalization mit numpy implementieren). Aber da OpenCV gängig ist, bleiben wir dabei.
Parameter-Defaults (vorläufig): Grayscale: via Pillow img.convert('L') oder OpenCV cv2.cvtColor. Deskew: aus (Standard), oder Max-Winkel 3° automatisiert. Median Blur: Kernel 3. CLAHE: clipLimit=2.0, tileGridSize=(8,8). Adaptive Threshold: Methode MEAN_C, Blockgröße 25 (angepasst je nach DPI, evtl. ~ window von ~5mm physikalisch), C=10 (subtrahierter Konstantwert zur Feinjustierung). Diese Defaults liefern in ~80% der Fälle bereits deutlich besseren Kontrast ohne aggressive Veränderungen – ausreichend als Grundeinstellung.
