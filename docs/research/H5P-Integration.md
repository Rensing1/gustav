Architekturvorschlag: H5P-Task in GUSTAV
Datenmodell und H5P-Referenzierung

Task-Typ und Referenz: Im Task-Datensatz wird ein neues Feld Task.kind eingeführt (Enumerationen native vs. h5p), um H5P-Aufgaben zu kennzeichnen. Für H5P-Aufgaben referenziert der Task eine H5P-Content-ID in der H5P-Content-Datenbank. Diese Verknüpfung erfolgt über eine separate Tabelle H5PContentReference oder zusätzliche Felder wie h5p_content_id (Primärschlüssel des Contents in der H5P-Storage) sowie Konfigurationsattribute: etwa library (Haupt-Content-Typ inkl. Version, z.B. H5P.InteractiveVideo 1.24), display_options (Flags für Anzeigeelemente wie Vollbild, Retry, Download), player_mode (Spielmodus: z.B. normal vs. preview für Lehrkräfte) und attempt_policy (Versuchslimit und -verhalten). Diese Felder erlauben eine klare Zuordnung des H5P-Inhalts zum GUSTAV-Task und definieren, wie der Player dargestellt und gesteuert wird.

Overlay-Strategie (GUSTAV vs. H5P-Felder): Wir lagern Aufgabenbeschreibung, Bewertungskriterien und Hinweise bewusst in den GUSTAV-Task aus (instruction_md, criteria, hints_md), während der H5P-Content primär die interaktive Komponente enthält. Instruktionen und Kontext stehen somit außerhalb des H5P, was Vorteile für Wiederverwendbarkeit und Lokalisierung bringt – z.B. kann derselbe H5P-Inhalt mit unterschiedlichen Aufgabenstellungen in verschiedenen Kursen genutzt oder relativ leicht übersetzt werden, ohne den H5P-Inhalt selbst zu duplizieren. H5P-interne Textfelder (Titel, Beschreibungen) sollten daher sparsam genutzt werden oder generisch bleiben, um Redundanzen zu vermeiden. So bleibt GUSTAV die führende Instanz für Lernziele und Kriterien, während H5P die Interaktivität liefert. Bei Änderungen oder Versionierung eines H5P-Inhalts kann durch diese Trennung nachvollzogen werden, welche Tasks betroffen sind, und Aktualisierungen lassen sich konsistent einspielen. Trade-off: Eine enge Kopplung (z.B. Instruktion innerhalb des H5P) könnte zwar in manchen Fällen bequem sein, erschwert aber Übersetzungen und globale Anpassungen. Die vorgeschlagene Overlay-Strategie stellt sicher, dass Plattform-Features wie Rubriken und KI-Feedback unabhängig vom H5P-Inhalt gepflegt werden können.

Speicherung und Struktur: Die H5P-Inhalte selbst werden self-hosted verwaltet, z.B. über @lumieducation/h5p-server. Dieser stellt die notwendigen Storage-Interfaces bereit, die wir auf unsere Infrastruktur anpassen. Ein H5P-Content besteht aus Library-Dateien (JS/CSS) und Content-spezifischen Assets (Bilder, Videos etc.) sowie State-Daten (User-Eingaben, Scores). Wir nutzen lokale Speicherressourcen (z.B. ein Datei-Volume oder einen S3-kompatiblen Storage via Supabase) für H5P-Library-Dateien und Content-Dateien, um DSGVO-konforme On-Premises-Datenhaltung sicherzustellen. Die lumieducation-Bibliothek bietet hierfür bereits Integrationen (u.a. h5p-mongos3 für MongoDB+S3, sowie Klassen für Dateispeicherung)
docs.lumi.education
docs.lumi.education
. In unserer Postgres-DB (Supabase) legen wir Metadaten zu den H5P-Inhalten ab: z.B. Zuordnung Content-ID ↔ Task, Versionen, Autor, Lizenz. So können wir per RLS steuern, wer welchen Content sieht/bearbeitet, während die schweren Binary-Assets im Filesystem/Objektstorage liegen.

H5PContentReference im Task: Dieses Objekt enthält u.a.: content_id (Primärschlüssel des H5P-Inhalts im H5P-System), main_library (z.B. H5P.DragAndDrop – Redundanz zur Content-Metadaten, optional für schnelle Filter), display_options (JSON mit Flags für Vollbild erlauben, Embed-Code anzeigen, Download-Button, Copyright-Button etc.), player_mode (z.B. normal vs. review – letzterer könnte alle Lösungen anzeigen für Lehrer-Vorschau) und attempt_policy. Attempt Policy spezifiziert, wie die Versuche gehandhabt werden: z.B. max_attempts (Übergang aus GUSTAV-Feld), Reset-Verhalten (Neustart vs. Fortsetzen) und Score-Aggregation (ob bei mehreren Versuchen der beste, letzte oder Durchschnitt zählt für die Bewertung). Diese Policy kann später erweitert werden, um differenzierte Logik abzubilden (z.B. Differentgewichtung pro Versuch). Für den MVP ist max_attempts jedoch der Kern: erlaubt oder verhindert den Retry-Button und steuert die Sperre nach Ausschöpfen der Versuche.

Metadaten-Abbildung: Felder wie due_at (Fälligkeitsdatum) und max_attempts werden im GUSTAV-Task gesetzt und über die attempt_policy an den H5P-Player vermittelt (siehe unten zur Versuchssperre). H5P selbst kennt diese Konzepte nicht global, daher übernimmt GUSTAV die Kontrolle. Das Fälligkeitsdatum fließt in die UI (z.B. Countdown oder Sperr-Hinweis) und in die Logik: nach due_at kein Start neuer Versuche mehr. Die H5P-Content-Metadaten (Titel, Lizenz, Autoren) werden in der GUSTAV-UI ebenfalls angezeigt, um Transparenz über Urheberrechte zu gewährleisten (H5P liefert Lizenzinformationen je Inhalt, die wir auslesen können).

Versionierung und Wiederverwendung: Da H5P-Inhalte eigenständige Objekte sind, können sie theoretisch von mehreren Tasks referenziert werden (z.B. identisches Quiz in zwei Kursen). In der Anfangsphase empfehlen wir 1 Task = 1 H5P-Content, um Komplexität zu minimieren. Später kann ein H5P-Inhalt durch Versionierung (H5P hält pro Content-Typ und Content-ID die Library-Version vor) aktualisiert oder geklont werden, falls er an verschiedenen Stellen genutzt wird. Updates der H5P-Libraries (Content-Typen) erfolgen unabhängig vom Task-Datenmodell – siehe Betrieb unten.
Einbettung und UI-Flow

Einbettungsstrategie: Wir favorisieren die Integration per Web Component gegenüber einem isolierten iFrame. Die lumieducation-Bibliothek bietet dazu <h5p-player> und <h5p-editor> Web Components, die die komplette H5P-Player/Editor-Funktionalität kapseln
docs.lumi.education
docs.lumi.education
. Diese können in unsere bestehenden HTMX/JS-Seiten direkt eingebunden werden, ohne ein Framework: z.B. <h5p-player content-id="42"></h5p-player>. Im Hintergrund lädt die Komponente alle benötigten JS/CSS des H5P-Core und der Content Libraries und rendert den Player innerhalb der Seite. Vorteile sind eine nahtlose Integration ins DOM, gleiche Origin (damit einfacher Zugriff auf Events und CSS) und geringerer Overhead bei Größenanpassung (keine PostMessage/Resizer-Skripte nötig). Zudem können wir die vom Web Component erzeugten Events direkt abfangen – insb. jedes xAPI-Event der H5P-Inhalte wird als DOM-Event xAPI am <h5p-player> Element ausgegeben
docs.lumi.education
. Dies erleichtert das Sammeln von Telemetrie enorm, da wir in unserem Frontend-JS einen Event-Listener registrieren können, um Interaktionsdaten direkt zu verarbeiten oder ans Backend (Supabase) zu senden.

Sicherheit und iFrame-Überlegung: Ein iFrame (mit sandbox und strikter CSP) würde zwar den H5P-Inhalt isolieren und potenziell Angriffsfläche reduzieren, bringt aber Nachteile: Events aus dem iFrame abzufangen erfordert PostMessage-Brücken, Fokussteuerung ist komplexer und responsive Höhe erfordert zusätzliche Scripts (H5P’s embed code nutzt z.B. h5p-resizer.js). Da wir die H5P-Core-Skripte selbst hosten und per Web Component einbinden, behalten wir die Kontrolle über CSP: wir können per CSP erlauben, dass H5P-Inline-Skripte und Styles (die wir aus dem H5P-Core laden) ausgeführt werden, aber externe Domains blockieren. Wichtig: H5P-Inhalte sind im Normalfall statisch und führen nur die mitgelieferten Libraries aus; dank unserem Upload-Scanner und SVG-Sanitizer (siehe unten) wird eingeschleuster JS-Code verhindert. Unter diesen Bedingungen ist die Web-Component-Lösung mit gleicher Origin vertretbar. Sollte sich eine Content-Type als unsicher erweisen, könnten wir für diese notfalls eine iFrame-Sandbox erzwingen. Insgesamt bietet die direkte Einbettung ein besseres User Experience: Interaktionen fügen sich ins Seitenlayout ein, und Assistive Technologien (Screenreader) können den H5P-Inhalt als Teil der Seite wahrnehmen statt in einem separaten Frame.

Responsivität & A11y im Player: H5P-Player sind responsiv gestaltet – etwa passt sich ein Interactive Video oder Course Presentation der verfügbaren Breite an. Mit Web Components verbleibt diese Responsivität erhalten; bei iFrames müsste man den Resizer korrekt konfigurieren. Die Tastatursteuerung und Fokus-Reihenfolge wird mit Web Components ebenfalls browser-nativ gehandhabt, d.h. der Tab-Fluss von Instruktion → H5P-Inhalt → nachfolgende Elemente bleibt natürlich. (Im iFrame-Szenario würde der Fokus in den Frame springen – auch lösbar, aber mit Web Components eleganter.) Sollten dennoch A11y-Probleme auftreten (z.B. bei komplexen Inhaltstypen), können wir gezielt nachbessern oder im Zweifel doch isolieren. Generell hat H5P für viele Inhaltstypen WCAG-Compliance, die in unserer Integration nicht verloren gehen darf.

UX-Sequenz: Der Ablauf für Lernende gliedert sich in drei Phasen: Vorbereitung, Interaktion, Reflexion. In der Vorbereitung sieht der Schüler zunächst die Instruktion (instruction_md) des Tasks – prominent oberhalb des H5P-Players. Hier werden Lernziel, Kontext und Arbeitsanweisung vermittelt, quasi als Advance Organizer. Ggf. werden hier auch Hinweise zur Bedienung gegeben, z.B. „Sie können das Quiz max. 2 Mal versuchen, nutzen Sie die Hinweise sparsam.“. Anschließend startet die Phase Interaktion mit dem eingebetteten H5P-Player, der die eigentliche Aufgabe darstellt (z.B. Quiz, Simulation, Video). Während der Bearbeitung kann der Nutzer bei Bedarf Hinweise (hints_md) abrufen. Diese werden nicht alle sofort gezeigt, sondern schrittweise freigeschaltet: etwa ein Button „Tipp anzeigen“ enthüllt den ersten Hinweis, ein weiterer Klick den nächsten, etc. (sofern vorhanden). So bleibt die Selbständigkeit gewahrt; nur bei Bedarf greift der Schüler auf die gestuften Hilfen zu. Technisch implementieren wir das als dynamische Einblendung unter dem Player: Die hints_md werden in Markdown gerendert, aber initial ausgeblendet. Jeder Hinweis kann eine neue Facette des Lösungswegs beleuchten – vom kleinsten Denkanstoß bis hin zur fast vollständigen Lösung beim letzten Hinweis.

Nach Abschluss der H5P-Aufgabe (z.B. nach Klick auf „Finish“ oder automatischem Ende) folgt die Reflexion: Wir zeigen die Bewertungskriterien (criteria) an, idealerweise zusammen mit dem vom System ermittelten Score und evtl. automatischem Feedback. Die Kriterien werden als Checkliste oder Rubrik dargestellt, sodass der Schüler seine Leistung daran messen kann. Bei einfachen Auto-Tasks (z.B. Multiple-Choice-Quiz) könnte dies bedeuten: „Kriterium: 80%+ der Fragen korrekt = Ziel erreicht“ – das System markiert entsprechend. Bei komplexeren Aufgaben (Essay, Branching Scenario) dienen die Kriterien der formativen Rückmeldung: Der Schüler sieht, auf welche Aspekte der Lösung die Lehrkraft achten wird (z.B. „Vollständigkeit der Argumente“, „Korrektheit der Fakten“). In diesen Fällen kann die Plattform zunächst einen groben automatischen Abgleich bieten (z.B. KI-Feedback zum Essay, siehe unten), die finale Bewertung nach Kriterien erfolgt aber manuell durch die Lehrkraft.

Hinweis- und Kriterien-UI: Die Hinweise und Kriterien sind stets zugänglich, jedoch zeitlich passend integriert: Hinweise vorzugsweise während der Bearbeitung (on-demand), Kriterien danach. UX-Detail: Soll ein Schüler vor Start schon die Kriterien sehen? Einerseits fördern transparente Bewertungsmaßstäbe die Zielorientierung, andererseits könnten sie bei Quiz-Aufgaben ablenken. Wir empfehlen, Kriterien standardmäßig eingeklappt anzuzeigen („Woran wird deine Lösung gemessen?“), damit interessierte Schüler diese vorab lesen können, ohne den Flow zu stören. Nach dem Attempt klappen wir die Rubrik automatisch aus und – falls der Task automatisch ausgewertet wurde – markieren vorläufig erfüllte/nicht erfüllte Kriterien (z.B. anhand des Scores, falls heuristisch zuordenbar). Bei manueller Bewertung bleibt es ein reiner Auszug der Maßstäbe.
Versuchskontrolle und Bewertung

Versuchszählung und Reset/Resume: GUSTAV steuert die Anzahl und Handhabung der Versuche zentral, um max_attempts und due_at zuverlässig durchzusetzen. Dank der lumieducation-Implementierung können wir mehrere User-States pro H5P-Content verwalten, indem wir einen contextId an den Player übergeben
docs.lumi.education
docs.lumi.education
. Konkret erstellen wir für jeden neuen Versuch einen eindeutigen contextId (z.B. die ID eines Attempt-DB-Eintrags), den wir beim Laden an H5PPlayer.render übergeben. Dadurch separiert das H5P-System die Interaktionsdaten je Versuch. Der erste Aufruf eines Tasks erzeugt Attempt #1 mit contextId, den wir speichern; solange dieser Versuch läuft (nicht abgeschlossen), bleibt Resume möglich – H5P speichert periodisch den Zustand (via Content User Data) und wir laden diesen beim nächsten Öffnen automatisch
docs.lumi.education
docs.lumi.education
. Wenn der Schüler bewusst neu beginnen möchte (z.B. er bricht ab, bevor er fertig ist), bieten wir eine „Von vorn beginnen“-Funktion, die denselben Attempt resettet – dies entspricht in H5P dem Löschen des contentUserData-States für diesen contextId. Standardmäßig wird jedoch der laufende Versuch fortgesetzt, um unbeabsichtigten Fortschrittsverlust zu vermeiden.

Ist ein Versuch abgeschlossen (H5P feuert ein completed-Event, siehe unten), wertet GUSTAV diesen aus und speichert Ergebnis + Timestamp. Hat der Schüler noch Versuche frei (attempts_used < max_attempts), wird ein neuer Attempt-Datensatz angelegt und ein neuer contextId generiert – der H5P-Player lädt somit beim nächsten Start frisch, ohne Vorgaben der vorherigen Runde. (Das H5P-ContentUserData-Feature erlaubt parallel mehrere states zu halten, wir müssen nur dafür sorgen, den gewünschten contextId jeweils zu verwenden
docs.lumi.education
.) Retry-Buttons: Viele H5P-Inhalte haben intern eine „Retry“/„Retry Quiz“-Funktion nach Abschluss. Um die Plattform-Logik zu kontrollieren, werden wir diese Schaltflächen ggf. unterbinden/hiden, zumindest wenn max_attempts überschritten wäre. Stattdessen leitet GUSTAV den Schüler über einen eigenen „Neuer Versuch“-Button in einen frischen Kontext. Dafür können wir z.B. die H5P Display Options nutzen: in den meisten Quiz-Content-Typen lässt sich der Retry-Button per Option deaktivieren. Alternativ, falls ein Content-Type das nicht vorsieht, könnten wir via H5P-API den Retry-Button aus dem DOM entfernen oder ein CSS-Hide setzen, sobald attempt_policy es verlangt.

Sperren nach Frist: Nach Ablauf von due_at wird keine neue Attempt-Erstellung mehr erlaubt. Praktisch heißt das: Ist das Fälligkeitsdatum überschritten und der Student hat noch nicht abgeschlossen, kann er zwar den aktuellen Versuch ggf. beenden, aber kein „Neustart“ mehr initiieren. Die UI zeigt „Aufgabe abgelaufen“ an und der Player wird in einen read-only Modus versetzt (H5P selbst hat keinen read-only Mode, aber wir können das Rendering unterbinden, bevor der Nutzer interagieren könnte). Evtl. lassen wir den Content noch ansehen (Review der eigenen Antworten), aber ohne Möglichkeit, Eingaben zu ändern. Dieses Verhalten wird über das Backend enforced: bei Ladevorgang prüft die API if now > due_at and attempts_used >= max_attempts_allowed or now > due_at and last_attempt_completed == false – je nachdem entweder Fehler „Frist verpasst“ oder in einem speziellen „viewOnly=true“-Modus laden (ggf. via Player-Parameter, soweit vorhanden, oder indem wir alle Interaktionselemente disabled schalten via CSS/JS).

Bewertung und Score-Mapping: Die meisten H5P-Content-Typen liefern am Ende einen Score (Punkte erzielt / Maximalpunkte). Über H5P’s Completion-Tracking bzw. xAPI erhalten wir diese Daten. Lumieducation unterscheidet hier: der H5P-Client kann via Completion Ajax die Summenpunkte an den Server melden
docs.lumi.education
docs.lumi.education
. Wir aktivieren diese Funktion (H5PConfig.setFinishedEnabled=true und entsprechendes Endpoint-Routing) sodass bei jedem Task-Abschluss Dauer, Score und Maxscore erfasst werden
docs.lumi.education
. Zusätzlich horchen wir auf das generische xAPI-Event, wodurch wir auch detailliertere Daten (Antworten, Teilpunkte pro Frage etc.) erhalten
h5p.org
. Für die primäre Bewertung des Task-Versuchs verwenden wir den absoluten Score-Prozentsatz: score_percent = (achievedScore / maxScore) * 100. Dieses fließt – je nach Aufgabenart – entweder direkt in die Bewertung ein oder wird als Orientierung für Lehrer genutzt. Beispielsweise kann ein Quiz-Task automatisch „bestanden“ markieren, wenn ≥ 60% erreicht wurden, analog zu den Kriterien.

Mapping zu criteria: Bei rein summativ bewerteten Aufgaben (z.B. Question Set mit x Punkten) wird es oft nur ein Bewertungskriterium geben („Score in %“). In solchen Fällen kann man einen Automapping vornehmen: Erfüllt bei 80%+ = Kriterium „Thema verstanden“ erfüllt, etc. Bei komplexeren Kriterien (z.B. Rubriken mit mehreren Dimensionen) ist die automatische Zuordnung schwieriger. Hier dient der H5P-Score eher als Hinweis. Heuristik: Man könnte z.B. Fragen im H5P thematisch den Kriterien zuordnen (etwa indem in der Task-Definition hinterlegt wird: Frage 1–3 → Kriterium A, Frage 4–5 → Kriterium B). Daraus könnte GUSTAV bei Auswertung ableiten: Kriterium A zu x% erfüllt, Kriterium B zu y%. Dies erfordert jedoch Annotation der H5P-Inhalte durch den Autor (z.B. in der Beschreibung der Fragen Schlagworte, die wir auslesen, oder eine Mapping-Tabelle in der DB). In der ersten Version empfehlen wir: Automatisch nur Gesamtpunktzahl auswerten, mehrere Kriterien werden der Lehrkraft zum manuellen Abhaken überlassen. Unsere Plattform kann die H5P-Ergebnisse aber aufbereiten, um die manuelle Bewertung zu unterstützen – etwa indem falsche Antworten hervorgehoben werden, oder ein KI-Modul Vorschläge macht, welche Kriterien evtl. nicht erfüllt sind.

Teilpunkte und Sonderfälle: Einige Content-Typen haben spezielles Scoring: Interactive Video z.B. kann Teilpunkte pro eingebetteter Quizfrage vergeben und am Ende ein Gesamtscore berechnen (den wir nutzen). Essay hingegen hat per se keine automatische Punktevergabe (optional kann ein einfacher Wörterlisten-Abgleich Punkte vergeben, oft aber unzuverlässig). Branching Scenario vergibt möglicherweise gar keine Punkte (der Pfad kann zu unterschiedlichen Enden führen, Score muss ggf. durch Punktevergabe in eingebetteten Quizzes realisiert werden). Solche Fälle erfordern manuelle Nachbewertung oder KI. Wir zeichnen auch hier alle xAPI-Events auf, z.B. den gesamten vom Schüler eingegebenen Essay-Text und ggf. die Selbstbewertung, um darauf basierend Feedback zu generieren. Manuelle Überschreibung: Die Lehrkraft hat immer die Möglichkeit, den von H5P ermittelten Score anzupassen. In der Bewertungsübersicht könnte z.B. ein Button „Score anpassen“ existieren, um etwa bei einem technisch richtigen, aber inhaltlich falschen Antwort (oder vice versa) Korrekturen vorzunehmen. Diese Änderungen werden dann in den criteria dokumentiert (z.B. Kriterium „Korrektheit“ wird trotz voller Punktzahl als „teilweise erfüllt“ markiert mit Hinweis).

Bewertungsweitergabe an GUSTAV: Sobald ein Versuch abgeschlossen ist, speichert das System einen TaskAttempt-Datensatz (user_id, task_id, attempt_nr, score, max_score, completed_at etc.). Dieser wird auch für den Schüler sichtbar: Er sieht z.B. „Versuch 1: 6/10 Punkte (60%) – nicht bestanden“. Falls weitere Versuche offen sind, wird er motiviert, es erneut zu versuchen, ggf. nach Konsultation der Hinweise. Bei max_attempts=1 entfällt diese Anzeige, dann sieht er nur das einmalige Ergebnis. Lehrkräfte sehen alle Attempt-Einträge der Klasse und können nach Bedarf eingreifen. Für den Gesamtkursfortschritt kann konfiguriert werden, ob der beste Versuch oder der letzte zählt – bei Prüfungsmodus eher der erste (um Raten zu vermeiden), bei Übungsmodus der beste (um Lernen zu belohnen). Diese Einstellung könnte über attempt_policy gesteuert werden (z.B. aggregation = best|last).
Authentifizierung, Berechtigungen und RLS

JWT-basierte Auth für H5P-Service: Wir implementieren H5P als Teil des Backends (z.B. als Node.js Microservice) und sichern die Schnittstellen mittels kurzlebiger JWTs, die vom Hauptsystem ausgestellt werden. Beim Aufruf des Players/Editors erhält der Client einen Token mit Claims wie sub (User-ID), role (z.B. student/teacher/author), sowie Kontext-IDs (unit_id, section_id, task_id). Diese Claims nutzt der H5P-Service, um Berechtigungen zu prüfen: Autoren dürfen neue Inhalte erstellen und bestehende bearbeiten (Editor-Komponente), Lehrkräfte dürfen Inhalte ansehen (Player im Preview/Review-Modus) und Ergebnisse einsehen, Schüler dürfen nur im Player-Modus ihre zugewiesenen Inhalte spielen.

Lumieducation’s Server-Library erfordert beim Rendern einen User-Kontext (Objekt mit User-ID, Name etc.), den wir aus dem JWT füllen. Wir übergeben ferner die Task-spezifische Security Context in den H5P-Server: etwa könnten wir eine benutzerdefinierte Authorization-Callback implementieren, die prüft „darf User X Content Y editieren/sehen?“. Im einfachsten Fall halten wir eine Zuordnung Content→Task in der DB vor; die RLS von Supabase gewährleistet, dass nur berechtigte Rollen diesen Task sehen. Der H5P-Service kann bei jedem loadContent Request mittels Task-ID Claim + DB-Join sicherstellen, dass Zugriff erlaubt ist (z.B. Student muss im Kurs eingeschrieben sein und Task gehört zu einem freigeschalteten Abschnitt). Falls nicht, liefert die API HTTP 403.

Editor-Einbettung für Autoren: Der <h5p-editor> Web Component wird nur gerendert, wenn ein berechtigter Nutzer (Autor oder Lehrkraft mit Bearbeitungsrecht) die Seite öffnet. Über den JWT wird dies serverseitig validiert, bevor wir dem Client überhaupt die Editor-Bibliotheken schicken. Das H5P-Editor-Modul selbst bringt nochmals eigene Permissions mit (z.B. verhindert es Bibliotheks-Installation, wenn der User nicht als Admin eingestuft ist). In lumieducation können wir diesen Editor-Zugriff steuern, indem wir beim Aufruf H5PEditor entsprechend konfigurieren. Uploads neuer .h5p-Dateien oder Bibliotheken erlauben wir nur für einen bestimmten Admin-Rolle oder über ein Deployment-Mechanismus (siehe Hub-Updates). Autoren im System können i.d.R. nur Inhalte erstellen aus den vorhandenen Libraries.

Player-Zugriffsmodi: Für Schüler wird der <h5p-player> im Solve-Modus geladen – Standard, mit ihren eigenen Attempts. Lehrkräfte erhalten zwei Varianten: a) Preview – d.h. den Player ohne Persistenz, als ob sie ein Schüler wären, aber Ergebnis fließt nicht in DB (könnte erreicht werden, indem wir den Player mit einem speziellen User-ID alias laden oder contextId nicht speichern). b) Review – das ist das Betrachten des Schüler-Versuchs. Hierfür können wir entweder einen schreibgeschützten Player laden, der den Content mit den gegebenen Antworten anzeigt. H5P erlaubt es, Ergebnisse nochmals anzuzeigen (z.B. kann man einen Content mit vorher gespeicherten Antworten laden). Über die xAPI-Daten oder contentUserData können wir die Eingaben rekonstruieren. Alternativ bietet lumieducation vielleicht die Möglichkeit, den Player mit vorgegebenem State zu initialisieren (ggf. über contentUserData retrieval eines fremden Users – muss aber durch Admin erlaubt sein). In jedem Fall wird der Lehrer-Review in GUSTAV eingebettet, so dass die Lehrkraft im Kontext des Versuchs auch gleich die Kriterien bewerten oder Feedback eingeben kann.

Asset-Auslieferung unter RLS: H5P-Inhalte enthalten Medien (Bilder, Audio, Video) und Library-Skripte, die an den Browser ausgeliefert werden müssen. Standardmäßig würde ein H5P-Server diese über offene URLs unter /h5p/content/... bereitstellen. Um Datenschutz und Zugriffsschutz zu gewährleisten, versehen wir diese Routen mit Auth-Checks. Das heißt, alle H5P-Routen (Content JSON, Bibliotheksdateien, Uploaded assets) laufen durch eine Middleware, die den JWT prüft. Für statische Assets kann ein Token-geschützter CDN-Pfad genutzt werden – z.B. signierte URLs mit Ablauf. Einfacher ist es, im Node-Service einen Express-Static Handler einzurichten, der vor Auslieferung req.user prüft (aus JWT) und anhand z.B. der URL (die Content-ID ist darin enthalten) verifiziert, dass user Zugriff hat. Damit greifen unsere Supabase-RLS-Regeln indirekt auch für die Dateien: der H5P-Service fragt die DB, ob user X den Task Y sieht, dessen Content Z er anfordert. Nur wenn ja, wird die Datei gestreamt. Dies verhindert, dass z.B. ein findiger Schüler mit URL eines Bildes aus einem nicht für ihn bestimmten H5P an diese Ressource käme.

Dritt-Inhalte und Embeds: Ein Spezialfall sind H5P-Inhaltstypen, die externe Ressourcen laden, etwa die YouTube-Integration im Interactive Video oder der IFrame Embedder. Da unsere CSP externes Nachladen blockt (siehe Sicherheit), werden solche Inhalte teils nicht funktionieren oder wir müssen Ausnahmen genehmigen. Aus Datenschutzgründen empfehlen wir, YouTube-Videos etc. vorher herunterzuladen (wenn Lizenz erlaubt) und als Datei einzubinden. Den IFrame-Embedder sollten wir ggf. gar nicht erst den Autoren zur Verfügung stellen (kann man über die H5P-Hub Administration deaktivieren). So behalten wir die Kontrolle über alle geladenen Assets.

Benutzerverwaltung und RLS intern: Die Supabase-DB nutzt RLS, typischerweise mit der Angabe auth.uid() in Policies. In unserem Setup kann der Node-Service entweder mit dem Service-Role (volle Rechte) auf die DB zugreifen und selbst filtern, oder – wenn möglich – den JWT im DB-Call durchreichen (Supabase ermöglicht z.B. im REST-API-Call den JWT mitzuschicken; bei direkter PG-Connection könnte man über die Auth-Extension den JWT auswerten). Für Einfachheit implementieren wir die Checks in Node, wie oben beschrieben, und nutzen Service-Role DB-Zugriff. Die H5P-Service-Komponenten schreiben also z.B. Attempt-Datensätze in die DB mit Service-Rechten, aber wir sorgen dafür, dass z.B. user_id korrekt gesetzt ist auf req.user.sub und die RLS-Policy (user_id = auth.uid()) greift dann für alle nachgelagerten Zugriffe.

Zusammengefasst: Ein Schüler-JWT erlaubt Lesen seines Tasks und Schreibzugriff auf seine Attempt-Daten; ein Lehrer-JWT erlaubt Lesen aller Task-Daten in seiner Klasse und Schreiben von Bewertungen; ein Autor-Token erlaubt zusätzlich Bearbeiten/Erstellen von H5P-Inhalten. Durch kurzlebige Tokens (z.B. Gültigkeit 5 Min) stellen wir sicher, dass Missbrauchsrisiken minimiert werden (z.B. abhanden gekommener Token). Bei längeren Sessions fordert der Client automatisch neue Tokens an.
Learning Analytics (xAPI-Events & Auswertung)

xAPI-Event Capture: H5P-Inhalte generieren von Haus aus xAPI Statements für praktisch alle Lerneraktionen
h5p.org
. Diese reichen von „hat versucht“ (attempted) über „beantwortet“ (answered mit den Details der Antwort) bis zu „abgeschlossen“ (completed mit Score). Über die Web-Component erhalten wir diese Events in JavaScript (Event-Listener auf <h5p-player>.on('xAPI', ...) bzw. via H5P.externalDispatcher falls nötig
h5p.org
). Wir senden jedes Statement als JSON an unser Backend, wo es in einer Supabase-Tabelle (JSONB-Spalte) gespeichert wird. So entsteht ein feingranulares Lernlog pro TaskAttempt. Alternativ bzw. ergänzend nutzen wir lumieducation’s Completion Tracking: Sobald ein Content abgeschlossen wird (typisch beim Klick auf „Check“/„Finish“), sendet der Player eine gekürzte Completion-Info an den Server (Zeit, Score)
docs.lumi.education
, die wir bereits in den Attempt-Datensatz schreiben können. Vollständige xAPI-Statements sind jedoch wertvoller für tiefergehende Analysen.

Event-Schema: Jedes gespeicherte Statement erhält kontextuelle Identifikatoren: unit_id, section_id, task_id (aus dem JWT oder Task-Metadaten eingefügt), user_id (aus JWT), sowie – falls vorhanden – attempt_id (unsere Attempt-Primärschlüssel, entspricht contextId). Damit können wir später Abfragen nach Kurs, Schüler, Aufgabe filtern. Das xAPI JSON selbst liefern die H5P-Libraries gemäß Spezifikation
h5p.org
. Typischerweise enthält ein Statement: verb (z.B. "answered", "completed"), object (Referenz auf die Aufgabe oder Unterfrage), result (Score, MaxScore, Erfolg bool, Antwort etc.), timestamp (wann), duration (Bearbeitungszeit für diese Aktion), und evtl. context (z.B. if SubContent). Wir normalisieren einige Felder in Top-Level-Spalten zur schnelleren Abfrage: z.B. verb, success (wenn vorhanden), score_ratio (Score/Max).

Inhaltstypen und ihre Events: Die wichtigsten Content-Typen senden folgende Kern-Events (Auswahl):

    Interactive Video: sendet pro eingebetteter Frage ein Answered-Event (inkl. welcher Interaktion, gegebene Antwort und ob korrekt)
    h5p.org
    h5p.org
    . Am Ende, wenn das Video fertig geschaut und alle Fragen beantwortet sind, ein Completed interactive video mit Gesamtscore
    h5p.org
    . Außerdem Interacted-Events z.B. bei jedem Klick in Quizfragen oder beim Springen in der Timeline
    h5p.org
    h5p.org
    . Daraus gewinnen wir Sehgewohnheiten (hat der Schüler vorspulen genutzt?), Quiz-Ergebnisse pro Fragment, etc.

    Question Set (Quiz): schickt für jede enthaltene Frage idR ein answered Event, und wenn alle durchlaufen, ein completed mit totalem Score. Bei falschen Antworten und mehreren Versuchen pro Frage können auch interacted/attempted Events vorkommen. Wichtig ist: es gibt score und maxScore im Completed-Statement, sowie Detailinfos in den Answered-Statements (welche Option gewählt usw.).

    Course Presentation: Jedes Quiz-Element darin verhält sich ähnlich wie eigenständige Fragen (xAPI pro Interaktion). Zudem werden bei Folienwechsel oft progressed Events gesendet (Seite X von Y erreicht). So lässt sich Nutzungsverhalten nachvollziehen (hat der Schüler die Präsentation komplett durchgeklickt?). Abschluss gibt es, wenn eine Summary Slide vorhanden ist evtl. als completed mit Score, ansonsten evtl. nur experienced.

    Essay: Der Essay-Content generiert beim Absenden i.d.R. ein answered oder completed Event mit Score 0 (da standardmäßig keine Autokorrektur) und möglicherweise dem Text als Teil des response im Statement. Dieser sollte in unserem Event-JSON sein (bspw. unter result.response den geschriebenen Text). Wir speichern diese Antwort auch separat (z.B. in einer Submission-Tabelle oder in den xAPI-Daten), um sie für KI-Feedback oder Lehrerbewertung bereitzustellen. Kein automatisches passed/failed, das entscheidet die Lehrkraft.

    Drag & Drop: Jedes Loslassen eines Drag-Items: Interacted drag and drop
    h5p.org
    , beim Klick auf „Überprüfen“: Answered drag and drop mit Score für diese DnD-Aufgabe
    h5p.org
    . Im result können wir sehen, welche Zuordnung richtig/falsch war (manchmal im response codiert, z.B. Auswahl-IDs). Summenscore falls Drag&Drop Teil eines Quiz-Sets.

    Branching Scenario: Dieser Typ ist komplex, sendet progressed Events beim Wechseln von Szenen/Knoten (inkl. Verweildauer in einem Knoten)
    github.com
    . Ein explizites completed Statement gibt es, wenn der Branch endet, eventuell mit Score falls so konfiguriert
    h5p.org
    . Wir können aus den Events rekonstruieren, welchen Pfad der Lernende genommen hat, wo falsche Entscheidungen getroffen wurden (wenn wir die Knoten als richtig/falsch einstufen). Diese Daten sind wertvoll, um zu sehen, welche Verzweigungen häufig zum Erfolg führen oder wo die meisten abbrechen.

Dashboard und Kennzahlen: Mit den gesammelten Daten bauen wir Auswertungen auf mehreren Ebenen:

    Task-Ebene (Einzelschüler): Detaillierte Ergebnis-Ansicht pro Versuch: Score, benötigte Zeit, Anzahl genutzter Hints, Fehler pro Frage. Diese Ansicht greift direkt auf die xAPI-Events: z.B. wir visualisieren in einem Diagramm welche Fragen falsch waren (anhand answered Events) und wie oft evtl. versucht (z.B. wenn interacted mehrfach vor dem finalen answered kam). Für Essays können wir die Textantwort und das KI-Feedback hier anzeigen.

    Task-Ebene (Klassenaggregat): z.B. eine Verteilungs-Grafik der Scores aller Schüler, Durchschnittliche Versuchszahl bis Erfolg, meistgewählte falsche Distraktoren (häufige Irrtümer) – letzteres ermitteln wir, indem wir alle answered Events analysieren: welche falsche Option wurde wie oft gewählt. xAPI liefert die gegebenen Antworten als Teil des Statement, die wir auswerten können. Solche Erkenntnisse helfen der Lehrkraft, Fehlerschwerpunkte zu identifizieren (z.B. 70% der Schüler haben Option B gewählt – ggf. ein Hinweis auf ein Misconception).

    Kriterium- / Kompetenz-Durchdringung: Falls wir Kriterien mit bestimmten Fragen/Skills verknüpfen, können wir pro Kriterium den Erfüllungsgrad ermitteln. Beispiel: Kriterium „Vokabelwissen“ – umfasst Quizfragen 1-5, der Schüler hat davon 4 richtig => 80% erfüllt. Aggregiert auf Kurs kann man sehen, Kriterium X ist im Schnitt 65% erreicht, während Kriterium Y 85% – so erkennt man, wo Nachholbedarf besteht. Diese Aggregation kann in einer Dashboard-Kachel je Kriterium (per Section oder Unit) angezeigt werden.

    Hint-Nutzung: Wir loggen die Nutzung der Hinweise als eigene Events (z.B. wenn der Schüler auf „Tipp anzeigen“ klickt, erzeugen wir ein Event hints_used mit Angabe welches Level). Dadurch können wir im Nachgang sehen, welche Aufgaben viele Hints erforderten. Ein hoher Anteil an genutzten Hinweisen könnte bedeuten, dass die Aufgabe sehr schwierig war oder die Instruktion unklar. Pro Schüler könnte man auch analysieren: Hat er einen Hinweis genutzt und danach Erfolg gehabt? Solche Muster unterstützen adaptives Lernen (z.B. Schüler, die trotz Hinweis scheitern, brauchen evtl. grundlegenderes Remediation).

    Zeit-on-Task: Jedes completed Event enthält die gesamte Bearbeitungsdauer (result.duration), die wir speichern. Zudem können wir Startzeiten mit Endzeiten differenzieren. Daraus können wir ableiten, wie lange Schüler an einer Aufgabe arbeiten. Ein Diagramm „Durchschnittliche Bearbeitungszeit vs. Score“ wäre denkbar, um zu prüfen, ob z.B. lange Bearbeitungszeit zu besserem Ergebnis führt oder ob einige zu schnell abgegeben haben. In schulischen Kontexten ist auch wichtig zu sehen, ob Aufgaben zu zeitaufwändig sind (z.B. wenn median >> erwartete Zeit).

    Kurs- und Abschnittsebene: Wir aggregieren die Task-Daten hoch: z.B. Abschlussrate pro Abschnitt (Anteil der Schüler, die alle H5Ps in Abschnitt X bestanden haben), oder Durchschnittsscore pro Task im Vergleich zwischen verschiedenen Klassen. Die JSONB-Eventspeicherung erlaubt es uns, auch nachträglich noch neue Metriken zu berechnen, indem wir die Statements abfragen. Wir können Supabase’s Funktionen (z.B. Materialized Views oder Edge Functions) nutzen, um bestimmte Auswertungen vorzuberechnen.

Beispiel xAPI-Auswertung: Angenommen, Interactive Video hat 3 integrierte Fragen. Wir würden aus den answered Events sehen, welche Fragen ein Schüler richtig hatte. Daraus ließe sich eine Fragenmatrix erstellen (Frage vs. Prozent richtig in Klasse). Ebenso könnten wir aus interacted Events des Videos ablesen, ob viele Schüler vor dem Ende abgebrochen haben (wenn wenige completed im Verhältnis zu attempted vorkommen).

Daten-Privacy: Die xAPI-Daten bleiben intern (Supabase DB mit RLS). Trotzdem müssen wir überlegen, wie lange wir sie aufbewahren (DSGVO Grundsatz Datensparsamkeit). Für Lernanalysen im laufenden Kurs sind sie notwendig; evtl. können wir nach Kursende einige detaillierte Events anonymisieren oder löschen, wenn sie nicht mehr benötigt werden. Aggregierte Statistiken können wir beibehalten.
Sicherheit und Betrieb

Content Security Policy (CSP): Alle H5P-Inhalte und Libraries werden von unserem Server geladen, daher setzen wir eine restriktive CSP: default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'; (H5P benötigt Inline-Styles für Drag&Drop Positionierung, daher 'unsafe-inline' für CSS, was akzeptabel ist, da die CSS von unseren Libraries stammt). Externe Domains (YouTube, Vimeo etc.) sind standardmäßig verboten. Wenn Lehrinhalte externe Medien benötigen, müssen sie über erlaubte Pfade (z.B. unser Medienserver) eingebunden werden. Ein Spezialfall sind Webfonts oder mathematische Formeln: H5P nutzt MathJax für LaTeX-Formeln – wir würden MathJax lokal hosten, um keine externen Calls zu haben. CSP wird auch im Editor gelten: Der H5P-Hub (für Content-Typ-Installation) ruft ggf. nach extern – das unterbinden wir; stattdessen laufen Updates offline (siehe weiter unten). Summiert stellt die CSP sicher, dass selbst wenn ein H5P-Inhalt bösartigen Code enthält, dieser nicht nach extern telefonieren oder fremden Code nachladen kann. Sollte ein Lehrer dennoch versuchen, z.B. via IFrame-Embedder externe Inhalte einzubinden, blockt die CSP dies – das UI würde z.B. ein leeres Frame anzeigen, und wir loggen einen CSP-Report (können CSP-Reporting-Endpoint einrichten) um solche Versuche nachzuvollziehen.

Upload-Sicherheit: Beim Hochladen von H5P-Paketen oder Assets greifen mehrere Schutzmaßnahmen: Zunächst validiert der H5P-Server die Paketstruktur gemäß H5P-Spezifikation (Dateien wie h5p.json, Libraries etc.)
h5p.org
h5p.org
. Dann filtern wir Datei-Endungen gegen eine Whitelist (H5P empfiehlt, unbekannte Dateien zu ignorieren
h5p.org
). Insbesondere SVGs: Hier nutzen wir @lumieducation/h5p-svg-sanitizer, um SVG-Inhalte auf XSS zu prüfen und ggf. zu bereinigen
docs.lumi.education
. Drittens schleusen wir alle hochgeladenen Dateien durch einen ClamAV-Virenscan (lumieducation stellt h5p-clamav-scanner bereit
docs.lumi.education
). Dieser Prozess läuft asynchron in der Upload-Pipeline: Erst wenn ClamAV ok meldet, wird das File persistent gespeichert. Bei Fund wird der Upload abgebrochen und gemeldet. Für den Editor-Upload (Bilder, Medien innerhalb eines Contents) gilt das gleiche. Dadurch schützen wir vor gängigen Malware- und XSS-Angriffen, die über Content eingeschleppt werden könnten.

Offlinetauglichkeit: Unsere H5P-Installation holt keine Libraries von einem CDN, sondern nutzt die mitgelieferten, lokal gespeicherten Core-Dateien (die lumieducation beim Setup aus der offiziellen PHP-Distribution herunterlädt
docs.lumi.education
docs.lumi.education
). Somit sind alle benötigten JS/CSS-Libs im Docker-Image oder im Installationspfad vorhanden. Content-Type-Libraries (z.B. H5P.MultiChoice 1.14) werden ebenfalls lokal gehalten. Für Schulnetzwerke ohne Internet bedeutet das: Sobald der Server installiert ist, braucht der Live-Betrieb keine Internetverbindung für das Ausspielen der Inhalte. Aber: Autoren könnten im Editor beim Einfügen von z.B. Videos via URL doch Netz brauchen – hier sollten wir Anleitungen geben, wie sie Medien vorher lokal speichern. Für multimediale H5Ps sollte die Schule idealerweise ein Media-Repository auf dem Server haben. Der Player selbst funktioniert offline im Browser, solange die Seite geladen werden kann (bei Stromausfall oder Server-down natürlich nicht). Falls absolute Offline-Lernmodule gewünscht sind, könnten wir optional den h5p-html-exporter nutzen, um einzelne H5P-Inhalte als eigenständige HTML-Dateien zur Verfügung zu stellen
docs.lumi.education
– dies ist aber eher für OER-Export gedacht.

Updates von Libraries (Hub): H5P veröffentlicht regelmäßig neue Content-Type-Versionen und Updates des Core. Da wir keine SaaS nutzen, müssen wir diese selbst einspielen. Content Types (Libraries): Es gibt zwei Wege, Libraries zu aktualisieren:

    H5P Hub im Editor: In Moodle/WordPress kann der Admin im H5P-Hub neue Libraries herunterladen. Lumieducation hat (noch) keine vollautomatische Hub-Integration erwähnt. Wir können jedoch einen Cron-Job implementieren, der die H5P Hub API abfragt (eine Liste verfügbarer Libraries und Versionen) und bei Neuigkeiten ein Admin-H5P-Paket herunterlädt und importiert. Alternativ können wir Updates manuell einspeisen, indem unser Team regelmäßig .h5p Beispieldateien von h5p.org herunterlädt, die neue Libraries enthalten, und diese via Editor-Upload installiert
    h5p.org
    h5p.org
    .

    Core-Updates: Diese betreffen H5P Core JS/CSS und Editor (z.B. Update von 1.24 auf 1.27). Lumieducation beschreibt dafür einen Prozess, wo man die neuen Core-Dateien aus dem offiziellen Repo holt und referenziert
    docs.lumi.education
    docs.lumi.education
    . Wir würden das im Rahmen von Paketupdates unseres H5P-Servers tun (ggf. Docker Image neu bauen).

Governance & Testen: Bevor neue Libraries produktiv werden, sollten sie geprüft werden. Wir richten eine Staging-Umgebung ein, in der Updates eingespielt und mit existierenden Inhalten getestet werden. H5P garantiert in der Regel Abwärtskompatibilität, aber es können visuelle Änderungen oder neue Features auftreten. Durch RLS können wir neue Libraries initial nur für Admins freischalten: d.h. der Admin installiert die Library, aber in der DB markieren wir sie als „unapproved“ – der Editor listet sie nicht für normale Autoren. Nach inhaltlicher Prüfung (und ggf. Schulung der Lehrer) setzen wir sie auf „approved“. Ein Rollback einer Library-Version ist nicht trivial, da Content bereits migriert sein könnte. H5P erlaubt parallele Versionen: d.h. alte Inhalte behalten die alte Library, neue Inhalte könnten aber schon die neue nutzen, wenn wir sie installieren. Soll ein Update wirklich rückgängig gemacht werden, müsste man die neue Version aus der DB löschen – das geht nur, wenn keine Inhalte damit erstellt wurden. Daher vorsichtige Einführung: evtl. initial „nur auf Staging Kurs nutzbar“.

Backups: H5P-Inhalte und Libraries (Dateisystem + DB-Einträge) sollten Teil unseres Backup-Konzepts sein. Supabase-DB wird regelmäßig gesichert; das Filesystem mit content folder ebenso (z.B. tägliches rsync-Backup oder Snapshots, um Medienverlust zu verhindern). Bei einem Restore muss sichergestellt sein, dass DB und Dateien konsistent sind (Content-ID stimmen mit Ordnern überein). Ein einfacherer Weg: Speichern aller H5P-Inhalte auch als .h5p-Paket in einem Backup-Speicher. So könnte man notfalls das System neu aufsetzen und alle Inhalte importieren. Die Attempt- und xAPI-Daten sind in der DB und somit via DB-Backup geschützt.

Performance und Skalierung: H5P ist client-seitig eher schwergewichtig (viele JS, insbesondere bei großen Content wie Course Presentation). Wir müssen die Auslieferung optimieren: z.B. GZIP/Brotli Komprimierung aktivieren für H5P JS/CSS, HTTP caching für Library files (die ändern sich selten – wir können long-lived Cache-Control Header setzen, da Dateinamen meist versioniert sind). Server-seitig ist Node mit lumieducation effizient genug; es nutzt Mongo oder FS für Storage, was O(1) Zugriff auf Content bedeutet. Wir sollten darauf achten, in einer Multi-Node Umgebung (falls Lastverteilung) die H5P content storage entweder zentral zu halten (NFS oder S3) oder bei Änderungen die Nodes synchron zu halten. Lumieducation bietet Locks via Redis, falls Editoren gleichzeitig zugreifen
docs.lumi.education
– für uns wohl selten relevant (zwei Leute editieren selben Content parallel). Aber bei verteilten Instanzen ist ein Shared-Lock-Store ratsam, um Race Conditions (z.B. Hub-Update) zu vermeiden.

Barrierefreiheit (A11y): Betrieblich relevant ist, dass alle Benutzer (auch solche mit Behinderungen) die H5P-Inhalte nutzen können. H5P.org führt eine Übersicht, welche Content-Typen als barrierefrei gelten. Viele interaktive Inhalte sind weitgehend barrierefrei, aber nicht alle vollständig
help.h5p.com
. Z.B. Interactive Video gilt als accessible (sofern Untertitel für Videos vorhanden)
h5p.org
, Drag & Drop hatte in älteren Versionen Probleme mit Tastaturbedienung, aber neuere Versionen implementieren zumindest Teil-Funktionen (z.B. via Tab+Enter) – dennoch ist Vorsicht geboten: Drag&Drop wird mit WCAG2.2 genauer überprüft
h5p.org
. Unsere Rolle im Betrieb: Wir müssen Autoren schulen, die A11y-Richtlinien je Content-Type zu beachten (z.B. Alternativtexte für Bilder in Drag&Drop, korrekte Überschriftenhierarchie in Course Presentations, ausreichende Farbkontraste). Wir ergänzen die Doku in GUSTAV: “H5P Accessibility Best Practices“ und prüfen neue Inhalte stichprobenartig. Sollte ein Schüler mit z.B. Sehbehinderung einen bestimmten Content nicht nutzen können, muss die Lehrkraft eine Alternative bereitstellen – die Plattform sollte darauf hinweisen (z.B. wir könnten Metadaten pflegen: Content-Type X is not fully accessible, um Lehrern beim Erstellen zu warnen).

Zusätzlich setzen wir Focus-Management: Nach dem Laden des H5P-Players sollte der Fokus auf den Anfang des Inhalts gesetzt werden (damit Screenreader von dort lesen). Web Components ermöglichen uns, nach initialized Event
docs.lumi.education
den Fokus ins Player-Div zu setzen. Für zeitbasierte Inhalte wie Videos sorgen wir, dass Steuerungen per Tastatur nutzbar sind (H5P liefert eigene Controls die meist <kbd>Space/Enter</kbd> unterstützten). Wir testen unsere integrierte Oberfläche auch mit Screenreader (NVDA/JAWS) und Tastatur, um sicherzustellen, dass Instruktion, Player, Hints, Criteria in sinnvoller Reihenfolge vorgelesen werden.

Logging & Monitoring: Aus Sicherheits- und Fehlersicht setzen wir Logging für den H5P-Service auf (Zugriffe, Fehler bei Content Render). Verdächtige Vorgänge (z.B. mehrfaches Scheitern des ClamAV oder CSP-Violations) alarmieren uns. Performance-Monitoring (Responsezeiten beim Laden eines H5P) hilft, Engpässe zu erkennen (z.B. zu große Videos). Für DSGVO führen wir ein Verarbeitungsverzeichnis für die Telemetriedaten (xAPI), da hier Leistungsdaten von Schülern gespeichert werden. Die Schüler werden transparent darüber informiert (z.B. in Datenschutzinfo der App), dass Interaktionen zu Lernzwecken aufgezeichnet werden.
Didaktische Leitlinien je Content-Typ
Rolle von Instruktion, Kriterien und Hinweisen

Unsere drei textuellen Felder im Aufgabenmodell (instruction_md, criteria, hints_md) übernehmen in H5P-Tasks spezifische didaktische Funktionen, ergänzend zum H5P-Inhalt:

    Instruction (Aufgabenstellung): Hier wird der Lernkontext geschaffen und der Arbeitsauftrag klar formuliert. Didaktisch fungiert die Instruction als Advance Organizer: Sie stellt Verbindungen zu Vorwissen her, benennt das Lernziel und gibt den Operator (z.B. „Analysiere…“, „Berechne…“). Bei H5P-Aufgaben, die oft sehr interaktiv sind, sorgt ein guter Instruction-Text dafür, dass Schüler wissen, warum sie die Aktivität machen. Beispiel: Vor einem Drag&Drop zu Anatomie könnte stehen: „Aufgabe: Ordnen Sie die Begriffe den richtigen Stellen im Herzdiagramm zu. Lernziel: Sie kennen den Aufbau des menschlichen Herzens.“. Auch Hinweise zur Bedienung oder Zeitvorgaben können hier erscheinen. Wichtig ist, die Sprache adressatengerecht zu halten (keine Überfrachtung), ggf. Bullet-Points bei komplexen Anweisungen.

    Criteria (Bewertungskriterien): Diese bieten Transparenz über die Leistungsbewertung und dienen der formativen Orientierung. Bei automatischen Quiz-Aufgaben mögen Kriterien trivial erscheinen („X von Y Punkten erreicht“). Dennoch kann man Kriterien nutzen, um qualitative Aspekte einzubringen. Beispielsweise: „Genauigkeit: Du hast alle Drag&Drop-Elemente korrekt zugeordnet“, „Sorgfalt: Du hast dir Hinweise geholt, wenn nötig, und Fehler im zweiten Versuch verbessert“. In offenen Aufgaben (Essay, Branching) sind Kriterien essenziell: Sie kommunizieren, worauf es ankommt (Inhaltliche Richtigkeit, Kreativität, Sprache, etc.). Die Kriterien sollten möglichst operationalisiert sein (für Schüler verständlich und überprüfbar). Lehrkräfte können auch Beispiellösungen oder Erwartungshorizonte hinter den Kriterien andeuten. Durch die vorherige Bekanntgabe der Kriterien wissen Schüler, wie sie ihren Erfolg selbst messen können – das fördert selbstreguliertes Lernen. Nach Abgabe können Kriterien für Feedback genutzt werden: Der Lehrer oder die KI kann jeden Punkt abhaken oder kommentieren, so erhält der Schüler differenziertes Feedback statt nur einer Zahl.

    Hints (Hinweise): Die Hinweise in hints_md sind ein zentrales Scaffolding-Werkzeug. Anstatt Schüler bei Schwierigkeiten allein zu lassen oder sofort die Lösung zu geben, bieten gestufte Hinweise immer konkretere Hilfe. Stufenweise Unterstützung: Der erste Hint könnte nur einen Denkanstoß geben („Schau dir nochmals die Definition von X an.“), der zweite mehr Details („Beachte besonders den Unterschied zwischen A und B…“) und der letzte fast die Lösung verraten („Denke daran: A ist immer doppelt so groß wie B in diesem Szenario.“). Für Aufgaben mit objektiven Lösungen (z.B. Matheaufgabe, MC-Frage) können Hinweise gezielt typische Fehlkonzepte adressieren (Fehlervorhersage: „Wenn du C als Antwort erhalten hast, hast du vermutlich X übersehen.“). Bei prozeduralen oder offenen Aufgaben (Essay, Experiment) sind Hinweise eher Strategie-Hilfen: „Strukturiere deinen Text mit einer Einleitung…“ oder „Überlege, welche Variablen du kontrollieren musst.“. Didaktisch fördern gut designte Hinweise die Lernenden, sich Schritt für Schritt selbst zu verbessern, ohne gleich frustriert aufzugeben. Wichtig: Hinweise sollten optional sein – die besten Schüler brauchen sie evtl. nicht – und es sollte keine Bewertungseinbuße geben, wenn man sie nutzt (außer man möchte gezielt Anreize setzen, z.B. jeder genommene Hinweis kostet Punkte – das könnte man in Kriterien abbilden, aber vorsichtig einsetzen um Schwächere nicht zu bestrafen).

Integrierte Nutzung: Instruktion, H5P-Interaktion, Hinweise und Kriterien sollten aufeinander abgestimmt sein. Ein möglicher didaktischer Ablauf: Der Schüler liest die Instruktion (versteht Ziel/Aufgabe), bearbeitet die H5P-Aktivität, stockt evtl. -> schaut einen Hinweis an, verbessert sich, schließt ab -> schaut in Kriterien, um sein Ergebnis einzuordnen. Diese Verzahnung sorgt für einen vollständigen Lernzyklus: Vorwissen aktivieren → Aktivität → Feedback → Reflexion. GUSTAV als Plattform stellt sicher, dass diese Metastruktur einheitlich bleibt, unabhängig vom spezifischen H5P-Typ.
Content-Typ-Empfehlungen pro Lernphase

H5P bietet eine Vielfalt an Inhaltstypen, die sich unterschiedlich gut für Phasen im Lernprozess eignen:

    Einstieg/Motivation: Hier geht es darum, Interesse zu wecken und Vorwissen zu aktivieren. Empfohlene Typen:

        Interactive Video: Ein Video mit eingestreuten Fragen oder Infos eignet sich, um ein Thema anschaulich einzuführen und Schüler aktiv zuzuschalten. Didaktik: Kurze Clips mit Schlüsselszenen, nach jeder Szene eine Frage, um zum Nachdenken anzuregen. Achtung: Immer Untertitel bereitstellen für Hörgeschädigte; Video-Länge <5min halten.

        Dialog Cards oder Flashcards: Perfekt für Vokabeln oder Begriffspaare wiederholen. Sie können im Einstieg genutzt werden, um Vorwissen abzufragen (z.B. Fachbegriffe <-> Erklärungen). Da sie selbstgesteuert sind, fördern sie initiales Abrufen von Wissen. A11y: Dialog Cards sind meist zugänglich (Tab zwischen Vorder-/Rückseite, Screenreader liest Text; Bildbeschreibung nicht vergessen).

        Branching Scenario (simpler Use): Evtl. als motivierendes Szenario: Der Schüler trifft Entscheidungen in einer Geschichte. Für den Einstieg sollte es nicht zu komplex sein, sondern eher ein Interessewecker, der in Hauptlernphase aufgelöst wird. Z.B. ein kurzes Szenario „Wähle deinen Weg – was würdest du tun?“, um Kontroversen anzustoßen.

    Erarbeitung (Wissensaneignung): In dieser Phase sollen Schüler aktiv mit neuem Stoff umgehen:

        Course Presentation: Eine interaktive Präsentation mit Folien, wo Informationen, Bilder und kleine Wissensfragen kombiniert sind. Das erlaubt dem Schüler, im eigenen Tempo durch Lernmaterial zu gehen, mit eingebauten Übungen zur Sicherung. Didaktik: gut strukturieren (Agenda Folie, klare Überschriften auf Folien), Interaktionen dosiert einsetzen (z.B. nach 2-3 Folien eine Frage). A11y: Präsentationen sind teilweise barrierefrei, aber z.B. Drag&Drop auf Folien ist problematisch; also alternative Zugänge bieten (z.B. alle Folieninhalte auch linear untereinander als Text anzeigbar machen).

        Drag & Drop: Für Zuordnungen, Aufbau von Konzepten oder Diagrammen. In der Erarbeitungsphase kann z.B. eine Aufgabe „Ordne die Begriffe an die richtige Stelle im Ablaufdiagramm“ den Lernprozess unterstützen. Didaktisch wichtig: Im Instruction-Text klarmachen, was gelernt werden soll. Man kann Drag&Drop auch als kollaborative Aufgabe gestalten (jedoch H5P selbst ist single-user, Kooperation muss extern moderiert werden). A11y: Hier ggf. Ausweichformat bereitstellen (z.B. gleiche Aufgabe als Lückentext oder Multiple Choice, falls Drag&Drop für jemanden nicht machbar ist).

        Virtual Tour/Interactive Book: Diese können in Erarbeitungsphasen eingesetzt werden, um umfangreicheres Material interaktiv zu vermitteln (z.B. eine virtuelle Labor-Tour, bei der man Stationen erkundet). Didaktisch sorgen solche Formate für intrinsische Motivation, allerdings muss man aufpassen, dass Lernziele nicht durch zu viel Spielerei aus dem Blick geraten – klare Aufträge in Instruktion formulieren („Finde die Antwort auf… innerhalb der Tour“).

    Übung & Diagnose: Hier sollen Schüler ihr Verständnis testen und Lücken identifizieren:

        Quiz (Question Set): Klassische Quiz-Sammlung (MC, Fill in the Blank, etc.)
        nextsoftwaresolutions.com
        . Gut geeignet, um am Ende einer Lerneinheit das Wissen abzufragen. Didaktik: Variation der Fragetypen, sofortiges Feedback pro Frage einschalten, damit Schüler aus Fehlern lernen. Progress-Anzeige motiviert zum Durchhalten. Achten auf umfassendes Abdecken des Stoffs. Score kann als Diagnose genutzt werden (z.B. <50% = Stoff erneut anschauen).

        Single Choice Set / Multichoice: Spezifische Varianten, eignen sich für kürzere Übungssequenzen (z.B. 5 Multiple-Choice-Fragen auf einmal). Vorteil: einfache Bedienung, für mobile Nutzung geeignet.

        Fill in the Blanks / Drag the Words: Für Üben von Definitionen, Formeln, Vokabeln im Kontext. In Diagnosephase kann man gezielt Lücken in einem Text lassen, um Wissenslücken offenzulegen.

        Branching Scenario: In Diagnosephase könnte ein Branching Scenario als Selbsttest verwendet werden: je nach Entscheidung verzweigt es und gibt am Ende eine Auswertung („Ihr Lösungsweg zeigt, dass Sie Konzept X noch nicht verstanden haben.“). Allerdings erfordert das viel didaktische Planung, die ggf. nicht im ersten MVP steckt.

    Transfer & Reflexion: Zur Anwendung des Gelernten in neuem Kontext und zur Metareflexion:

        Essay: Eine offene Frage, bei der Schüler Gelerntes in eigenen Worten darlegen oder argumentativ anwenden. Ideal für Reflexion oder Transfer („Wie würden Sie die gelernten Prinzipien in Situation Y anwenden?“). H5P-Essay kann einfache automatische Feedbacks (Stichwortsuche) geben, aber Hauptfeedback kommt von KI/Lehrer. Didaktik: Klare Aufgabe formulieren, evtl. Kriterien angeben (z.B. mindestens 200 Wörter, Argument pro/contra). Schülern ruhig erlauben, den Essay mehrfach zu bearbeiten (Draft -> Feedback -> Final), um Schreibkompetenz zu fördern.

        Documentation Tool: Dieses H5P-Tool führt Lernende durch eine Reihe von Reflexionsfragen und generiert am Ende ein PDF-Dokument mit ihren Antworten. Gut einsetzbar am Ende eines Projekts oder einer Unterrichtseinheit, wo Schüler z.B. Ziele, Erkenntnisse, offene Fragen festhalten. Didaktisch ist es eine geleitete Reflexion. Vorteil: Das Ergebnis kann der Schüler speichern (Lerntagebuch) und mit Lehrer teilen. Accessibility: weitgehend okay, da es Text-Eingaben sind; aber man muss die generierte PDF ggf. auf Screenreader-Tauglichkeit testen.

        Image Collage/Memory Game etc.: Eher ungewöhnlich für Transfer, aber man könnte z.B. ein Memory Game einsetzen, wo Fachbegriffe mit Beispielen gematcht werden – als spielerische Wiederholung am Ende.

        Interactive Video: Auch für Reflexion nutzbar, z.B. ein Video eines Phänomens zeigen und Schüler müssen interpretierende Fragen beantworten (Transfer des Gelernten auf neues Beispiel).

Zusammenfassend sollen Lehrer die geeigneten Content-Typen passend zum Lernziel wählen. Für reines Faktenüben: Quiz, Flashcards. Für Verständnis: Interactive Video mit eingebetteten Verständnisfragen. Für Anwendung: Branching Scenario oder offene Aufgaben. Für Synthese/Reflexion: Essay, Documentation Tool. GUSTAV wird mit Leitfäden unterstützen: „Wenn du X erreichen willst, probiere H5P-Typ Y“.
Verzahnung mit KI-Feedback

Die GUSTAV-Plattform verfügt über KI-gestütztes Feedback, das wir gezielt mit H5P kombinieren:

H5P-internes Feedback vs. KI: Viele H5P-Inhalte liefern sofortiges Feedback, vor allem bei geschlossenen Formaten. Beispiel: Multiple Choice zeigt nach „Überprüfen“ richtige/falsche Markierungen und ggf. eine vom Autor definierte Erklärung zur richtigen Antwort. Dieses Feedback ist schnell und spezifisch – hier genügt das H5P-eigene System vollkommen. Ein KI-Feedback wäre hier überflüssig oder könnte sogar verwirren. Ähnlich bei Drag&Drop: Das System zeigt richtige Platzierungen, eventuell kann der Autor auch für jeden Fehler eine Text-Rückmeldung vordefinieren. Solche direkten Rückmeldungen sollten wir nutzen, um den Lernfluss nicht zu unterbrechen.

Einsatz der KI bei offenen Aufgaben: Sobald es um Freitext, komplexe Problemstellungen oder kreative Aufgaben geht, sind H5P’s Bordmittel limitiert. Zum Beispiel: Essay-Content kann nur prüfen, ob gewisse Wörter vorkommen, was ein sehr rohes Maß ist. Hier kann unsere KI einspringen: Sobald der Schüler seinen Essay eingereicht hat, kann die KI eine qualitative Analyse liefern – z.B. Kohärenz des Arguments, orthographische Korrektheit, Abdeckung der geforderten Punkte. Dieses Feedback kann dem Schüler in natürlicher Sprache gegeben werden, ergänzt durch Markierungen im Text. Ebenso bei Documentation Tool-Ergebnissen: die KI könnte das PDF/Antworten durchgehen und dem Lernenden eine Zusammenfassung seines Lernfortschritts oder Tipps geben („Du hast reflektiert, dass... Vielleicht könntest du noch überlegen, warum...“).

Meta-Feedback zu Lösungswegen: Bei komplexen Branching Scenarios oder Interactive Videos (mit Entscheidungen) kann KI helfen, Muster zu erkennen: Z.B. „Du hast dich im Szenario immer für den sicheren Weg entschieden. Versuche beim nächsten Mal auch Risiko einzugehen, um zu sehen, was passiert.“ – so ein Feedback geht über richtig/falsch hinaus und hilft dem Lernenden, sein Vorgehen zu reflektieren. KI kann auch alternative Lösungen aufzeigen: „Es gäbe noch einen anderen Weg, der über Schritt X zum Ziel führt…“.

Vermeidung falscher Positiv/Negativ: KI ist nicht unfehlbar. Wir müssen darauf achten, dass KI-Feedback nicht absolut gesetzt wird. Daher präsentieren wir es als „KI-Rückmeldung“ getrennt vom offiziellen Bewertungsteil. Wenn KI z.B. im Essay etwas fälschlich als Plagiat markiert oder einen korrekten Satz missversteht, muss der Schüler wissen, dass finale Noten von der Lehrkraft kommen. Die Lehrkraft erhält das KI-Feedback auch, kann es überprüfen und bei offensichtlichen Fehlurteilen korrigierend eingreifen. Ggf. kann der Lehrer per Knopfdruck das KI-Feedback ausblenden, wenn es irreführend war, und stattdessen einen eigenen Kommentar hinterlassen.

Rubriken-basiertes Rescoring: Eine interessante Nutzung der KI ist das Bewerten entlang der Kriterien. Angenommen, wir haben 3 Kriterien für einen Essay (Inhalt, Struktur, Sprache). Die KI könnte jeden dieser Aspekte separat bewerten (z.B. mit einem Score 1-5 oder erfüllt/teilweise/nicht) basierend auf den vorgegebenen Beschreibungen. Diese vorläufige Rubrik-Auswertung würde dem Lehrer Zeit sparen: er sieht z.B. KI meint „Inhalt: erfüllt, Struktur: teilweise (Argumentation springt etwas), Sprache: erfüllt mit wenigen Fehlern“. Der Lehrer kann dem zustimmen oder Anpassungen machen. Technisch würden wir der KI die Kriterienbeschreibungen prompten und sie bitten, den Schülertext daran zu messen. Rescoring: Falls z.B. KI einen Aspekt zu schlecht bewertet hat (false negative) und der Lehrer korrigiert das, könnte das System lernen (mittelfristig) oder zumindest diese Änderung dokumentieren, damit der Schüler sieht „Kriterium X wurde manuell auf erfüllt gesetzt trotz KI-Einschätzung“. So bleibt Bewertung transparent.

Zeitpunkt des KI-Feedbacks: Wir müssen auch didaktisch entscheiden, wann KI-Feedback gegeben wird. Möglichst nachdem der Schüler seine eigene Lösung fertig hat, um produktives Denken nicht zu früh zu stören. Beispielsweise nicht schon während des Schreibens ständig KI-Tipps geben (das wäre eher ein intelligenter Tutor, den wir evtl. später integrieren können). Momentan fokussieren wir auf Post-Submission Feedback.

KI bei objektiven Aufgaben: Denkbar, aber vorsichtig: Bei MC/Quiz könnte KI erklärende Hinweise geben, warum die richtige Antwort richtig ist, oder eine kleine Auswertung („Du hast 3 von 5 Physikfragen falsch – die KI vermutet, du hast Mühe mit dem Konzept der Energieerhaltung.“). Solche Analysen kann KI aus den Fehlermustern ziehen, sofern genügend Daten. Allerdings sind H5P-Contents schon oft mit Erklärungs-Feedback versehen (vom Autor vorgegeben). Hier riskieren wir Dopplungen oder widersprüchliche Infos. Daher: Für automatische Quiz nutzen wir primär das Autor-Feedback. KI könnte ergänzend eingesetzt werden, wenn z.B. kein spezifisches Feedback vorhanden ist oder um generelle Lerntipps zu geben („Lernstrategie: Wiederhole das Kapitel 3 im Buch, dort findest du die Grundlagen zu den falsch beantworteten Fragen.“).

Zusammenarbeit Lehrer-KI: Am Ende sollte die Lehrkraft immer das letzte Wort haben. GUSTAV kann KI-Vorschläge anzeigen (auch in Lehrer-Ansicht), z.B. eine KI-generierte Musterlösung oder eine Einschätzung der Klassenleistung. Der Lehrer nutzt dies zur Vorbereitung des Unterrichts (z.B. KI erkennt, dass 80% der Klasse Frage 4 falsch hatten → Lehrer geht dieses Thema nochmal durch). Hier wird KI zum Assistenten in der Nachbereitung.

Abschließend: Die Integration von KI-Feedback in H5P-Tasks soll gezielt dort passieren, wo menschliches Feedback sonst stark gefordert wäre (Essays, freies Problemlösen). Wo H5P klare richtige Antworten kennt, verlassen wir uns auf das unmittelbare, eindeutige Feedback des Systems
discuss.openedx.org
(xAPI liefert Score, die Platform markiert entsprechend bestanden
discuss.openedx.org
). Diese kombinierte Strategie gewährleistet, dass Schüler sowohl schnelles Input-Feedback bekommen, als auch tiefergehendes, adaptives Feedback für Lernfortschritt und Verständnis.
Minimaler PoC-Backlog (2–3 Wochen)

Um die Integration prototypisch zu demonstrieren, fokussieren wir einen Vertical Slice, der die wichtigsten Komponenten abdeckt:

    Datenmodell & API anpassen: Erweiterung des OpenAPI-Schemas für Task-Verwaltung, um Task.kind (Enum) und H5P-spezifische Felder aufzunehmen. Z.B. POST /tasks erlaubt kind: "h5p" und einen Block h5p_reference mit contentId etc. Backend (z.B. Python FastAPI oder Node) implementiert diese neuen Felder, inkl. Validation (z.B. contentId muss existieren). Schreibrechte nur für Autoren. Ein erster Migrationsschritt in der DB legt die nötigen Spalten/Tables an.

    H5P-Server einbinden: Aufsetzen von @lumieducation/h5p-server in unserer Dev-Umgebung. Für den PoC kann der Storage einfach im Filesystem erfolgen (kleinere Inhalte, Single-Node). Wir konfigurieren einen Express Router via h5pAjaxExpressRouter
    docs.lumi.education
    , der alle benötigten H5P-Ajax-Endpunkte bereitstellt (Content-Bibliotheken laden, ContentState speichern, etc.). Basis-Auth via JWT (z.B. Express Middleware, die JWT prüft, bevor sie an h5pAjax weitergibt). Wir integrieren auch die ClamAV-Scanner-Middleware in den Upload-Flow.

    Editor-UI (Autoren) minimal: Eine Seite in GUSTAV (z.B. /author/h5p/new) mit dem <h5p-editor> Element. Der Editor lädt zunächst den Content-Type-Browser (Bibliothekenliste). Für PoC beschränken wir uns auf 1–2 Content-Typen (z.B. Multiple Choice und Interactive Video) – d.h. diese Libraries müssen wir initial installieren. Test: Autor kann eine einfache MC-Frage erstellen, speichern -> es entsteht ein H5P-Content-Eintrag in DB/FS. Rückmeldung an Autor: Content ID.

    Task anlegen mit H5P: UI für Lehrkraft, um einen existierenden H5P-Content einem Task zuzuordnen. Im PoC einfach: Nach dem Erstellen im Editor notiert sich der Autor die Content-ID und trägt sie beim Erstellen des Tasks ein. (Später besser via Dropdown-Liste oder direkt aus Editor -> Task, aber das geht über MVP hinaus). Speichern Task mit instruction_md etc.

    Studenten-Player-Flow: Implementieren der Task-Anzeige-Seite für Schüler: Zeige Instruction, dann <h5p-player content-id=...> entsprechend Task, darunter (vorerst statisch) einen „Zeige Hinweis“-Button und Kriterien Abschnitt. Für PoC-Happy-Path reicht: Instruction und Player werden angezeigt, kein Hints/Criteria-Interaktion nötig. Die Lade-Callback des Web Components ruft unsere API /h5p/content/<id> auf, holt PlayerModel und rendert. Test: Schüler sieht die MC-Frage, kann sie beantworten, bekommt Feedback „richtig/falsch“ vom H5P und „Retry“ falls erlaubt.

    xAPI Logging: In den Web-Component Event-Listener einhängen: on xAPI -> AJAX POST zu /analytics/xapi mit dem Statement. Server schreibt in xapi_events Tabelle (JSONB). Im PoC reicht es, final das Completed-Statement zu speichern. Prüfung: Nach Abschluss der MC-Frage taucht ein Eintrag in DB auf mit score.

    Score-Mapping: Bei Completed-Event triggert Server zusätzlich Logik: Ein TaskAttempt-Record erstellen oder aktualisieren. Im PoC: Legen wir eine Tabelle task_attempts(user, task, attempt_no, score, max_score, completed_at) an. Das Completed xAPI liefert Score 1/1 oder 0/1, wir berechnen % und setzen evtl. passed boolean (score==max?). Task in Demo hat max_attempts=∞ erstmal.

    Dashboard-Kachel: Als Proof-of-Concept ein einfaches Diagramm oder Kennzahl. Z.B. auf Lehrer-Dashboard: „Durchschnittsscore für Task XY = 80%“. Das kann per SQL aus task_attempts berechnet werden. Oder ein kleines Balkendiagramm: Score-Verteilung (z.B. 5 SuS 100%, 2 SuS 0%). PoC: statisch generieren mit Chart.js im Frontend, Daten per API laden. Wichtig ist zu zeigen, dass Analytics funktioniert. Alternativ oder zusätzlich: Anzeigen der xAPI-Log-Rohdaten für Developer-Insight.

    Versuchslimit & Reset (basic): Setzen von max_attempts bei Task (z.B. 2) und Implementation: Wenn ein Schüler Completed hat und attempts < max, dann „Neustart“-Button einblenden, der einfach die Seite refresht mit neuem contextId (im PoC können wir contextId random generieren ohne Persistenz). Wenn attempts == max, dann keine Neustart-Möglichkeit -> evtl. Meldung „Keine weiteren Versuche“. Due Date Logik kann im PoC hardcoded simuliert werden (z.B. setzen wir ein vergangenes Datum und schauen, dass dann „Task abgelaufen“ erscheint statt Player).

    Testing: Erstellen eines automatisierten Tests (pytest/selenium oder Playwright). Z.B.:

        Setup: Autor erstellt MC-Inhalt „2+2?“ mit Antwort 4. Task angelegt.

        Schüler A öffnet Task, beantwortet falsch -> H5P feedback „falsch“, nutzt Retry, beantwortet richtig -> Completed.

        Prüfen: task_attempts hat 1 Eintrag (score 1/1). xapi_events hat mind. 1 Completed und 1 Answered.

        Schüler B macht nur falsch und gibt auf -> Completed (0/1).

        Lehrer-View: via API holt average -> 50%.
        Dieser Test stellt sicher, dass Kernfunktionen laufen.

Abgrenzung im MVP: Nicht alles oben gesagte wird im PoC voll umgesetzt. Fokus liegt auf dem Durchstich: H5P-Inhalt laden, Versuchslogik basic, Score erfassen, Anzeige für minimalen Analytics-Fall. Hints und Criteria können erstmal nur als statischer Text erscheinen, ohne gestufte Freischaltung oder auto-Auswertung. KI-Integration ist MVP-Out-of-scope. Security: Im PoC evtl. JWT-Schutz vereinfachen (z.B. globaler dev-Schalter), aber Konzept nachweisen (z.B. zwei Rollen testen: Schüler kann Editor nicht laden, Autor kann).
Risiken und Trade-offs

Wie bei jeder größeren Integration gibt es Risiken und abzuwägende Kompromisse:

    Wartungsaufwand (Libraries & Server): Die lumieducation H5P-NodeJS-Library ist aktiv, aber wir sind abhängig von deren Updates für Sicherheit und neue H5P-Versionen. Das erfordert, dass wir im Team Know-how aufbauen, die H5P-Core-Updates einzupflegen
    docs.lumi.education
    . Ein Trade-off wäre, statt Self-Hosting doch ein bestehendes Plugin (Moodle, LTI) zu verwenden – aber das passt nicht zu unseren Anforderungen (KISS, Integrationstiefe). Wir mitigieren das Risiko durch isolierte Implementierung: der H5P-Teil ist modular, könnte theoretisch ausgetauscht werden, falls lumieducation-Projekt stockt. Bislang (Stand 2025) ist es jedoch recht stabil (v10.x) und in Production bei Projekten im Einsatz. Notfalls könnten wir auf die PHP-H5P-Implementierung ausweichen, aber das bräche mit unserer Tech-Stack.

    Komplexität für Lehrende: H5P-Editor ist mächtig, aber auch komplex, v.a. bei verschachtelten Inhalten. Risiko: Überforderung der Lehrkräfte, Inkompatibilität mit KISS-Prinzip der Plattform. Wir minimieren das, indem wir Guidelines und Schulungen anbieten (z.B. welche 5 Content-Typen man zuerst ausprobieren soll). Auch könnten wir eine Bibliotheks-Auswahl einschränken: nur didaktisch hochwertige, erprobte Typen standardmäßig aktivieren (z.B. Quiz, D&D, Interactive Video, Essay, Branching) und exotischere (Timeline, Personality Quiz etc.) ausblenden, bis Bedarf entsteht.

    Netzwerk-Bedingungen: In Schulnetzen mit geringer Bandbreite könnten große H5P-Inhalte (v.a. Videos) zum Problem werden. Außerdem haben manche Schulen strikte Whitelists für Web-Traffic. Unsere Self-hosted Lösung versucht, offline zu bleiben, aber z.B. YouTube-Videos werden blockiert sein (wir blocken die auch per CSP). Lösung: Schulung, dass Videos lokal eingebunden werden. Zudem sollten wir Content-Größen optimieren: Videos komprimieren, Bilder nicht übermäßig groß. Wir könnten optional eine Preload-Funktion anbieten: z.B. der Lehrer preloaded morgens alle Videos in den Klassenraum-Cache (evtl. Browser-Cache oder zentraler Proxy). Trade-off: zusätzlicher Implementationsaufwand vs. flüssigere Nutzung.

    Barrierefreiheit: Obwohl wir A11y berücksichtigen, bleibt ein Restrisiko, dass einzelne H5P-Typen nicht vollständig barrierefrei sind. Das könnte uns im schlimmsten Fall zu Accessibility-Beschwerden führen. Wir begegnen dem, indem wir von vornherein in der Doku deklarieren, welche Inhalte nicht für alle geeignet sind und Alternativen anbieten. Unser Testing mit Hilfsmitteln muss kontinuierlich sein. Wenn z.B. WCAG 2.2 neue Anforderungen bringt (Drag&Drop könnte dann als non-compliant gelten), müssten wir ggf. temporär diesen Typ deaktivieren oder auf Fixes der H5P-Community hoffen. Das ist ein Trade-off zwischen Feature-Vielfalt und Inklusion. Wir entscheiden uns klar für Inklusion: Lieber eine Interaktivität weniger anbieten als einen Schüler ausschließen. Daher wird die Devise sein: Standard-Content-Typen nur, wenn A11y gewährleistet; spielerische Typen (Memory, Agamotto etc.) nur gezielt und mit Absicherung.

    Datenschutz: Speicherung von detaillierten Leistungsdaten (xAPI logs) birgt das Risiko, bei unsachgemäßer Handhabung gegen Datenschutz zu verstoßen. Insbesondere, wenn Analysen personenbezogen erstellt werden (Lernverläufe etc.), müssen wir sicherstellen, dass nur Berechtigte (Lehrer, Schüler selbst) Zugriff haben. Supabase RLS deckt das technisch ab. Dennoch müssen wir im Datenschutzkonzept klar beschreiben, was wir speichern (z.B. Interaktionen inkl. falscher Antworten) und wie lange. Ein mögliches Trade-off: Granularität vs. Speicherbedarf/Privacy. Wir könnten entscheiden, die ausführlichen xAPI-Events nach X Monaten zu löschen oder zu anonymisieren (nur noch aggregate Stats behalten). Wir werden hier auf Rückmeldung der Schulen und Datenschutzbeauftragten achten.

    Security (Code Injection): H5P erlaubt in manchen Content-Typen benutzerdefiniertes HTML oder Script (z.B. Embedder). Unsere Maßnahmen (CSP, Sanitizer) sollten das abfangen. Aber falls es eine unbekannte Lücke gibt, könnte ein Angreifer versuchen, z.B. in einem H5P-Textfeld bösartigen JS unterzubringen, der dann im Browser des Schülers ausgeführt wird (z.B. Session Hijacking). Wir vertrauen den Sanitizing-Funktionen, müssen aber wachsam sein. Als zusätzliche Schicht könnten wir eine Review-Pflicht für von Lehrern erstellte Inhalte einführen (z.B. Admin schaut drüber), aber das skaliert nicht gut. Stattdessen setzen wir Logging: Sollten CSP-Reports auftauchen, schauen wir sofort nach dem entsprechenden Content. Eine weitere Überlegung: Uploads auf bestimmte MIME types beschränken (z.B. keine HTML-Dateien als Assets). Dieser Balanceakt zwischen Freiraum für Autoren und Sicherheit werden wir mit Policies lösen (in Schulungen: „Betten Sie keinen fremden Code ein!“ etc.).

    Feature Scope Creeping: H5P bringt sehr viele Möglichkeiten, Gefahr ist, dass wir versuchen, alles zu implementieren (LRS vollumfänglich, alle xAPI auswerten, gamification etc.) und dabei Kernfeatures vernachlässigen. Daher im Rollout klare Prioritäten: Zuerst Stabilität, Kern-Content-Typen, grundlegende Analytics. Fancy Features wie Live-Collaboration in H5P (geht mit Multi-User Addon) oder AR/VR-Inhalte sind nicht sofort nötig. Wir definieren eine Roadmap, um solchen Verlockungen zu widerstehen, bis Grundfunktionen solide sind.

    Lehrkräfte-Workflow: Einführung von H5P bedeutet auch Änderung im Workflow der Lehrkräfte. Einige könnten mit dem Editor hadern oder es entsteht Mehraufwand beim Erstellen interaktiver Inhalte vs. herkömmlicher PDF-Aufgaben. Um den Mehrwert zu heben, müssen wir Erfolgsbeispiele liefern und Support bieten (evtl. Vorlagen anbieten, OER-H5Ps teilen). Sonst besteht das Risiko, dass H5P-Feature ungenutzt bleibt oder nur wenige Enthusiasten es nutzen. Das wäre zwar kein technisches Versagen, aber didaktisch/politisch schade. Also planen wir Pilotphasen mit freiwilligen Lehrern, sammeln Feedback, verbessern Usability (vielleicht vereinfachte Oberfläche für häufige Typen – z.B. ein „Quick Quiz“ Creator).

Fazit: Die Integration von H5P in GUSTAV bietet enormes Potential an Interaktivität und Daten für personalisiertes Lernen
discuss.openedx.org
discuss.openedx.org
, erfordert aber sorgfältige Umsetzung und ständige Wartung. Mit dem skizzierten Architekturvorschlag und didaktischen Leitlinien stellen wir sicher, dass technische und pädagogische Aspekte Hand in Hand gehen. Durch iterative Entwicklung (PoC → Pilot → Rollout) und bewusste Risikoabfederung (Security, A11y, Training) werden wir H5P erfolgreich und nachhaltig in GUSTAV integrieren.
