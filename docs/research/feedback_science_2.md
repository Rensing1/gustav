# GUSTAV Feedback Engine: Ein didaktisch fundiertes und technisch realisierbares Konzept


## Einleitung: Von der Feedback-Forschung zur technischen Implementierung

Die Entwicklung der Lernplattform GUSTAV steht vor einer zentralen Herausforderung, die sowohl p√§dagogischer als auch technischer Natur ist: die Skalierung von qualitativ hochwertigem, formativem Feedback. Die p√§dagogische Dringlichkeit dieses Vorhabens ist unbestritten. Die Forschung von Hattie, Shute und Narciss hat wiederholt gezeigt, dass wirksames Feedback zu den einflussreichsten Interventionen zur Steigerung von Lernleistungen z√§hlt.<sup>1</sup> Es beantwortet die drei fundamentalen Fragen des Lernenden: "Wo stehe ich?" (Feed Up), "Wie geht es voran?" (Feedback) und "Wo geht es als N√§chstes hin?" (Feed Forward). Die gr√∂√üte H√ºrde f√ºr die Umsetzung im Schulalltag ist der immense Zeitaufwand f√ºr Lehrkr√§fte. GUSTAV zielt darauf ab, diese L√ºcke durch k√ºnstliche Intelligenz zu schlie√üen.

Dieses Ziel muss jedoch unter einer signifikanten technischen Einschr√§nkung erreicht werden: dem Einsatz von relativ kleinen, lokal gehosteten Large Language Models (LLMs) mit etwa 8 Milliarden Parametern. Diese Entscheidung ist aus Gr√ºnden des Datenschutzes, der Kostenkontrolle und der Unabh√§ngigkeit von externen Anbietern strategisch klug. Gleichzeitig limitiert sie die verf√ºgbare Rechenleistung, die Komplexit√§t der m√∂glichen Inferenzketten und vor allem die Gr√∂√üe des Kontextfensters, also die Menge an Informationen, die das Modell gleichzeitig verarbeiten kann.<sup>5</sup> Die Qualit√§t des generierten Feedbacks h√§ngt somit in au√üergew√∂hnlichem Ma√üe von einer intelligenten, effizienten und pr√§zisen Bereitstellung von Kontext ab.

Dieses Dokument legt ein umfassendes, evidenzbasiertes Konzept f√ºr die Feedback-Engine von GUSTAV vor. Es navigiert gezielt im Spannungsfeld zwischen den hohen p√§dagogischen Idealen der Feedback-Forschung und den pragmatischen Realit√§ten kleiner, lokaler LLMs. Es dient dem Entwicklungsteam als strategische und technische Entscheidungsgrundlage f√ºr die Architektur und Implementierung eines Systems, das Lehrkr√§fte entlastet und Sch√ºlern einen echten, lernf√∂rderlichen Mehrwert bietet.


## A. Fundamentale Feedback-Strategie: Einmalig vs. Interaktiv

Die Wahl der grundlegenden Interaktionsform zwischen Sch√ºler und KI ‚Äì ein einmaliger, umfassender Bericht oder ein schrittweiser, dialogischer Prozess ‚Äì ist eine der wichtigsten Architekturentscheidungen. Die optimale Strategie ist keine bin√§re Entscheidung, sondern eine kontextabh√§ngige, die sich an der Art der Aufgabe und dem spezifischen Lernziel orientieren muss.


### 1. P√§dagogische Vor- und Nachteile: Eine differenzierte Analyse

Die p√§dagogische Wirksamkeit von Feedback h√§ngt stark von seiner Darreichungsform ab. Je nach Komplexit√§t der Aufgabe und dem angestrebten Lernziel sind unterschiedliche Modi √ºberlegen.

Der Fall f√ºr einmaliges (statisches) Feedback:

F√ºr Aufgaben mit geringer intrinsischer kognitiver Last, wie Wissensabfragen (z.B. Vokabeltests, historische Daten), das Anwenden einfacher Formeln oder das Befolgen klar definierter Prozeduren, ist ein einmaliges, unmittelbares Feedback √§u√üerst effektiv. Es erf√ºllt mehrere Schl√ºsselfunktionen:



* **Schnelle Korrektur:** Es korrigiert Fehlkonzeptionen, bevor sie sich im Ged√§chtnis verfestigen k√∂nnen. Studien belegen, dass Sch√ºler, die unmittelbares Feedback erhalten, signifikant besser abschneiden als jene mit verz√∂gertem oder keinem Feedback.<sup>8</sup>
* **Effizienz:** Der Prozess ist sowohl f√ºr den Sch√ºler als auch f√ºr das System ressourcenschonend. Der Sch√ºler erh√§lt alle relevanten Informationen geb√ºndelt und kann die Aufgabe abschlie√üen.
* **Klarheit:** Bei geschlossenen Aufgaben mit einer klaren Richtig/Falsch-Dimension bietet ein statischer Bericht eine unmissverst√§ndliche R√ºckmeldung.

Dieses Vorgehen entspricht in Hattie & Timperleys Modell prim√§r dem Feedback auf der Aufgabenebene (*task level*), das sich auf die Korrektheit der L√∂sung konzentriert (z.B. "Deine Antwort ist nicht korrekt. Das richtige Datum ist 1066, da die Schlacht von Hastings in diesem Jahr stattfand.").<sup>2</sup>

Der Fall f√ºr interaktives (dialogisches) Feedback:

F√ºr komplexe, offene und schlecht definierte Aufgaben wie das Verfassen von Essays, das Entwickeln einer Argumentation, das L√∂sen mehrstufiger Probleme oder kreative Schreibprozesse ist ein interaktiver Ansatz p√§dagogisch weit √ºberlegen. Er transformiert den Lernprozess von einer passiven Informationsaufnahme zu einer aktiven Auseinandersetzung.9 Die Vorteile sind vielf√§ltig:



* **F√∂rderung von Denkprozessen:** Anstatt eine fertige Korrektur zu pr√§sentieren, kann die KI als sokratischer Tutor agieren. Sie stellt gezielte Fragen, die den Sch√ºler zum Nachdenken anregen und ihn dabei unterst√ºtzen, eigene L√∂sungen zu entwickeln. Dies f√∂rdert kritisches Denken und Probleml√∂sekompetenzen.<sup>9</sup>
* **Scaffolding und Prozessbegleitung:** Der Dialog erm√∂glicht es der KI, den Lernprozess zu begleiten und gezielte Hilfestellungen (Scaffolding) anzubieten, die genau auf den aktuellen Bedarf des Sch√ºlers zugeschnitten sind.<sup>11</sup> Das Feedback kann so von der reinen Aufgabenebene auf die Prozess- und Selbstregulationsebene gehoben werden ("Du hast eine klare These formuliert. Welche Belege aus dem Text k√∂nntest du nutzen, um dein zweites Argument zu untermauern?").<sup>2</sup>
* **Verbesserte Wissensverankerung:** Aktive Lernmethoden, zu denen ein interaktiver Feedback-Dialog z√§hlt, f√ºhren nachweislich zu einer besseren und nachhaltigeren Wissensverankerung als passive Methoden wie das reine Lesen eines Berichts.<sup>12</sup>

Allerdings birgt der interaktive Ansatz eine subtile psychologische Herausforderung. Untersuchungen zeigen, dass Studierende in aktiven Lernumgebungen zwar objektiv mehr lernen, aber subjektiv das *Gef√ºhl* haben, weniger zu lernen.<sup>12</sup> Dies liegt am h√∂heren kognitiven Aufwand, der f√ºr die aktive Auseinandersetzung erforderlich ist. W√§hrend statisches Feedback passiv konsumiert wird, erfordert ein Dialog die Verarbeitung der KI-R√ºckmeldung, die Formulierung einer eigenen Antwort oder √úberarbeitung und die erneute Interaktion. Dieser "produktive Kampf" kann f√§lschlicherweise als Zeichen f√ºr ineffektives Lernen interpretiert werden und zu Frustration f√ºhren. F√ºr GUSTAV bedeutet dies, dass der interaktive Modus nicht nur eine simple Chat-Oberfl√§che sein darf. Die KI muss sorgf√§ltig darauf ausgelegt sein, diesen kognitiven Aufwand zu managen, indem sie den Prozess transparent macht und den Sch√ºler ermutigt, z.B. durch metakognitive Einw√ºrfe wie: "Das ist eine anspruchsvolle Frage. Schritt f√ºr Schritt kommen wir der L√∂sung n√§her. Was w√§re dein erster Gedanke dazu?".


### 2. Technische Implikationen eines interaktiven Ansatzes

Ein interaktiver Ansatz stellt deutlich h√∂here Anforderungen an die technische Architektur, insbesondere an die Datenhaltung im Supabase-Backend. Ein einfaches Speichern von Chat-Protokollen, wie es in vielen Conversational-AI-Anwendungen √ºblich ist <sup>14</sup>, ist hier unzureichend. Der Fokus muss auf der Nachverfolgung der Evolution der Sch√ºlerarbeit liegen, da das Feedback immer im Kontext einer spezifischen Version dieser Arbeit steht.

Die Datenbankarchitektur muss daher versionszentriert sein. Der Dreh- und Angelpunkt ist nicht die Konversation selbst, sondern die eingereichte Sch√ºlerl√∂sung in ihren verschiedenen Fassungen. Eine einfache Verkn√ºpfung von Nachrichten mit einem Benutzer w√ºrde den Bezug zur jeweiligen Arbeitsversion verlieren. Stattdessen muss die Datenbank in der Lage sein, den gesamten iterativen Prozess abzubilden: Ein Sch√ºler reicht Version 1 ein, erh√§lt dazu Feedback in einer Sitzung, √ºberarbeitet seine L√∂sung und reicht Version 2 ein, was eine neue Feedback-Sitzung ausl√∂sen kann.

Eine robuste und skalierbare Datenbankstruktur in Supabase k√∂nnte wie folgt aussehen:



* **assignments**: Speichert die von der Lehrkraft erstellten Aufgaben.
    * assignment_id (PK), teacher_id, title, description, learning_materials_url, feedback_config (JSONB, speichert die Lehrereinstellungen aus Teil A.3 und B.2).
* **submissions**: Speichert jede einzelne Einreichung eines Sch√ºlers. Dies ist die zentrale Tabelle f√ºr die Versionierung.
    * submission_id (PK), student_id, assignment_id, version_number (Integer, z.B. 1, 2, 3), content (Text), submitted_at (Timestamp).
* **feedback_sessions**: Kapselt eine Interaktionssitzung, die sich auf eine bestimmte Einreichung bezieht.
    * session_id (PK), submission_id (FK zu submissions), started_at (Timestamp).
* **feedback_messages**: Speichert die einzelnen Nachrichten innerhalb einer Sitzung.
    * message_id (PK), session_id (FK zu feedback_sessions), parent_message_id (FK zu sich selbst, f√ºr Threading), sender_type (Enum: 'student', 'ai'), message_content (Text), timestamp.

Diese Struktur stellt sicher, dass jede Feedback-Nachricht eindeutig einer Sitzung und damit einer spezifischen Version der Sch√ºlerarbeit zugeordnet werden kann. Dies erm√∂glicht eine l√ºckenlose Rekonstruktion des Lernprozesses f√ºr die Lehrkraft und liefert der KI den notwendigen historischen Kontext f√ºr nachfolgende Interaktionen.<sup>16</sup>


### 3. Klare Empfehlung und Konfiguration durch die Lehrkraft

GUSTAV sollte beide Modi ‚Äì einmaliges und interaktives Feedback ‚Äì unterst√ºtzen, um der Vielfalt p√§dagogischer Szenarien gerecht zu werden. Die Wahl des Modus sollte der Lehrkraft √ºberlassen werden, da sie die Lernziele und die Eignung der Aufgabe am besten beurteilen kann. Die Konfiguration im Frontend muss dabei so einfach und intuitiv wie m√∂glich gestaltet sein und sich an bew√§hrten UI-Mustern f√ºr Lernmanagementsysteme (LMS) orientieren, die auf Klarheit und Reduzierung der kognitiven Last f√ºr die Lehrkraft abzielen.<sup>18</sup>

**UI/UX-Vorschlag f√ºr die Aufgabenkonfiguration:**

Im Einstellungsbereich f√ºr eine neue Aufgabe sollte die Lehrkraft eine klare, nicht-technische Wahlm√∂glichkeit erhalten:



---
**Feedback-Modus f√ºr GUSTAV-KI**

W√§hlen Sie, wie Sch√ºler Feedback zu dieser Aufgabe erhalten sollen.

üîò **Einmaliger Bericht**

Der Sch√ºler reicht seine Arbeit einmal ein und erh√§lt einen vollst√§ndigen Feedback-Bericht. Ideal f√ºr Wissens√ºberpr√ºfungen, Tests oder finale Abgaben.

üîò **Interaktiver Dialog**

Der Sch√ºler kann in einen Dialog mit der KI treten, um seine Arbeit schrittweise zu verbessern. Ideal f√ºr Entw√ºrfe, kreatives Schreiben und komplexe Probleml√∂sungen.



---
Diese einfache Konfiguration erm√∂glicht es der Lehrkraft, mit einem Klick die p√§dagogisch passende Feedback-Strategie f√ºr die jeweilige Aufgabe festzulegen.

Die folgende Tabelle fasst die Analyse zusammen und dient als Entscheidungshilfe f√ºr die Implementierung.

**Tabelle A.1: Vergleich der Feedback-Modalit√§ten**


<table>
  <tr>
   <td>Kriterium
   </td>
   <td>Einmaliges (Statisches) Feedback
   </td>
   <td>Interaktives (Dialogisches) Feedback
   </td>
  </tr>
  <tr>
   <td><strong>P√§dagogisches Ziel</strong>
   </td>
   <td>Schnelle Korrektur, Wissenssicherung, summative Bewertungsvorbereitung
   </td>
   <td>Prozessbegleitung, F√∂rderung von Denkprozessen, Selbstregulation, formative Entwicklung
   </td>
  </tr>
  <tr>
   <td><strong>Typische Aufgabentypen</strong>
   </td>
   <td>Wissensabfragen, Rechenaufgaben, L√ºckentexte, definierte Prozeduren
   </td>
   <td>Essay-Entw√ºrfe, Argumentationsanalysen, kreatives Schreiben, komplexe Probleml√∂sungen
   </td>
  </tr>
  <tr>
   <td><strong>Kognitive Belastung (Sch√ºler)</strong>
   </td>
   <td>Gering; passive Aufnahme eines Berichts
   </td>
   <td>Hoch; erfordert aktive Verarbeitung, Reflexion und Reaktion
   </td>
  </tr>
  <tr>
   <td><strong>Sch√ºler-Engagement</strong>
   </td>
   <td>Gering bis mittel; reaktiv
   </td>
   <td>Hoch; aktiv und partizipativ
   </td>
  </tr>
  <tr>
   <td><strong>Umgang mit Fehlkonzeptionen</strong>
   </td>
   <td>Korrigiert das Endergebnis
   </td>
   <td>Kann den Denkfehler im Prozess identifizieren und korrigieren
   </td>
  </tr>
  <tr>
   <td><strong>Technische Komplexit√§t</strong>
   </td>
   <td>Gering; ein API-Aufruf, einfache Datenhaltung
   </td>
   <td>Hoch; erfordert Zustandsverwaltung, komplexe Datenbankstruktur, Konversationslogik
   </td>
  </tr>
</table>



## B. Steuerung von Umfang und Tiefe des Feedbacks

Eines der gr√∂√üten Risiken von automatisiertem Feedback ist die kognitive √úberlastung des Lernenden. Ein Sch√ºler, der gleichzeitig detaillierte R√ºckmeldungen zu Rechtschreibung, Satzbau, Argumentationsstruktur und Inhalt erh√§lt, kann diese F√ºlle an Informationen nicht effektiv verarbeiten. Die Folge ist, dass das Feedback ignoriert wird oder sogar demotivierend wirkt. Eine effektive Feedback-Engine muss daher Mechanismen zur Steuerung von Umfang und Tiefe des Feedbacks implementieren.


### 1. Analyse der kognitiven √úberlastung

Die **Cognitive Load Theory (CLT)** bietet den entscheidenden theoretischen Rahmen f√ºr das Verst√§ndnis dieses Problems.<sup>20</sup> CLT postuliert, dass unser Arbeitsged√§chtnis, der Ort der bewussten Informationsverarbeitung, eine sehr begrenzte Kapazit√§t hat.<sup>22</sup> Lernen findet statt, wenn Informationen aus dem Arbeitsged√§chtnis erfolgreich in das Langzeitged√§chtnis √ºbertragen und dort in bestehende Wissensstrukturen (Schemata) integriert werden.

CLT unterscheidet drei Arten von kognitiver Belastung <sup>23</sup>:



1. **Intrinsische Last:** Die dem Lerninhalt innewohnende Komplexit√§t. Das Erlernen der Grundrechenarten hat eine geringere intrinsische Last als das Verst√§ndnis der Quantenmechanik.
2. **Extrinsische Last:** Die Belastung, die durch die Art der Informationsdarbietung entsteht und nicht direkt dem Lernen dient. Unklare Anweisungen, eine √ºberladene Benutzeroberfl√§che oder eben zu umfangreiches, unstrukturiertes Feedback erzeugen eine hohe extrinsische Last.
3. **Germane Last:** Die "n√ºtzliche" Belastung, die durch die mentalen Anstrengungen entsteht, neue Informationen zu verstehen und Schemata im Langzeitged√§chtnis aufzubauen.

Das Ziel jeder didaktischen Gestaltung ‚Äì und somit auch der GUSTAV Feedback Engine ‚Äì muss es sein, die **extrinsische Last zu minimieren**, um m√∂glichst viel Kapazit√§t des Arbeitsged√§chtnisses f√ºr die **germane Last** freizuhalten.<sup>25</sup> Ein Feedback, das den Sch√ºler mit zu vielen Korrekturpunkten auf einmal konfrontiert, maximiert die extrinsische Last. Der Sch√ºler ist damit besch√§ftigt, die verschiedenen Hinweise zu sortieren und zu priorisieren, anstatt sich auf die eigentliche Verarbeitung und das Lernen zu konzentrieren.<sup>2</sup>


### 2. Mechanismen zur Steuerung: Lehrkraft vs. Sch√ºler

Um die kognitive Last zu managen, muss das System den Fokus und die Granularit√§t des Feedbacks steuern k√∂nnen. Die zentrale Frage ist, wer diese Steuerung aus√ºben sollte: die Lehrkraft, die den Lernprozess gestaltet, oder der Sch√ºler, der den Lernprozess durchl√§uft.

Lehrkraft-gesteuerte Steuerung (Teacher-Centered):

In diesem Modell legt die Lehrkraft bei der Erstellung der Aufgabe fest, auf welche Aspekte die KI achten soll (z.B. "Nur auf die Argumentationsstruktur achten", "Fokus auf Rechtschreibung und Grammatik").



* **Vorteile:**
    * **Didaktische Ausrichtung:** Das Feedback wird pr√§zise auf die Lernziele der jeweiligen Aufgabe ausgerichtet. Die Lehrkraft kann bewusst Schwerpunkte setzen, die dem aktuellen Lernstand der Klasse entsprechen.<sup>26</sup>
    * **Struktur und Klarheit:** Der Sch√ºler erh√§lt eine klare Orientierung und wird nicht von Aspekten abgelenkt, die f√ºr die aktuelle Aufgabe weniger relevant sind. Dies reduziert die extrinsische Last.
* **Nachteile:**
    * **Mangelnde Flexibilit√§t:** Ein Sch√ºler, der zwar an der Argumentationsstruktur arbeiten soll, aber grundlegende Probleme mit dem Satzbau hat, die ihn blockieren, erh√§lt keine Hilfe in diesem Bereich.
    * **Reduzierte Autonomie:** Der Sch√ºler wird in eine passive Rolle gedr√§ngt und hat keine M√∂glichkeit, selbst zu entscheiden, wo er Unterst√ºtzung ben√∂tigt. Dies kann die Entwicklung von Selbstregulations- und metakognitiven F√§higkeiten hemmen.

Sch√ºler-gesteuerte Steuerung (Student-Centered):

In diesem Modell kann der Sch√ºler selbst w√§hlen, zu welchen Aspekten er Feedback erhalten m√∂chte (z.B. "Gib mir nur einen Tipp zur Einleitung", "Pr√ºfe die Grammatik in diesem Absatz").



* **Vorteile:**
    * **F√∂rderung der Lernautonomie:** Der Sch√ºler wird zum aktiven Gestalter seines Lernprozesses. Er lernt, seine eigenen St√§rken und Schw√§chen zu reflektieren und gezielt nach Hilfe zu fragen. Dies ist ein Kernaspekt der Selbstregulation.<sup>28</sup>
    * **Bedarfsorientierung:** Das Feedback wird "just-in-time" und genau dort abgerufen, wo der Sch√ºler es ben√∂tigt und mental bereit ist, es zu verarbeiten.
* **Nachteile:**
    * **Fehleinsch√§tzung durch Novizen:** Insbesondere lernschw√§chere Sch√ºler k√∂nnen ihre eigenen Defizite oft nur unzureichend einsch√§tzen. Sie neigen dazu, sich auf oberfl√§chliche Fehler (z.B. Tippfehler) zu konzentrieren, w√§hrend sie grundlegende strukturelle Probleme √ºbersehen.
    * **Gefahr der Unterforderung:** Ein Sch√ºler k√∂nnte aus Bequemlichkeit nur nach einfachem Feedback fragen und die Auseinandersetzung mit komplexeren, anspruchsvolleren Aspekten meiden.

Die Debatte zwischen lehrer- und sch√ºlerzentrierten Ans√§tzen wird oft als Dichotomie dargestellt, doch in der Praxis ist eine Kombination oft am wirkungsvollsten.<sup>26</sup> Ein rein lehrergesteuertes System ignoriert die individuellen Bed√ºrfnisse des Sch√ºlers, w√§hrend ein rein sch√ºlergesteuertes System den Sch√ºler ohne die notwendige expertenbasierte F√ºhrung l√§sst.

Die L√∂sung liegt in einem **hybriden, zweistufigen Kontrollmodell**. Dieses Modell verbindet die didaktische F√ºhrung der Lehrkraft mit der prozessualen Autonomie des Sch√ºlers.



1. **Stufe 1 (Lehrkraft-Kontrolle): Definition des M√∂glichkeitsraums.** Die Lehrkraft definiert f√ºr jede Aufgabe die *verf√ºgbaren* Feedback-Dimensionen. Sie legt den "Lehrplan" f√ºr das Feedback fest und stellt sicher, dass dieser auf die Lernziele abgestimmt ist. Sie definiert sozusagen die Leitplanken, innerhalb derer sich der Sch√ºler bewegen kann.
2. **Stufe 2 (Sch√ºler-Kontrolle): Navigation im M√∂glichkeitsraum.** Der Sch√ºler w√§hlt aus den von der Lehrkraft freigegebenen Dimensionen aus, zu welchem Aspekt er *jetzt* Feedback erhalten m√∂chte und in welcher Tiefe (z.B. ein kurzer Hinweis vs. eine detaillierte Erkl√§rung). Dies erm√∂glicht es dem Sch√ºler, die kognitive Last selbst zu steuern und das Feedback in verdaubaren Portionen ("chunks") abzurufen, was dem Arbeitsged√§chtnis entgegenkommt.<sup>31</sup>

Dieses Modell schafft eine "Zone der proximalen Entwicklung" (Wygotski), in der die Lehrkraft den Rahmen f√ºr die Herausforderung vorgibt, w√§hrend die KI als anpassungsf√§higes Werkzeug (Scaffold) dient, das der Sch√ºler nach Bedarf einsetzen kann.


### 3. UI/UX f√ºr die Steuerung

Die Benutzeroberfl√§che muss dieses zweistufige Modell einfach und intuitiv abbilden.

Lehrer-Interface (bei der Aufgabenerstellung):

Die Lehrkraft ben√∂tigt eine einfache M√∂glichkeit, die Feedback-Fokusbereiche auszuw√§hlen. Ein komplexer Regel-Editor 33 w√§re hier kontraproduktiv. Eine Checkliste mit vordefinierten und benutzerdefinierten Optionen ist vorzuziehen.34



---
**GUSTAV AI Feedback-Fokus**

W√§hlen Sie die Aspekte aus, zu denen die KI Feedback geben darf.



* [‚úì] Rechtschreibung & Grammatik
* [‚úì] Argumentationsstruktur & roter Faden
* [‚úì] Klarheit der These
* [ ] Verwendung von Quellen & Zitaten
* [ ] Stil & Ausdruck
* [+] Eigenes Kriterium hinzuf√ºgen...



---
Sch√ºler-Interface (w√§hrend der Bearbeitung):

Die Darstellung h√§ngt vom gew√§hlten Feedback-Modus ab.



* **Im einmaligen Modus:** Der generierte Bericht wird durch die von der Lehrkraft gew√§hlten Fokusbereiche strukturiert.
* **Im interaktiven Modus:** Der Sch√ºler erh√§lt aktive Steuerungselemente. Anstatt nur einen Text einzugeben, kann er die KI gezielt anweisen:



---
*Sch√ºler gibt einen Absatz ein oder markiert einen Textabschnitt.*

**GUSTAV:** Was m√∂chtest du zu diesem Abschnitt wissen?

Argumentationsstruktur pr√ºfen Stil verbessern Grammatik checken

*Nach Auswahl, z.B. "Argumentationsstruktur pr√ºfen":*

**GUSTAV:** Okay, ich schaue mir die Argumentation an. Wie detailliert soll mein Feedback sein?


## Gib mir nur einen Tipp Zeige mir das Hauptproblem Gib mir eine ausf√ºhrliche Analyse

Dieses Design gibt dem Sch√ºler die Kontrolle √ºber das "Was" (innerhalb des von der Lehrkraft gesetzten Rahmens) und das "Wie viel" des Feedbacks und ist somit ein direktes Instrument zur Selbstregulation der kognitiven Last.


## C. Technische Umsetzung & KI-Architektur

Die erfolgreiche Implementierung der GUSTAV Feedback Engine mit kleinen, lokalen 8B-LLMs h√§ngt entscheidend von einer durchdachten KI-Architektur ab. Effiziente Kontextbereitstellung, eine robuste, mehrstufige Prompting-Strategie und der gezielte Einsatz des DSPy-Frameworks sind die drei S√§ulen dieser Architektur.


### 1. Kontextbereitstellung: Das A und O f√ºr kleine Modelle

Kleine LLMs sind im Vergleich zu ihren gro√üen Pendants deutlich empfindlicher gegen√ºber irrelevantem oder schlecht strukturiertem Kontext.<sup>5</sup> Die Qualit√§t des generierten Feedbacks ist eine direkte Funktion der Qualit√§t und Pr√§zision des Inputs.<sup>36</sup>



* **Minimaler vs. Optimaler Kontext:**
    * **Minimal notwendig:** Um √ºberhaupt eine rudiment√§re R√ºckmeldung geben zu k√∂nnen, ben√∂tigt die KI die Aufgabenstellung und die Sch√ºlerl√∂sung.
    * **Optimal:** F√ºr qualitativ hochwertiges, didaktisch wertvolles Feedback sind weitere Informationen unerl√§sslich: das explizite Lernziel der Aufgabe, detaillierte Bewertungskriterien (eine Rubrik), eine Musterl√∂sung als Referenz und, falls zutreffend, relevante Ausz√ºge aus den Lernmaterialien, auf die sich die Aufgabe bezieht.
* Strategie zur Definition des Feedback-Fokus: \
Die Art und Weise, wie die Lehrkraft die Bewertungskriterien und die Musterl√∂sung bereitstellt, hat einen direkten Einfluss auf die Pr√§zision des LLM-Prompts. Eine einzelne, flexible Textbox f√ºr den "Feedback-Fokus" ist zwar f√ºr die Lehrkraft einfach zu bedienen, birgt aber erhebliche Nachteile f√ºr die KI. Sie verleitet zu narrativen, unstrukturierten Eingaben, die f√ºr ein kleines LLM schwer zu parsen sind und das Signal-Rausch-Verh√§ltnis im Prompt verschlechtern. \
Eine weitaus robustere Methode ist die Verwendung **separater, strukturierter Datenbankfelder** f√ºr verschiedene Kontexttypen (z.B. evaluation_criteria, model_solution, learning_objective). Dieser Ansatz zwingt die Lehrkraft zu einer klareren, kategorisierten Eingabe. Diese strukturierten Daten k√∂nnen dann im Backend programmgesteuert zu einem hochoptimierten Prompt mit klaren Trennern und √úberschriften (z.B. ### Bewertungskriterien ###, ### Musterl√∂sung ###) zusammengesetzt werden. Dies maximiert die Klarheit und stellt sicher, dass das 8B-Modell seine begrenzten kognitiven Ressourcen auf die relevanten Informationen konzentrieren kann. Der geringf√ºgig h√∂here Aufwand bei der UI-Gestaltung wird durch eine signifikant h√∂here Zuverl√§ssigkeit und Qualit√§t des generierten Feedbacks mehr als aufgewogen.

**Tabelle C.1: Vergleich der Strategien zur Kontextbereitstellung**


<table>
  <tr>
   <td>Strategie
   </td>
   <td>Beschreibung
   </td>
   <td>Vorteile (Lehrkraft)
   </td>
   <td>Vorteile (KI-Pr√§zision)
   </td>
   <td>Nachteile
   </td>
   <td>Empfehlung f√ºr GUSTAV
   </td>
  </tr>
  <tr>
   <td><strong>Einzelnes, flexibles feedback_focus-Feld</strong>
   </td>
   <td>Ein einziges Textfeld, in das die Lehrkraft alle Anweisungen, Kriterien und Beispiele frei eingibt.
   </td>
   <td>Maximale Einfachheit, keine vorgegebene Struktur.
   </td>
   <td>Gering. Hohes Risiko f√ºr unklare, mehrdeutige oder "verrauschte" Prompts.
   </td>
   <td>Erfordert, dass das LLM die Intention der Lehrkraft aus unstrukturiertem Text interpretieren muss, was f√ºr kleine Modelle fehleranf√§llig ist.
   </td>
   <td><strong>Nicht empfohlen.</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Separate, strukturierte Datenbankfelder</strong>
   </td>
   <td>Dedizierte Felder f√ºr Bewertungskriterien, Musterl√∂sung, Lernziel etc.
   </td>
   <td>F√ºhrt die Lehrkraft zu pr√§ziseren Eingaben. Geringf√ºgig h√∂herer UI-Aufwand.
   </td>
   <td>Sehr hoch. Erm√∂glicht die Erstellung von sauberen, klar strukturierten Prompts mit hohem Signal-Rausch-Verh√§ltnis.
   </td>
   <td>Erfordert ein durchdachteres UI-Design f√ºr die Aufgabenerstellung.
   </td>
   <td><strong>Dringend empfohlen.</strong>
   </td>
  </tr>
</table>




* Umgang mit externem Material: Eine pragmatische RAG-light-Strategie: \
H√§ufig beziehen sich Aufgaben auf externe Texte (z.B. eine Kurzgeschichte, einen Sachtext), die das Kontextfenster eines 8B-Modells bei weitem sprengen w√ºrden. Hier ist eine "Retrieval-Augmented Generation" (RAG)-Strategie erforderlich. F√ºr GUSTAV wird eine leichtgewichtige, lokal ausf√ºhrbare "RAG-light"-Pipeline vorgeschlagen, deren Ziel nicht die Beantwortung von Fragen aus dem Dokument, sondern die gezielte Injektion von relevantem Kontext in den Feedback-Prompt ist. Leichte, CPU-freundliche RAG-Implementierungen sind mit Werkzeugen wie ChromaDB und effizienten Embedding-Modellen realisierbar.38 \
**Technische Pipeline (RAG-light):**
    1. **Indizierung (einmalig pro Aufgabe):** Wenn eine Lehrkraft ein Dokument (z.B. PDF, TXT) hochl√§dt, wird dieses serverseitig verarbeitet.
        * Mit einer Bibliothek wie UnstructuredLoader wird der Text extrahiert.<sup>38</sup>
        * Der Text wird mit einem RecursiveCharacterTextSplitter in handhabbare, sich √ºberlappende Abschnitte (Chunks) von z.B. 512 Tokens zerlegt.<sup>41</sup>
        * F√ºr jeden Chunk wird mit einem kleinen, lokalen Embedding-Modell (z.B. nomic-embed-text, bereitgestellt √ºber Ollama) ein Vektor-Embedding erzeugt.
        * Die Chunks und ihre Embeddings werden in einer lokalen ChromaDB-Vektordatenbank gespeichert, die mit der assignment_id verkn√ºpft ist.
    2. **Retrieval (bei jeder Feedback-Anfrage):**
        * Wenn ein Sch√ºler Feedback anfordert, wird eine Suchanfrage (Query) generiert. Diese kann aus der Aufgabenstellung und dem spezifischen Satz oder Absatz der Sch√ºlerl√∂sung bestehen, zu dem Feedback gew√ºnscht wird.
        * Mit dieser Query werden die Top-k (z.B. k=3) semantisch √§hnlichsten Chunks aus der ChromaDB-Sammlung abgerufen.
    3. **Injektion:**
        * Der Text dieser abgerufenen Chunks wird in den finalen LLM-Prompt unter einer klaren √úberschrift wie ### Relevante Ausz√ºge aus dem Lernmaterial ### eingef√ºgt.

Dieser Ansatz stellt sicher, dass das LLM sein Feedback auf die relevanten Passagen des Quellenmaterials st√ºtzen kann, ohne dass das Kontextfenster √ºberlastet wird.


### 2. Prompting-Strategie & Mehrstufigkeit

Eine einzelne, komplexe Anweisung an ein kleines LLM f√ºhrt oft zu unzuverl√§ssigen Ergebnissen. Die Zerlegung komplexer Aufgaben in eine Kette einfacherer, spezialisierter Schritte ist eine grundlegende Technik des Prompt Engineering und f√ºr die Arbeit mit 8B-Modellen unerl√§sslich.<sup>7</sup>



* Best√§tigung des mehrstufigen Prozesses (Analyse ‚Üí Feedback): \
Der vorgeschlagene Zwei-Schritt-Prozess ist die robusteste Vorgehensweise. Ein einziger Prompt, der ein 8B-Modell anweist, eine Sch√ºlerl√∂sung zu analysieren, Fehler zu identifizieren, diese gegen eine Rubrik zu bewerten UND dann ein didaktisch wertvolles, unterst√ºtzendes Feedback zu formulieren, ist kognitiv zu anspruchsvoll f√ºr das Modell. Die Trennung in zwei spezialisierte Aufgaben reduziert die Komplexit√§t jedes einzelnen Schrittes drastisch:
    1. **Analyse-Schritt:** Ein rein logischer, analytischer Task. Das Modell konzentriert sich darauf, die Sch√ºlerl√∂sung mit den Kriterien abzugleichen und seine Ergebnisse in einem strukturierten Format (JSON) auszugeben.
    2. **Feedback-Generierungs-Schritt:** Ein kreativer, sprachlicher Task. Das Modell erh√§lt die strukturierte Analyse als Input und konzentriert sich ausschlie√ülich darauf, diese in eine p√§dagogisch wertvolle, sprachlich angemessene Form zu bringen, die der definierten Persona entspricht.

    Diese Modularit√§t erh√∂ht nicht nur die Zuverl√§ssigkeit, sondern erleichtert auch das Debugging und die sp√§tere Optimierung mit DSPy.

* Entwicklung robuster Basis-Prompts: \
Die folgenden Prompts dienen als Vorlagen. Sie werden dynamisch mit den kontextuellen Informationen aus der Datenbank bef√ºllt. \
**Schritt 1: Analyse-Prompt (Basis)** \
Du bist ein pr√§ziser und objektiver Analyse-Assistent. Deine Aufgabe ist es, die Sch√ºlerl√∂sung ausschlie√ülich anhand der vorgegebenen Kriterien und der Musterl√∂sung zu bewerten. Deine Analyse muss streng faktenbasiert sein und sich auf die bereitgestellten Informationen st√ºtzen. Gib deine Analyse in einem strukturierten JSON-Format aus. \
 \
### Aufgabenstellung \
{{assignment_description}} \
 \
### Bewertungskriterien \
{{evaluation_criteria}} \
 \
### Musterl√∂sung (falls vorhanden) \
{{model_solution}} \
 \
### Relevante Ausz√ºge aus dem Lernmaterial \
{{retrieved_context_chunks}} \
 \
### Sch√ºlerl√∂sung \
{{student_answer}} \
 \
--- \
ANWEISUNG: \
Analysiere die Sch√ºlerl√∂sung Schritt f√ºr Schritt. Identifiziere f√ºr jedes Bewertungskriterium spezifische St√§rken und Schw√§chen. Zitiere f√ºr jeden Punkt w√∂rtlich den relevanten Teil der Sch√ºlerl√∂sung als Beleg. \
 \
Gib deine Ausgabe AUSSCHLIESSLICH als valides JSON-Objekt aus. Das Objekt soll zwei Schl√ºssel haben: "strengths" und "weaknesses". Jeder Schl√ºssel enth√§lt eine Liste von Objekten, wobei jedes Objekt die Felder "criterion" (das exakte Kriterium), "quote_from_solution" (das w√∂rtliche Zitat) und "analysis" (deine kurze, objektive Analyse) enth√§lt. Formuliere keine subjektiven Meinungen. \
 \
**Schritt 2: Feedback-Prompt (Basis)** \
Du bist GUSTAV, ein unterst√ºtzender, geduldiger und motivierender Lern-Coach f√ºr Sch√ºlerinnen und Sch√ºler der Sekundarstufe. Deine Tonalit√§t ist immer positiv, ermutigend und auf Augenh√∂he. Du sprichst den Sch√ºler direkt mit "Du" an. Dein Feedback ist IMMER formativ, spezifisch, handlungsorientiert und nicht-wertend. \
 \
Dein Ziel ist es, dem Sch√ºler zu helfen, die drei Kernfragen nach Hattie zu beantworten: \
1. Wo stehe ich? (Feed Up & Feedback) \
2. Wie geht es voran? (Feedback) \
3. Wo geht es als N√§chstes hin? (Feed Forward) \
 \
Basierend auf der folgenden strukturierten Analyse der Sch√ºlerarbeit, formuliere nun ein lernf√∂rderliches, dialogisches Feedback. \
 \
### Analyse der St√§rken und Schw√§chen \
{{analysis_json}} \
 \
--- \
ANWEISUNG: \
Formuliere das Feedback nach folgendem Schema: \
1.  **Positiver Einstieg:** Beginne mit einer spezifischen, positiven Beobachtung. W√§hle eine konkrete St√§rke aus der Analyse und erkl√§re, warum sie gut ist. (z.B. "Mir ist positiv aufgefallen, wie du...") \
2.  **Wichtigster Verbesserungspunkt:** Konzentriere dich auf EINEN zentralen Verbesserungspunkt aus der Analyse. Erkl√§re das Problem klar und verst√§ndlich. Vermeide wertende Sprache (nicht "das ist falsch", sondern "hier gibt es noch Potenzial f√ºr mehr Klarheit"). \
3.  **Konkreter n√§chster Schritt (Feed Forward):** Gib einen klaren, umsetzbaren Tipp oder stelle eine gezielte Frage, die dem Sch√ºler hilft, den n√§chsten Schritt zu gehen. (z.B. "Versuche doch mal, diesen Satz umzuformulieren, indem du...", "Welches Beispiel aus dem Text k√∂nnte dieses Argument noch st√§rker machen?"). \
4.  **Ermutigender Abschluss:** Schlie√üe mit einer motivierenden Bemerkung, die den Sch√ºler zum Weitermachen anregt. (z.B. "Du bist auf einem sehr guten Weg. Ich bin gespannt auf deine √úberarbeitung!"). \

* **Dynamische Anpassung:** Die Logik zur Anpassung an verschiedene Aufgabentypen (z.B. "Fasse zusammen" vs. "Beurteile") wird nicht im Prompt hartcodiert. Sie wird durch die evaluation_criteria gesteuert, die von der Lehrkraft bereitgestellt werden. Der Analyse-Prompt ist universell; er wendet die jeweils g√ºltigen Kriterien an. Dies macht das System flexibel und erweiterbar, ohne dass f√ºr jeden neuen Aufgabentyp ein neuer Prompt entwickelt werden muss.


### 3. Rolle von DSPy: Vom Programmieren zum Optimieren

DSPy ist das ideale Framework, um diese mehrstufige, kontextabh√§ngige Pipeline zu orchestrieren. Es erlaubt uns, die Logik deklarativ zu beschreiben und die m√ºhsame, manuelle Prompt-Optimierung durch einen algorithmischen Prozess zu ersetzen.<sup>44</sup>



* Startpunkt mit dspy.Predict und dspy.Module: \
Die Implementierung beginnt mit der Definition der beiden Schritte als separate dspy.Predict-Module, da dies der einfachste und modularste Ansatz ist.46 \
Python \
import dspy \
 \
class AnalyzeAnswer(dspy.Signature): \
    """Analyzes a student's answer based on criteria and context, providing a structured JSON output.""" \
    assignment_description = dspy.InputField() \
    evaluation_criteria = dspy.InputField() \
    student_answer = dspy.InputField() \
    retrieved_context_chunks = dspy.InputField(desc="Relevant snippets from learning material.") \
    analysis_json = dspy.OutputField(desc="A structured JSON with 'strengths' and 'weaknesses'.") \
 \
class GenerateFeedback(dspy.Signature): \
    """Generates formative feedback based on a structured analysis.""" \
    analysis_json = dspy.InputField() \
    formative_feedback = dspy.OutputField(desc="Supportive, actionable feedback for the student.") \
 \
class GustavFeedbackPipeline(dspy.Module): \
    def __init__(self): \
        super().__init__() \
        self.analyzer = dspy.Predict(AnalyzeAnswer) \
        self.feedback_generator = dspy.Predict(GenerateFeedback) \
 \
    def forward(self, assignment_description, evaluation_criteria, student_answer, retrieved_context_chunks): \
        analysis_result = self.analyzer( \
            assignment_description=assignment_description, \
            evaluation_criteria=evaluation_criteria, \
            student_answer=student_answer, \
            retrieved_context_chunks=retrieved_context_chunks \
        ) \
        feedback_result = self.feedback_generator(analysis_json=analysis_result.analysis_json) \
        return feedback_result \

* Evolution zu dspy.ChainOfThought: \
Insbesondere der Analyse-Schritt profitiert von einer expliziten Anweisung zum schrittweisen Denken. F√ºr komplexe Analysen, bei denen mehrere Kriterien gleichzeitig gepr√ºft werden m√ºssen, kann das dspy.Predict-Modul durch ein dspy.ChainOfThought-Modul ersetzt werden.45 Dies instruiert das LLM, seinen Denkprozess zu explizieren, bevor es das finale JSON generiert, was die Genauigkeit und Zuverl√§ssigkeit bei kleinen Modellen oft signifikant erh√∂ht. Die Signatur w√ºrde entsprechend angepasst: \
... -> reasoning, analysis_json.
* Vorbereitung f√ºr die Optimierung mit Telepromptern: \
Der entscheidende Vorteil von DSPy manifestiert sich in der Optimierungsphase.44 Um diese zu erm√∂glichen, muss von Beginn an ein qualitativ hochwertiger Datensatz aufgebaut werden.
    * **Datensatz-Erstellung:** Es muss ein Prozess etabliert werden, um Beispiele f√ºr exzellentes Feedback zu sammeln. Dies k√∂nnen von Lehrkr√§ften manuell erstellte oder validierte Feedback-Instanzen sein. Jedes Beispiel wird als dspy.Example-Objekt gespeichert und enth√§lt alle Eingabefelder (Aufgabe, Kriterien, Sch√ºlerl√∂sung) sowie die "goldenen" Ausgabefelder (analysis_json, formative_feedback).
    * **Optimierungsprozess:** Sobald ein kleiner Datensatz von 20-50 qualitativ hochwertigen Beispielen vorliegt, kann ein DSPy-Compiler (Teleprompter) wie BootstrapFewShot eingesetzt werden. Dieser Compiler testet verschiedene Kombinationen der Trainingsbeispiele als Few-Shot-Demonstrationen, um die effektivsten Prompts f√ºr die AnalyzeAnswer- und GenerateFeedback-Module zu finden. Er "kompiliert" das DSPy-Programm zu einer optimierten Version, die diese gelernten Demonstrationen automatisch in die Prompts einf√ºgt. Dieser algorithmische Ansatz ersetzt wochenlanges manuelles "Prompt-Tuning" und ist der Kern der DSPy-Philosophie.


## D. Entlastung der Lehrkr√§fte

Ein KI-gest√ºtztes Feedback-System entfaltet sein volles Potenzial erst, wenn es nicht nur die Korrekturarbeit erleichtert, sondern Lehrkr√§fte auch bei der Vorbereitung und Erstellung von Aufgaben aktiv unterst√ºtzt. GUSTAV kann hier durch einen intelligenten, hybriden Ansatz einen erheblichen Mehrwert schaffen.


### 1. Optionale Nutzung st√§rkerer LLMs zur Kriterienerstellung

Das Formulieren von klaren, pr√§zisen und didaktisch sinnvollen Bewertungskriterien (dem feedback_focus) ist eine anspruchsvolle und zeitintensive Aufgabe. W√§hrend ein lokales 8B-Modell gut darin ist, nach vorgegebenen Regeln Feedback zu geben, sind gr√∂√üere, leistungsf√§higere Modelle (wie GPT-4o oder Claude 3 Opus) deutlich √ºberlegen, wenn es um die kreative und nuancierte Aufgabe geht, solche Kriterien √ºberhaupt erst zu erstellen.<sup>49</sup>

Gleichzeitig ist der Einsatz von Cloud-basierten APIs f√ºr die Verarbeitung von Sch√ºlerdaten im europ√§ischen Schulkontext aus Datenschutzgr√ºnden (DSGVO/GDPR) h√∂chst problematisch und in der Regel nicht zul√§ssig.<sup>52</sup>

Diese beiden Aspekte lassen sich durch eine **hybride KI-Architektur** in Einklang bringen, die strikt zwischen Lehrer- und Sch√ºler-Workflows trennt:



* **Sch√ºler-Workflow (Lokal):** Jegliche Verarbeitung von Sch√ºlerl√∂sungen und die Generierung von Feedback an Sch√ºler erfolgt **ausschlie√ülich** √ºber das lokal gehostete 8B-LLM. Es werden zu keinem Zeitpunkt Sch√ºlerdaten an externe Server gesendet.
* **Lehrer-Workflow (Optional Cloud):** Bei der Erstellung einer neuen Aufgabe wird der Lehrkraft eine **optionale Assistenzfunktion** angeboten, z.B. ein Button mit der Beschriftung "Hilf mir bei der Erstellung der Bewertungskriterien".
    * Wenn die Lehrkraft diese Funktion aktiviert, sendet das System ausschlie√ülich die anonymen Aufgabendetails (z.B. Titel, Beschreibung, Fach, Klassenstufe) an eine externe, leistungsstarke LLM-API.
    * Die API generiert einen Vorschlag f√ºr eine Bewertungsrubrik oder eine Musterl√∂sung.
    * Dieser Vorschlag wird der Lehrkraft im Frontend angezeigt, die ihn dann bearbeiten, anpassen und **lokal** in der GUSTAV-Datenbank speichern kann.

**Technische und datenschutzrechtliche Implikationen:**



* **Technisch:** Diese Funktion erfordert eine serverseitige Integration mit einer externen API (z.B. OpenAI, Anthropic), ein sicheres Management von API-Schl√ºsseln und idealerweise Mechanismen zur Kostenkontrolle.
* **Datenschutz (DSGVO):** Die Umsetzung muss strengen Kriterien folgen:
    * **Opt-In:** Die Funktion darf nur auf expliziten Wunsch der Lehrkraft aktiviert werden.
    * **Transparenz:** Es muss eine klare und verst√§ndliche Information angezeigt werden, die dar√ºber aufkl√§rt, dass die (anonymen) Aufgabendaten zur Bearbeitung an einen externen Dienstleister (z.B. "OpenAI in den USA") gesendet werden.
    * **Keine Sch√ºlerdaten:** Es muss technisch sichergestellt sein, dass niemals personenbezogene Daten von Sch√ºlern in diesen API-Aufrufen enthalten sind.
    * **Datenverarbeitungsvertrag:** Der Betreiber der GUSTAV-Plattform (die Schule oder der Tr√§ger) sollte einen Auftragsverarbeitungsvertrag (AVV) bzw. ein Data Processing Addendum (DPA) mit dem API-Anbieter abschlie√üen, um die datenschutzrechtlichen Pflichten zu regeln.<sup>54</sup>

Dieser hybride Ansatz bietet das Beste aus beiden Welten: die volle Entlastung f√ºr Lehrkr√§fte durch State-of-the-Art-KI bei der Vorbereitung und die maximale Datensicherheit f√ºr Sch√ºler bei der Bearbeitung.


### 2. Weitere KI-gest√ºtzte Unterst√ºtzungsm√∂glichkeiten

Aufbauend auf derselben hybriden Architektur k√∂nnen Lehrkr√§ften weitere zeitsparende Werkzeuge angeboten werden, die die Akzeptanz und den Nutzen der Plattform weiter steigern <sup>55</sup>:



* **Generierung von Aufgabenvariationen:** Aus einer bestehenden Aufgabe kann die KI auf Knopfdruck alternative Fragestellungen oder Szenarien entwickeln, um z.B. f√ºr verschiedene Lerngruppen zu differenzieren.
* **Anpassung des Schwierigkeitsgrades:** Die KI kann eine Aufgabe f√ºr leistungsst√§rkere Sch√ºler anspruchsvoller formulieren oder f√ºr Sch√ºler mit F√∂rderbedarf vereinfachen ("Binnendifferenzierung").
* **Erstellung von Musterl√∂sungen:** Basierend auf der Aufgabenstellung und den erstellten Kriterien kann die KI eine detaillierte Musterl√∂sung generieren, die der Lehrkraft als Referenz f√ºr die Bewertung dient.
* **Generierung von Lernzielen:** Die KI kann basierend auf der Aufgabe passende, kompetenzorientierte Lernziele vorschlagen.

Diese Funktionen positionieren GUSTAV nicht nur als Feedback-Werkzeug, sondern als umfassenden digitalen Assistenten, der Lehrkr√§fte im gesamten Unterrichtszyklus unterst√ºtzt.


## E. Risiken und deren Mitigation

Die Implementierung eines KI-gest√ºtzten Feedback-Systems birgt inh√§rente Risiken, die proaktiv adressiert werden m√ºssen. Eine verantwortungsvolle Entwicklung erfordert eine umfassende Analyse potenzieller Fehlerquellen und die Implementierung einer mehrschichtigen Strategie aus technischen und prozessualen Gegenma√ünahmen.


### 1. Identifikation der gr√∂√üten Risiken



* **Sachlich falsches Feedback (Halluzinationen):** Das LLM generiert Fakten, Zitate oder Korrekturen, die plausibel klingen, aber sachlich falsch sind. Dies ist eines der bekanntesten Probleme von LLMs und kann den Lernprozess direkt untergraben.<sup>57</sup>
* **Inkonsistente Bewertungen:** Das System bewertet identische oder sehr √§hnliche Fehler bei verschiedenen Sch√ºlern oder zu unterschiedlichen Zeitpunkten inkonsistent. Dies untergr√§bt die Fairness und die Verl√§sslichkeit des Feedbacks.<sup>49</sup>
* **Zu generisches oder oberfl√§chliches Feedback:** Die KI gibt vage, nichtssagende R√ºckmeldungen (z.B. "Guter Ansatz!", "Das k√∂nntest du noch verbessern."), die dem Sch√ºler keine konkreten, handlungsorientierten Hinweise geben und somit den Kriterien von Hattie und Shute nicht entsprechen.<sup>60</sup>
* **Umgehung des Lernprozesses ("Gaming the System"):** Sch√ºler nutzen den interaktiven Modus nicht zum Lernen, sondern um durch schnelles Ausprobieren ("Trial and Error") iterativ die von der KI akzeptierte L√∂sung zu finden, ohne die zugrundeliegenden Konzepte zu verstehen.<sup>61</sup>
* **Bias und Fairness:** Das LLM reproduziert unbewusste Vorurteile (Bias) aus seinen Trainingsdaten. Es k√∂nnte beispielsweise bestimmte sprachliche Stile bevorzugen, die mit bestimmten sozio√∂konomischen oder kulturellen Hintergr√ºnden korrelieren, und so Sch√ºler unbeabsichtigt benachteiligen.<sup>57</sup>


### 2. Konkrete technische und prozessuale Gegenma√ünahmen

Eine effektive Risikominimierung erfordert eine Kombination aus pr√§ventiven Ma√ünahmen in der Architektur, detektiven Ma√ünahmen w√§hrend des Betriebs und transparenten, p√§dagogischen Rahmenbedingungen.

Mehrschichtige Verteidigung gegen Halluzinationen und sachliche Fehler:

Da Halluzinationen nicht vollst√§ndig eliminiert werden k√∂nnen, muss eine "Defense in Depth"-Strategie verfolgt werden.59



1. **Pr√§vention durch Grounding:** Die in Abschnitt C.1 beschriebene RAG-light-Architektur ist die erste und wichtigste Verteidigungslinie. Indem der Prompt mit relevanten Ausz√ºgen aus den Lernmaterialien "geerdet" (grounded) wird, wird die Wahrscheinlichkeit, dass das LLM Fakten erfindet, signifikant reduziert.
2. **Detektion durch Selbst-Verifikation:** Die Prompt-Kette kann um einen Verifikationsschritt erweitert werden. Nachdem das Feedback generiert wurde, wird das LLM in einem zweiten Aufruf instruiert: "√úberpr√ºfe jede Tatsachenbehauptung im folgenden Feedbacktext. Wenn eine Aussage nicht direkt durch die bereitgestellten Lernmaterialien oder die Musterl√∂sung gest√ºtzt wird, markiere sie als 'unsicher' oder formuliere sie als Frage um."
3. **Transparenz in der UI:** Jedes von der KI generierte Feedback muss unmissverst√§ndlich als solches gekennzeichnet sein. Ein permanenter, gut sichtbarer Disclaimer ist obligatorisch: "Dieses Feedback wurde von GUSTAV, einer KI, erstellt. Es dient als Anregung f√ºr deine √úberarbeitung. √úberpr√ºfe wichtige Fakten und sprich im Zweifel immer mit deiner Lehrkraft."
4. **Konfidenz-Scoring (fortgeschritten):** Viele LLM-APIs (auch √ºber Ollama) k√∂nnen die Log-Wahrscheinlichkeiten (logprobs) f√ºr die generierten Tokens ausgeben. Wenn das Modell eine Faktenaussage mit sehr niedriger kumulativer Wahrscheinlichkeit generiert, deutet dies auf eine hohe Unsicherheit hin. Die UI kann solche Passagen visuell hervorheben (z.B. durch eine gepunktete Unterstreichung), um den Sch√ºler zur Vorsicht zu mahnen.<sup>58</sup>

P√§dagogische und technische Ma√ünahmen gegen die Umgehung des Lernprozesses:

Das "Gaming" des Systems ist weniger ein technisches als ein p√§dagogisches Problem, das aber durch technisches Design beeinflusst werden kann.65



1. **Rate Limiting:** Die Anzahl der Feedback-Anfragen pro Sch√ºler und Zeiteinheit (z.B. maximal 3 Anfragen in 15 Minuten) kann technisch begrenzt werden. Dies verlangsamt den "Trial and Error"-Prozess und zwingt den Sch√ºler, √ºber das erhaltene Feedback nachzudenken, bevor er die n√§chste Anfrage stellt.
2. **Transparenz f√ºr die Lehrkraft:** Die in Abschnitt A.2 entworfene, versionszentrierte Datenbank speichert die gesamte Interaktionshistorie. Lehrkr√§fte m√ºssen die M√∂glichkeit haben, diesen Verlauf einzusehen, um zu erkennen, ob ein Sch√ºler konstruktiv mit dem System arbeitet oder es nur ausnutzt.
3. **F√∂rderung der Metakognition:** Das System kann aktiv zur Reflexion anregen. Nach der Einreichung einer √ºberarbeiteten Version k√∂nnte die KI eine metakognitive Frage stellen: "Beschreibe kurz, welche √Ñnderungen du auf Basis meines letzten Feedbacks vorgenommen hast und warum du denkst, dass dies eine Verbesserung ist." Die Antwort des Sch√ºlers wird ebenfalls gespeichert und ist f√ºr die Lehrkraft einsehbar. Dies verschiebt den Fokus von der reinen L√∂sungsfindung hin zur bewussten Auseinandersetzung mit dem Lernprozess.

Die folgende Matrix fasst die Risiken und die vorgeschlagenen, mehrschichtigen Gegenma√ünahmen systematisch zusammen.

**Tabelle E.1: Risiko-Mitigations-Matrix f√ºr die GUSTAV Feedback Engine**


<table>
  <tr>
   <td>Risiko
   </td>
   <td>Wahrscheinlichkeit
   </td>
   <td>Auswirkung
   </td>
   <td>Technische Mitigation
   </td>
   <td>P√§dagogische/Prozess-Mitigation
   </td>
  </tr>
  <tr>
   <td><strong>Sachliche Fehler (Halluzination)</strong>
   </td>
   <td>Mittel
   </td>
   <td>Hoch
   </td>
   <td>1. Grounding durch RAG-light. 2. Selbst-Verifikations-Schritt im Prompt. 3. Konfidenz-Scoring (logprobs) zur Kennzeichnung unsicherer Aussagen.
   </td>
   <td>1. Permanenter, klarer Disclaimer in der UI. 2. Schulung der Sch√ºler im kritischen Umgang mit KI-generierten Inhalten (AI Literacy). 3. M√∂glichkeit f√ºr Sch√ºler, fehlerhaftes Feedback zu melden.
   </td>
  </tr>
  <tr>
   <td><strong>Inkonsistente Bewertung</strong>
   </td>
   <td>Mittel
   </td>
   <td>Hoch
   </td>
   <td>1. Verwendung strukturierter Kriterien (JSON-Input). 2. Einsatz von dspy.ChainOfThought f√ºr einen nachvollziehbaren Analyseprozess. 3. Hohe Temperatur-Einstellungen (temperature=0.1) f√ºr deterministischere Ausgaben.
   </td>
   <td>1. Lehrkr√§fte sollten Stichproben durchf√ºhren. 2. M√∂glichkeit f√ºr Sch√ºler, eine manuelle √úberpr√ºfung durch die Lehrkraft anzufordern, wenn sie eine Bewertung f√ºr unfair halten.
   </td>
  </tr>
  <tr>
   <td><strong>Generisches Feedback</strong>
   </td>
   <td>Hoch (bei kleinen Modellen)
   </td>
   <td>Mittel
   </td>
   <td>1. Mehrstufiger Prompt (Analyse ‚Üí Feedback). 2. Sehr spezifische Anweisungen im Feedback-Prompt (Hattie-Fragen, positiver Einstieg, konkreter n√§chster Schritt). 3. DSPy-Optimierung mit "Gold-Standard"-Beispielen f√ºr spezifisches Feedback.
   </td>
   <td>1. Lehrkr√§fte sollten bei der Erstellung der Bewertungskriterien auf Spezifit√§t achten. 2. Gesammelte Beispiele f√ºr gutes/schlechtes Feedback zur kontinuierlichen Verbesserung des Systems nutzen.
   </td>
  </tr>
  <tr>
   <td><strong>Umgehung des Lernprozesses</strong>
   </td>
   <td>Hoch
   </td>
   <td>Hoch
   </td>
   <td>1. Rate Limiting f√ºr Feedback-Anfragen. 2. L√ºckenlose Protokollierung der gesamten Interaktions- und Versionshistorie. 3. Implementierung von metakognitiven Reflexionsfragen.
   </td>
   <td>1. Lehrkr√§ften Zugriff auf die Lernhistorie geben und sie schulen, diese zu interpretieren. 2. Aufgaben so gestalten, dass sie h√∂here Denkprozesse erfordern, die nicht leicht "erraten" werden k√∂nnen (z.B. durch pers√∂nliche Reflexionen).<sup>65</sup>
   </td>
  </tr>
  <tr>
   <td><strong>Bias & Fairness</strong>
   </td>
   <td>Mittel
   </td>
   <td>Hoch
   </td>
   <td>1. Verwendung von Prompts, die explizit zu neutraler, nicht-wertender Sprache anweisen. 2. Implementierung eines "Bias-Check"-Schrittes, bei dem die KI ihr eigenes Feedback auf potenziell voreingenommene Formulierungen √ºberpr√ºft.
   </td>
   <td>1. Regelm√§√üige Audits der Systemausgaben durch diverse Lehrkr√§fte. 2. Etablierung eines klaren Kanals, √ºber den Sch√ºler und Lehrkr√§fte als voreingenommen empfundenes Feedback melden k√∂nnen. 3. Transparente Kommunikation √ºber die Grenzen und potenziellen Biases von KI.
   </td>
  </tr>
</table>



## Schlussfolgerung: Ein pragmatischer Weg zu wirksamem KI-Feedback

Dieses Konzept skizziert eine Architektur f√ºr die GUSTAV Feedback Engine, die das p√§dagogisch W√ºnschenswerte mit dem technisch Machbaren in Einklang bringt. Der Kern des Ansatzes liegt in der Erkenntnis, dass kleine, lokale LLMs keine Allesk√∂nner sind, aber bei sorgf√§ltiger Orchestrierung zu hochwirksamen Werkzeugen f√ºr formatives Feedback werden k√∂nnen.

**Zusammenfassung der strategischen Empfehlungen:**



1. **Hybrides Feedback-Modell:** Implementierung eines von der Lehrkraft konfigurierbaren Modus f√ºr **einmaliges (statisches) oder interaktives Feedback**, um unterschiedlichen Aufgabentypen und Lernzielen gerecht zu werden.
2. **Zweistufige Feedback-Kontrolle:** Etablierung eines Kontrollsystems, bei dem die **Lehrkraft den Rahmen** der Feedback-Aspekte vorgibt und der **Sch√ºler den Detailgrad und Zeitpunkt** des Feedbacks w√§hlt, um kognitive √úberlastung zu vermeiden und die Lernautonomie zu f√∂rdern.
3. **Modulare Zwei-Schritt-KI-Architektur:** Aufbau einer robusten Pipeline in DSPy, die den Prozess in **Analyse und Feedback-Generierung** trennt. Diese Architektur muss auf hochstrukturierten Kontextdaten und einem pragmatischen **RAG-light-Ansatz** basieren, um die Leistung kleiner LLMs zu maximieren.
4. **Hybrides KI-Modell zur Lehrerentlastung:** Nutzung eines **optionalen, API-basierten Zugriffs auf leistungsst√§rkere LLMs ausschlie√ülich f√ºr Lehrkr√§fte** zur Unterst√ºtzung bei der Aufgabenerstellung (z.B. Rubriken), bei gleichzeitigem Schutz aller Sch√ºlerdaten durch rein lokale Verarbeitung.
5. **Mehrschichtige Risikomitigation:** Implementierung einer umfassenden Strategie zur Risikominimierung, die **pr√§ventive technische Ma√ünahmen** (Grounding, strukturierte Prompts), **detektive Mechanismen** (Selbst-Verifikation, Konfidenz-Scores) und **p√§dagogisch-prozessuale Rahmenbedingungen** (Transparenz, Metakognition, AI Literacy) kombiniert.

**Vorschlag f√ºr eine Implementierungs-Roadmap:**

Ein phasenweiser Ansatz erm√∂glicht es dem Entwicklungsteam, fr√ºhzeitig einen Mehrwert zu schaffen, aus der Praxis zu lernen und die Komplexit√§t schrittweise und kontrolliert zu steigern.



* **Phase 1 (Minimal Viable Product - MVP):**
    * Fokus auf den **einmaligen (statischen) Feedback-Modus**.
    * Implementierung der Kernarchitektur: Supabase-Schema (ohne komplexe Sessions), Zwei-Schritt-Prompting-Kette in DSPy mit dspy.Predict.
    * Umsetzung der lehrerseitigen Konfiguration f√ºr Feedback-Fokusbereiche.
    * Ziel: Schnelle Bereitstellung einer stabilen Basisfunktionalit√§t f√ºr einfache Aufgabentypen.
* **Phase 2 (Interaktiver Modus & Sch√ºler-Kontrolle):**
    * Einf√ºhrung des **interaktiven Dialog-Modus**.
    * Erweiterung des Datenbankschemas um feedback_sessions und feedback_messages.
    * Implementierung der sch√ºlerseitigen UI-Elemente zur Steuerung von Feedback-Aspekt und -Granularit√§t.
    * Ziel: Erm√∂glichung eines echten formativen Dialogs und St√§rkung der Sch√ºlerautonomie.
* **Phase 3 (Erweiterte Kontextualisierung & Lehrer-Assistenz):**
    * Entwicklung und Integration der **RAG-light-Pipeline** f√ºr den Umgang mit externen Lernmaterialien.
    * Implementierung der **optionalen, API-basierten Assistenzfunktionen** f√ºr Lehrkr√§fte (z.B. Rubrik-Generator).
    * Ziel: Maximierung der Feedback-Qualit√§t durch besseres Grounding und signifikante Entlastung der Lehrkr√§fte im Vorbereitungsprozess.
* **Laufender Prozess (Kontinuierliche Optimierung):**
    * Von Beginn an: Aufbau des **"Gold-Standard"-Datensatzes** durch Sammeln und Validieren exzellenter Feedback-Beispiele.
    * Nach Phase 2: Beginn der regelm√§√üigen Optimierungszyklen mit **DSPy-Telepromptern** (BootstrapFewShot), um die Prompt-Effektivit√§t kontinuierlich und algorithmisch zu verbessern.

Dieser gestufte Weg stellt sicher, dass GUSTAV auf einem soliden Fundament aufgebaut wird, das p√§dagogische Wirksamkeit, technische Stabilit√§t und verantwortungsvollen Umgang mit KI in den Mittelpunkt stellt.


#### Referenzen



1. Application of the Hattie and Timperley Power of Feedback Model with graduate teacher education students - Digital Commons @ USF - University of South Florida, Zugriff am Juli 25, 2025, [https://digitalcommons.usf.edu/cgi/viewcontent.cgi?article=1301&context=m3publishing](https://digitalcommons.usf.edu/cgi/viewcontent.cgi?article=1301&context=m3publishing)
2. Providing Educational Feedback - ScholarBlogs, Zugriff am Juli 25, 2025, [https://scholarblogs.emory.edu/digitalmatters/files/2019/08/ProvidingEducationalFeedback.pdf](https://scholarblogs.emory.edu/digitalmatters/files/2019/08/ProvidingEducationalFeedback.pdf)
3. A Matrix of Feedback for Learning - ERIC, Zugriff am Juli 25, 2025, [https://files.eric.ed.gov/fulltext/EJ1213749.pdf](https://files.eric.ed.gov/fulltext/EJ1213749.pdf)
4. Feedback in schools - Visible Learning, Zugriff am Juli 25, 2025, [https://www.visiblelearning.com/sites/default/files/Feedback%20article.pdf](https://www.visiblelearning.com/sites/default/files/Feedback%20article.pdf)
5. Evaluating the Sensitivity of LLMs to Prior Context - arXiv, Zugriff am Juli 25, 2025, [https://arxiv.org/html/2506.00069v1](https://arxiv.org/html/2506.00069v1)
6. I believe we're at a point where context is the main thing to improve on. - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/](https://www.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/)
7. What are your use cases for small (1-3-8B) models? : r/LocalLLaMA - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/](https://www.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/)
8. 5 Reasons Why Immediate Feedback is Important for Effective Learning - InteDashboard, Zugriff am Juli 25, 2025, [https://www.blog.intedashboard.com/blogs/tbl-learning/immediate-feedback](https://www.blog.intedashboard.com/blogs/tbl-learning/immediate-feedback)
9. Taking Education to the Next Level: The Benefits of Interactive Teaching - Kitaboo, Zugriff am Juli 25, 2025, [https://kitaboo.com/interactive-teaching/](https://kitaboo.com/interactive-teaching/)
10. Dynamic Learning v. Static Learning (DO THIS, NOT THAT) - Shake Up Learning, Zugriff am Juli 25, 2025, [https://shakeuplearning.com/blog/dynamic-learning-v-static-learning-not/](https://shakeuplearning.com/blog/dynamic-learning-v-static-learning-not/)
11. Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions - arXiv, Zugriff am Juli 25, 2025, [https://arxiv.org/html/2404.03429v1](https://arxiv.org/html/2404.03429v1)
12. Measuring actual learning versus feeling of learning in response to ..., Zugriff am Juli 25, 2025, [https://www.pnas.org/doi/10.1073/pnas.1821936116](https://www.pnas.org/doi/10.1073/pnas.1821936116)
13. Why Student Success Depends On Continuous Feedback - Harvard Business Publishing, Zugriff am Juli 25, 2025, [https://hbsp.harvard.edu/inspiring-minds/why-student-success-depends-on-continuous-feedback](https://hbsp.harvard.edu/inspiring-minds/why-student-success-depends-on-continuous-feedback)
14. Conversation history | Dialogflow CX - Google Cloud, Zugriff am Juli 25, 2025, [https://cloud.google.com/dialogflow/cx/docs/concept/conversation-history](https://cloud.google.com/dialogflow/cx/docs/concept/conversation-history)
15. (Part 2) Build a Conversational RAG with Mistral-7B and LangChain | by Madhav Thaker, Zugriff am Juli 25, 2025, [https://medium.com/@thakermadhav/part-2-build-a-conversational-rag-with-langchain-and-mistral-7b-6a4ebe497185](https://medium.com/@thakermadhav/part-2-build-a-conversational-rag-with-langchain-and-mistral-7b-6a4ebe497185)
16. Building Stateful Conversations with Postgres and LLMs | by Levi Stringer | Medium, Zugriff am Juli 25, 2025, [https://medium.com/@levi_stringer/building-stateful-conversations-with-postgres-and-llms-e6bb2a5ff73e](https://medium.com/@levi_stringer/building-stateful-conversations-with-postgres-and-llms-e6bb2a5ff73e)
17. Database Schema for Private Chat and Group Chat - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/Database/comments/wvrpc4/database_schema_for_private_chat_and_group_chat/](https://www.reddit.com/r/Database/comments/wvrpc4/database_schema_for_private_chat_and_group_chat/)
18. How to Design an LMS: Best Practices and Trends - Anyforsoft, Zugriff am Juli 25, 2025, [https://anyforsoft.com/blog/lms-design/](https://anyforsoft.com/blog/lms-design/)
19. LMS UI/UX Design: How to Build a Clear & Modern User Interface - Riseapps, Zugriff am Juli 25, 2025, [https://riseapps.co/lms-ui-ux-design/](https://riseapps.co/lms-ui-ux-design/)
20. Cognitive load theory in practice - Examples for the classroom - NSW Department of Education, Zugriff am Juli 25, 2025, [https://education.nsw.gov.au/content/dam/main-education/about-us/educational-data/cese/2017-cognitive-load-theory-practice-guide.pdf](https://education.nsw.gov.au/content/dam/main-education/about-us/educational-data/cese/2017-cognitive-load-theory-practice-guide.pdf)
21. The Application of Cognitive Load Theory to the Design of Health and Behavior Change Programs: Principles and Recommendations - PubMed Central, Zugriff am Juli 25, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12246501/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12246501/)
22. An introduction to cognitive load theory - The Education Hub, Zugriff am Juli 25, 2025, [https://theeducationhub.org.nz/an-introduction-to-cognitive-load-theory/](https://theeducationhub.org.nz/an-introduction-to-cognitive-load-theory/)
23. Cognitive Load Theory - The Decision Lab, Zugriff am Juli 25, 2025, [https://thedecisionlab.com/reference-guide/psychology/cognitive-load-theory](https://thedecisionlab.com/reference-guide/psychology/cognitive-load-theory)
24. How to use Cognitive Load Theory with students with SEND | InnerDrive, Zugriff am Juli 25, 2025, [https://www.innerdrive.co.uk/blog/cognitive-load-theory-send/](https://www.innerdrive.co.uk/blog/cognitive-load-theory-send/)
25. Six Strategies You May Not Be Using To Reduce Cognitive Load - The eLearning Coach, Zugriff am Juli 25, 2025, [https://theelearningcoach.com/learning/reduce-cognitive-load/](https://theelearningcoach.com/learning/reduce-cognitive-load/)
26. Complete Guide to Student-Centered vs. Teacher-Centered Learning - University of San Diego Online Degrees, Zugriff am Juli 25, 2025, [https://onlinedegrees.sandiego.edu/teacher-centered-vs-student-centered-learning/](https://onlinedegrees.sandiego.edu/teacher-centered-vs-student-centered-learning/)
27. What are the main features that differentiate between the pupil-centered teaching and the teacher-centered teaching? - Quora, Zugriff am Juli 25, 2025, [https://www.quora.com/What-are-the-main-features-that-differentiate-between-the-pupil-centered-teaching-and-the-teacher-centered-teaching](https://www.quora.com/What-are-the-main-features-that-differentiate-between-the-pupil-centered-teaching-and-the-teacher-centered-teaching)
28. Learning-Focused Feedback - Universally Designed, Zugriff am Juli 25, 2025, [https://universallydesigned.education/learning-focused-feedback/](https://universallydesigned.education/learning-focused-feedback/)
29. Teacher-Centered Versus Student-Centered Learning, Zugriff am Juli 25, 2025, [https://www.studentcenteredworld.com/teacher-centered-versus-student-centered/](https://www.studentcenteredworld.com/teacher-centered-versus-student-centered/)
30. Teacher-centered vs Student centered = A Tired & False Dichotomy : r/teaching - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/teaching/comments/19bsobd/teachercentered_vs_student_centered_a_tired_false/](https://www.reddit.com/r/teaching/comments/19bsobd/teachercentered_vs_student_centered_a_tired_false/)
31. 7 Tips To Reduce Cognitive Overload In eLearning, Zugriff am Juli 25, 2025, [https://elearningindustry.com/7-tips-reduce-cognitive-overload-elearning](https://elearningindustry.com/7-tips-reduce-cognitive-overload-elearning)
32. Teaching Young Students How to Overcome Cognitive Overload - Edutopia, Zugriff am Juli 25, 2025, [https://www.edutopia.org/article/cognitive-overload-elementary-school/](https://www.edutopia.org/article/cognitive-overload-elementary-school/)
33. Configure assignment methods and rules for queues - Learn Microsoft, Zugriff am Juli 25, 2025, [https://learn.microsoft.com/en-us/dynamics365/customer-service/administer/configure-assignment-rules](https://learn.microsoft.com/en-us/dynamics365/customer-service/administer/configure-assignment-rules)
34. Automated Feedback | Setting up in Assignment Review - FeedbackFruits, Zugriff am Juli 25, 2025, [https://help.feedbackfruits.com/hc/en-us/articles/23527132384658](https://help.feedbackfruits.com/hc/en-us/articles/23527132384658)
35. Assignment Settings 4.1 - CTL Faculty Support - MacEwan Help Centre, Zugriff am Juli 25, 2025, [https://helpcentre.macewan.ca/space/ETS/1813743184/Assignment+Settings+4.1](https://helpcentre.macewan.ca/space/ETS/1813743184/Assignment+Settings+4.1)
36. Real-Time Feedback Techniques for LLM Optimization - Ghost, Zugriff am Juli 25, 2025, [https://latitude-blog.ghost.io/blog/real-time-feedback-techniques-for-llm-optimization/](https://latitude-blog.ghost.io/blog/real-time-feedback-techniques-for-llm-optimization/)
37. Unveiling Context-Aware Criteria in Self-Assessing LLMs - arXiv, Zugriff am Juli 25, 2025, [https://arxiv.org/html/2410.21545v1](https://arxiv.org/html/2410.21545v1)
38. Local LLM Guide: RAG Implementation on Industrial Hardware | OnLogic, Zugriff am Juli 25, 2025, [https://www.onlogic.com/blog/local-llm-guide/](https://www.onlogic.com/blog/local-llm-guide/)
39. A light-weight no-cost implementation of web based Retrieval-Augmented Generation | by Anthony Demeusy | Medium, Zugriff am Juli 25, 2025, [https://medium.com/@anthony.demeusy/a-light-weight-no-cost-implementation-of-web-based-retrieval-augmented-generation-548a898ed313](https://medium.com/@anthony.demeusy/a-light-weight-no-cost-implementation-of-web-based-retrieval-augmented-generation-548a898ed313)
40. Based on your experience what is the smallest and optimal local model for RAG? - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/18q9xva/based_on_your_experience_what_is_the_smallest_and/](https://www.reddit.com/r/LocalLLaMA/comments/18q9xva/based_on_your_experience_what_is_the_smallest_and/)
41. RAG With Llama 3.1 8B, Ollama, and Langchain: Tutorial - DataCamp, Zugriff am Juli 25, 2025, [https://www.datacamp.com/tutorial/llama-3-1-rag](https://www.datacamp.com/tutorial/llama-3-1-rag)
42. Mastering Multi-Stage Prompt Structures: 5 Essential Tips | White Beard Strategies, Zugriff am Juli 25, 2025, [https://whitebeardstrategies.com/blog/mastering-multi-stage-prompt-structures-5-essential-tips/](https://whitebeardstrategies.com/blog/mastering-multi-stage-prompt-structures-5-essential-tips/)
43. How to Use Prompt Engineering Techniques for Deep Inquiry, Creative Mapping, and Strategic Insight with ChatGPT : r/ChatGPTPromptGenius - Reddit, Zugriff am Juli 25, 2025, [https://www.reddit.com/r/ChatGPTPromptGenius/comments/1k6naun/how_to_use_prompt_engineering_techniques_for_deep/](https://www.reddit.com/r/ChatGPTPromptGenius/comments/1k6naun/how_to_use_prompt_engineering_techniques_for_deep/)
44. LLMOps with DSPy: Build RAG Systems Using Declarative Programming - PyImageSearch, Zugriff am Juli 25, 2025, [https://pyimagesearch.com/2024/09/09/llmops-with-dspy-build-rag-systems-using-declarative-programming/](https://pyimagesearch.com/2024/09/09/llmops-with-dspy-build-rag-systems-using-declarative-programming/)
45. DSPy | Clio AI Deep Dive, Zugriff am Juli 25, 2025, [https://www.clioapp.ai/deep-dives/dspy](https://www.clioapp.ai/deep-dives/dspy)
46. Programming, Not Prompting: A Hands-On Guide to DSPy | Towards Data Science, Zugriff am Juli 25, 2025, [https://towardsdatascience.com/programming-not-prompting-a-hands-on-guide-to-dspy/](https://towardsdatascience.com/programming-not-prompting-a-hands-on-guide-to-dspy/)
47. An Exploratory Tour of DSPy: A Framework for Programing Language Models, not Prompting | by Jules S. Damji | The Modern Scientist | Medium, Zugriff am Juli 25, 2025, [https://medium.com/the-modern-scientist/an-exploratory-tour-of-dspy-a-framework-for-programing-language-models-not-prompting-711bc4a56376](https://medium.com/the-modern-scientist/an-exploratory-tour-of-dspy-a-framework-for-programing-language-models-not-prompting-711bc4a56376)
48. Easiest Tutorial to Learn DSPy with LLM Example - YouTube, Zugriff am Juli 25, 2025, [https://www.youtube.com/watch?v=Jfpxjg8xj9w](https://www.youtube.com/watch?v=Jfpxjg8xj9w)
49. Automated assignment grading with large language models: insights from a bioinformatics course - Oxford Academic, Zugriff am Juli 25, 2025, [https://academic.oup.com/bioinformatics/article/41/Supplement_1/i21/8199383](https://academic.oup.com/bioinformatics/article/41/Supplement_1/i21/8199383)
50. Grading Massive Open Online Courses Using Large Language Models - ACL Anthology, Zugriff am Juli 25, 2025, [https://aclanthology.org/2025.coling-main.263.pdf](https://aclanthology.org/2025.coling-main.263.pdf)
51. How Teachers Can Use AI in the Classroom for Lesson Planning, Zugriff am Juli 25, 2025, [https://www.maryvilleca2.com/post/how-teachers-can-use-ai-for-lesson-planning](https://www.maryvilleca2.com/post/how-teachers-can-use-ai-for-lesson-planning)
52. GDPR and Google Cloud, Zugriff am Juli 25, 2025, [https://cloud.google.com/privacy/gdpr](https://cloud.google.com/privacy/gdpr)
53. Large language models (LLM) | European Data Protection Supervisor, Zugriff am Juli 25, 2025, [https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/large-language-models-llm_en](https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/large-language-models-llm_en)
54. Data security and privacy precautions for Using Third-Party LLM APIs in Enterprise, Zugriff am Juli 25, 2025, [https://www.rohan-paul.com/p/data-security-and-privacy-precautions](https://www.rohan-paul.com/p/data-security-and-privacy-precautions)
55. TeachMateAI, Zugriff am Juli 25, 2025, [https://teachmateai.com/](https://teachmateai.com/)
56. Free, AI-powered teacher assistant by Khan Academy - Khanmigo, Zugriff am Juli 25, 2025, [https://www.khanmigo.ai/teachers](https://www.khanmigo.ai/teachers)
57. Risks of Generative Artificial Intelligence in Higher Education: A critical perspective - International Journal of Advances in Engineering and Management ( IJAEM ), Zugriff am Juli 25, 2025, [https://ijaem.net/issue_dcp/Risks%20of%20Generative%20Artificial%20Intelligence%20in%20Higher%20Education%20A%20critical%20perspective.pdf](https://ijaem.net/issue_dcp/Risks%20of%20Generative%20Artificial%20Intelligence%20in%20Higher%20Education%20A%20critical%20perspective.pdf)
58. LLM Hallucination Detection and Mitigation: Best Techniques - Deepchecks, Zugriff am Juli 25, 2025, [https://www.deepchecks.com/llm-hallucination-detection-and-mitigation-best-techniques/](https://www.deepchecks.com/llm-hallucination-detection-and-mitigation-best-techniques/)
59. The Beginner's Guide to Hallucinations in Large Language Models | Lakera ‚Äì Protecting AI teams that disrupt the world., Zugriff am Juli 25, 2025, [https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models)
60. Student Perspectives on the Benefits and Risks of AI in Education - arXiv, Zugriff am Juli 25, 2025, [https://arxiv.org/html/2505.02198v1](https://arxiv.org/html/2505.02198v1)
61. How to Prevent the Misuse of AI in Education - TAO Testing, Zugriff am Juli 25, 2025, [https://www.taotesting.com/blog/misuse-of-ai-in-education/](https://www.taotesting.com/blog/misuse-of-ai-in-education/)
62. Guide: How Professors Can Discourage and Prevent AI Misuse, Zugriff am Juli 25, 2025, [https://automatedteach.com/p/guide-professors-discourage-prevent-ai-misuse](https://automatedteach.com/p/guide-professors-discourage-prevent-ai-misuse)
63. Using AI to address common challenges in student feedback - SchoolAI, Zugriff am Juli 25, 2025, [https://schoolai.com/blog/using-ai-address-common-challenges-student-feedback](https://schoolai.com/blog/using-ai-address-common-challenges-student-feedback)
64. Hallucinations in LLMs: Can You Even Measure the Problem? - Medium, Zugriff am Juli 25, 2025, [https://medium.com/google-cloud/hallucination-detection-measurement-932e23b1873b](https://medium.com/google-cloud/hallucination-detection-measurement-932e23b1873b)
65. From Detection to Prevention: How to Discourage AI Misuse in Academia, Zugriff am Juli 25, 2025, [https://detecting-ai.com/blog/from-detection-to-prevention-how-to-discourage-ai-misuse-in-academia](https://detecting-ai.com/blog/from-detection-to-prevention-how-to-discourage-ai-misuse-in-academia)
66. How can I Revise my Assignments to Deter Student use of AI? | Office of Digital Learning | University of Nevada, Reno, Zugriff am Juli 25, 2025, [https://www.unr.edu/digital-learning/instructional-strategies/understanding-and-integrating-generative-ai-in-teaching/how-can-i-revise-my-assignments-to-deter-student-use-of-ai](https://www.unr.edu/digital-learning/instructional-strategies/understanding-and-integrating-generative-ai-in-teaching/how-can-i-revise-my-assignments-to-deter-student-use-of-ai)
