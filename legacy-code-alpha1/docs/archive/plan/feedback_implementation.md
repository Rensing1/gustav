# Implementierungskonzept: GUSTAVs KI-gestÃ¼tztes Feedback-System

## Stand: 2025-08-01

## Aktueller Implementierungsstatus

### âœ… Erfolgreich implementiert:

1. **Zweistufige "Atomare Analyse"-Pipeline**
   - Atomare Analyse pro Kriterium funktioniert (`processor.py`)
   - PÃ¤dagogische Synthese generiert Feed-Back und Feed-Forward getrennt
   - Robustes Template-basiertes Parsing statt JSON

2. **DSPy Signaturen**
   - `AnalyseSingleCriterion`: Analysiert ein Kriterium mit Template-Output
   - `GeneratePedagogicalFeedback`: Erzeugt strukturiertes pÃ¤dagogisches Feedback

3. **Datenbankstruktur**
   - Migration 20250801123332 erfolgreich durchgefÃ¼hrt
   - `feedback_focus` aufgeteilt in `assessment_criteria` (JSONB Array) und `solution_hints` (TEXT)
   - Neue Spalten `feed_back_text` und `feed_forward_text` in submission Tabelle

4. **Service-Integration**
   - `service.py` nutzt die neue atomare Pipeline
   - Fehlerbehandlung und Logging implementiert
   - AbwÃ¤rtskompatibilitÃ¤t durch kombiniertes Feedback gewÃ¤hrleistet

5. **UI-Integration** âœ…
   - Teacher-UI: Eingabe von bis zu 5 Bewertungskriterien als separate Felder (`detail_editor.py`)
   - Teacher-UI: Eingabefeld fÃ¼r LÃ¶sungshinweise implementiert
   - Student-UI: Getrennte Anzeige von Feed-Back ("Wo du stehst") und Feed-Forward ("Dein nÃ¤chster Schritt")
   - Live-Unterricht View: Vorschau und Bearbeitung des strukturierten Feedbacks
   - Fallback fÃ¼r altes Feedback-Format gewÃ¤hrleistet

### ðŸš§ TODO / NÃ¤chste Schritte:

1. **Prompt-Optimierung**
   - Few-Shot-Beispiele fÃ¼r bessere Feedback-QualitÃ¤t
   - Persona-Anpassung (Klassenstufe aus Profil)
   - Feedback-Historie fÃ¼r Mehrfachabgaben implementieren

2. **Performance**
   - Parallelisierung der atomaren Analysen
   - Caching fÃ¼r identische Kriterien
   - Progress-Anzeige wÃ¤hrend Analyse

3. **Erweiterte Features**
   - Mehrfachabgaben mit Feedback-Historie
   - Gewichtung von Bewertungskriterien
   - Spezifische Prompts fÃ¼r unterschiedliche Aufgabentypen

## 1. Zielsetzung und Leitprinzipien

Dieses Dokument beschreibt die technische und konzeptionelle Architektur fÃ¼r die KI-gestÃ¼tzte Feedback-Engine der Lernplattform. Das Ziel ist die Entwicklung eines robusten, skalierbaren und pÃ¤dagogisch wertvollen Systems, das in der Lage ist, SchÃ¼lern formatives Feedback zu geben.

Die Implementierung folgt zwei zentralen Leitprinzipien:

1.  **PÃ¤dagogische Fundierung:** Das generierte Feedback muss den in `feedback_science.md` dargelegten wissenschaftlichen Kriterien genÃ¼gen. Im Fokus stehen aufgabenbezogenes Feed-Back und Feed-Forward in einer unterstÃ¼tzenden, nicht-wertenden TonalitÃ¤t.
2.  **Technische Robustheit:** Die Architektur muss den EinschrÃ¤nkungen eines lokal betriebenen 8b-Sprachmodells Rechnung tragen. Im Mittelpunkt stehen ZuverlÃ¤ssigkeit, Steuerbarkeit und Effizienz, umgesetzt mit dem DSPy-Framework.

Es ist zu beachten, dass der SchÃ¼ler seine Aufgaben gegebenfalls mehrfach abgeben kann. Der Lehrer bestimmt dies bei der Erstellung von Aufgaben (Standard: max. 1 Abgabe).

## 2. Architektur: Die zweistufige "Atomare Analyse"-Pipeline

Um die KomplexitÃ¤t fÃ¼r das 8b-Modell zu reduzieren und die ZuverlÃ¤ssigkeit zu maximieren, wird eine zweistufige Pipeline implementiert. Anstatt eines einzigen, komplexen LLM-Aufrufs, der alles auf einmal erledigen soll, zerlegen wir den Prozess in logische, voneinander getrennte Schritte.

### 2.1. Grundprinzip

Die Kernidee ist, die **objektive Analyse** der SchÃ¼lerlÃ¶sung von der **pÃ¤dagogischen Formulierung** des Feedbacks zu trennen. Wir vermeiden das fehleranfÃ¤llige Generieren eines einzigen, groÃŸen JSON-Objekts, indem wir die Analyse in atomare Einheiten zerlegen.

### 2.2. Schritt 1: Der "Analytiker" (Atomare Analyse pro Kriterium)

In diesem Schritt wird die SchÃ¼lerlÃ¶sung nicht als Ganzes, sondern Kriterium fÃ¼r Kriterium analysiert. Dies geschieht in einer Schleife in der Anwendungslogik. FÃ¼r jedes vom Lehrer definierte Bewertungskriterium wird ein fokussierter LLM-Aufruf gestartet.

*   **Aufgabe:** Bewerte die SchÃ¼lerlÃ¶sung im Hinblick auf *ein einziges, spezifisches Kriterium*.
*   **Kontext:** ErhÃ¤lt die Aufgabenstellung, die SchÃ¼lerlÃ¶sung, die LÃ¶sungshinweise und das eine zu prÃ¼fende Kriterium.
*   **Output:** Ein sehr kleines, einfach strukturiertes JSON-Objekt, das nur die Analyse fÃ¼r dieses eine Kriterium enthÃ¤lt.

### 2.3. Schritt 2: Der "PÃ¤dagoge" (Synthese des Feedbacks)

Nachdem die Analyse-Schleife durchgelaufen ist, werden die einzelnen Analyse-JSONs zu einem Gesamt-Analyseobjekt zusammengefÃ¼gt. Dieses strukturierte Objekt wird dann an den zweiten, pÃ¤dagogischen Schritt Ã¼bergeben.

*   **Aufgabe:** Formuliere aus der vollstÃ¤ndigen, strukturierten Analyse ein kohÃ¤rentes, pÃ¤dagogisch wertvolles Feedback.
*   **Kontext:** ErhÃ¤lt das Gesamt-Analyseobjekt und die optionale Feedback-Historie.
*   **Output:** Zwei separate, aber zusammenhÃ¤ngende Textteile: ein **Feed-Back** und ein **Feed-Forward**.

### 2.4. Visuelles Flussdiagramm

```
[START]
   |
   V
[Inputs: Aufgabe, LÃ¶sung, Kriterien (Liste), LÃ¶sungshinweise, Historie]
   |
   V
/-------------------------------------\
|  Analyse-Schleife (Schritt 1)       |
|                                     |
|  FOR each `kriterium` in `Kriterien`: |
|     |                               |
|     V                               |
|  [LLM-Aufruf: AnalyseSingleCriterion]----> Input: (Aufgabe, LÃ¶sung, kriterium, LÃ¶sungshinweise)
|     |                               |
|     V                               |
|  [Output: Kleines Analyse-JSON]     |
|     |                               |
|  <--- Sammle JSONs in `final_analysis` |
|                                     |
\-------------------------------------/
   |
   V
[Input fÃ¼r Schritt 2: `final_analysis`, `Historie`]
   |
   V
[LLM-Aufruf: GeneratePedagogicalFeedback]
   |
   V
[Output: `feed_back_text`, `feed_forward_text`]
   |
   V
[ENDE]
```

## 3. Detail-Spezifikation der DSPy-Komponenten

Die Architektur wird durch zwei klar definierte DSPy-Signaturen umgesetzt.

### 3.1. Signatur fÃ¼r Schritt 1: `AnalyseSingleCriterion`

Diese Signatur ist das Arbeitspferd der Analyse-Schleife. Sie ist bewusst schlank und fokussiert gehalten.

```python
import dspy

class AnalyseSingleCriterion(dspy.Signature):
    """Analysiert die SchÃ¼lerlÃ¶sung im Hinblick auf EIN spezifisches Kriterium."""

    task_description = dspy.InputField(desc="Die von der Lehrkraft gestellte Aufgabe.")
    student_solution = dspy.InputField(desc="Die vom SchÃ¼ler eingereichte LÃ¶sung.")
    solution_hints = dspy.InputField(desc="Die von der Lehrkraft bereitgestellte MusterlÃ¶sung oder Hinweise zur sachlichen Korrektheit.")
    criterion_to_check = dspy.InputField(desc="Das eine Kriterium, das jetzt geprÃ¼ft werden soll.")

    single_analysis_text = dspy.OutputField(
        desc="""Strukturierte Antwort im folgenden Format (GENAU SO, mit GroÃŸbuchstaben fÃ¼r die Labels):
STATUS: [WÃ¤hle EINES: erfÃ¼llt / nicht erfÃ¼llt / teilweise erfÃ¼llt]
ZITAT: "[Kopiere ein wÃ¶rtliches Zitat aus der SchÃ¼lerlÃ¶sung]"
ANALYSE: [Schreibe eine kurze, objektive BegrÃ¼ndung]"""
    )
```

**BegrÃ¼ndung:**
*   `solution_hints` wird hier benÃ¶tigt, damit die sachliche Korrektheit direkt bei der Analyse geprÃ¼ft werden kann.
*   **Update 2025-08-01**: Da viele LLMs (insbesondere gemma3:12b) Schwierigkeiten mit der Generierung von validem JSON haben, wurde auf ein strukturiertes Text-Template umgestellt. Dies erhÃ¶ht die Robustheit erheblich.

### 3.2. Signatur fÃ¼r Schritt 2: `GeneratePedagogicalFeedback`

Diese Signatur ist fÃ¼r die Kommunikation mit dem SchÃ¼ler zustÃ¤ndig. Ihre wichtigste Eigenschaft ist die Aufteilung des Outputs in zwei separate Felder, um die pÃ¤dagogische Struktur zu garantieren.

```python
class GeneratePedagogicalFeedback(dspy.Signature):
    """Formuliert auf Basis einer strukturierten Analyse ein pÃ¤dagogisch wertvolles Feedback."""

    analysis_json = dspy.InputField(desc="Das zusammengefasste JSON-Objekt mit der Analyse aller Kriterien.")
    student_persona = dspy.InputField(desc="Informationen zum SchÃ¼ler, z.B. '8. Klasse', um den Ton anzupassen.")
    feedback_history = dspy.InputField(
        desc="Der Verlauf der bisherigen Feedback-Runden fÃ¼r diese Aufgabe.",
        required=False
    )

    feed_back_text = dspy.OutputField(desc="Der Teil des Feedbacks, der den Ist-Zustand beschreibt (Wo stehe ich?), beginnend mit einem positiven Einstieg.")
    feed_forward_text = dspy.OutputField(desc="Der Teil des Feedbacks, der einen konkreten, umsetzbaren nÃ¤chsten Schritt vorschlÃ¤gt (Wo geht es als NÃ¤chstes hin?).")
```

**BegrÃ¼ndung:**
*   Die Trennung in `feed_back_text` und `feed_forward_text` ist eine entscheidende MaÃŸnahme zur QualitÃ¤tssicherung. Sie **zwingt** das LLM, beide fÃ¼r effektives Feedback notwendigen Komponenten zu generieren.
*   Diese Struktur ermÃ¶glicht es dem Frontend, die beiden Teile des Feedbacks unterschiedlich darzustellen (z.B. den Feed-Forward als hervorgehobene "NÃ¤chster Schritt"-Box), was die VerstÃ¤ndlichkeit und Handlungsorientierung fÃ¼r den SchÃ¼ler erhÃ¶ht.

## 4. Der Orchestrierungs-Prozess (Anwendungslogik)

Der folgende Pseudo-Code skizziert, wie die DSPy-Module in der Anwendungslogik gesteuert werden.

```python
# 4.1. Vorbereitung
# Inputs aus dem System laden: task, solution, teacher_criteria (Liste), hints, history
atomic_analyzer = dspy.Predict(AnalyseSingleCriterion)
feedback_synthesizer = dspy.Predict(GeneratePedagogicalFeedback)
final_analysis_obj = {"strengths": [], "weaknesses": []}

# 4.2. Die Analyse-Schleife
for criterion in teacher_criteria:
    try:
        # FÃ¼hre fÃ¼r jedes Kriterium einen fokussierten LLM-Aufruf durch
        result = atomic_analyzer(
            task_description=task,
            student_solution=solution,
            solution_hints=hints,
            criterion_to_check=criterion
        )
        # Parse die strukturierte Text-Antwort
        analysis_data = parse_template_response(result.single_analysis_text)
        analysis_data['criterion'] = criterion # FÃ¼ge das Kriterium fÃ¼r den Kontext hinzu

        # Sortiere das Ergebnis in die finale Struktur ein
        if analysis_data['status'] == 'erfÃ¼llt':
            final_analysis_obj["strengths"].append(analysis_data)
        else:
            final_analysis_obj["weaknesses"].append(analysis_data)
            
    except Exception as e:
        # Robuste Fehlerbehandlung fÃ¼r den Fall, dass ein einzelner Aufruf fehlschlÃ¤gt
        print(f"Fehler bei der Analyse des Kriteriums '{criterion}': {e}")

# 4.3. Die Synthese
# Stelle sicher, dass das analysis_json nicht leer ist
if final_analysis_obj["strengths"] or final_analysis_obj["weaknesses"]:
    final_feedback = feedback_synthesizer(
        analysis_json=json.dumps(final_analysis_obj),
        student_persona="SchÃ¼ler/in der 9. Klasse",
        feedback_history=history
    )
    # Gib die beiden separaten Textteile an das Frontend weiter
    # z.B. display_feedback(final_feedback.feed_back_text, final_feedback.feed_forward_text)
else:
    # Fallback, falls die gesamte Analyse fehlschlÃ¤gt
    print("Es konnte leider kein automatisches Feedback generiert werden.")
```

## 5. BegrÃ¼ndung zentraler Entscheidungen und Alternativen

| Thema | Unsere Entscheidung & BegrÃ¼ndung | Betrachtete Alternativen & Warum verworfen |
| :--- | :--- | :--- |
| **Struktur der Analyse** | **Atomare Analyse-Schleife:** Jeder LLM-Aufruf im ersten Schritt erzeugt nur ein winziges, flaches JSON pro Kriterium. **BegrÃ¼ndung:** Maximale ZuverlÃ¤ssigkeit und drastisch reduzierte FehleranfÃ¤lligkeit bei der JSON-Generierung durch ein kleines 8b-Modell. | **Ein groÃŸer JSON-Blob:** Ein einziger LLM-Aufruf generiert ein komplexes, verschachteltes JSON. **Verworfen weil:** Zu fehleranfÃ¤llig fÃ¼r ein 8b-Modell. Ein Syntaxfehler macht das gesamte Ergebnis unbrauchbar. |
| **Struktur des Feedbacks** | **Separate Felder fÃ¼r Feed-Back & Feed-Forward:** Der "PÃ¤dagoge" generiert zwei getrennte Text-Outputs. **BegrÃ¼ndung:** Garantiert die VollstÃ¤ndigkeit des Feedbacks und ermÃ¶glicht eine flexiblere, klarere Darstellung im Frontend. | **Ein einzelner Textblock:** Das LLM formuliert einen einzigen, kohÃ¤renten Text. **Verworfen weil:** Geringere ZuverlÃ¤ssigkeit (Gefahr, dass der Feed-Forward vergessen wird) und starre DarstellungsmÃ¶glichkeiten in der UI. |
| **Kontext-Bereitstellung (Lehrer-UI)** | **Strukturierte Eingabefelder:** Die UI bietet separate Felder fÃ¼r "Bewertungskriterien" und "LÃ¶sungshinweise". **BegrÃ¼ndung:** Erzwingt klare, strukturierte Eingaben, was zu einem sauberen, "rauschfreien" Prompt fÃ¼r die KI fÃ¼hrt â€“ Ã¼berlebenswichtig fÃ¼r ein kleines Modell. | **Eine einzige "Magic Textbox":** Ein groÃŸes Textfeld fÃ¼r alle Anweisungen. **Verworfen weil:** FÃ¼hrt zu unstrukturierten, mehrdeutigen Prompts, die die Leistung und ZuverlÃ¤ssigkeit der KI stark beeintrÃ¤chtigen. |
| **Umgang mit Aufgabentypen** | **Universeller, datengesteuerter Ansatz:** Eine einzige, agnostische Pipeline, deren Verhalten durch die vom Lehrer gelieferten `evaluation_criteria` gesteuert wird. **BegrÃ¼ndung:** Extrem wartungs- und skalierbar. | **Spezialisierte Prompts/Signaturen:** FÃ¼r jeden Aufgabentyp eine eigene Signatur. **Verworfen weil:** Hoher Entwicklungs- und Wartungsaufwand. |
| **DialogfÃ¤higkeit (Historie)** | **Historie als Input fÃ¼r den "PÃ¤dagogen" (Schritt 2):** Die Historie wird nur zur Formulierung des Feedbacks genutzt, nicht zur Analyse. **BegrÃ¼ndung:** GewÃ¤hrleistet eine objektive Analyse der *aktuellen* LÃ¶sung in Schritt 1. | **Historie als Input fÃ¼r den "Analytiker" (Schritt 1):** Die KI analysiert die neue LÃ¶sung im Licht der alten. **Verworfen weil:** Gefahr der "Voreingenommenheit" bei der Analyse. |

## 6. Prompt-Design: Die Einbettung der PÃ¤dagogik

Die Einhaltung der pÃ¤dagogischen Kriterien wird durch ein striktes Regelwerk im Prompt des "PÃ¤dagogen" (`GeneratePedagogicalFeedback`) sichergestellt.

**Auszug aus dem Kern-Prompt fÃ¼r `GeneratePedagogicalFeedback`:**

```
Du bist GUSTAV, ein sachlicher und unterstÃ¼tzender Lern-Coach.

### Absolute Regeln:
1.  **SpezifitÃ¤t:** Beziehe dich IMMER auf konkrete Zitate aus der Analyse.
2.  **Keine Personenbewertung:** Bewerte NIEMALS die Person. Beziehe dich IMMER auf den Text.
3.  **Keine Prozessbewertung:** Kommentiere NIEMALS den Lernprozess.
4.  **Keine LÃ¶sungen:** Gib NIEMALS die LÃ¶sung direkt vor.

### Deine Aufgabe:
Basierend auf der folgenden Analyse, fÃ¼lle die beiden Felder `feed_back_text` und `feed_forward_text` aus. Wenn eine `feedback_history` vorhanden ist, erkenne Fortschritte an.

### Analyse:
{{analysis_json}}
{{feedback_history}}

---
### FELD 1: `feed_back_text`
Beginne IMMER mit einer spezifischen, positiven Beobachtung. Beschreibe dann klar und wertfrei den wichtigsten Verbesserungspunkt.
Beispiel: "Super, ich sehe, du hast den Hinweis zur Einleitung umgesetzt! Sie ist jetzt viel prÃ¤gnanter. Mir ist bei der Analyse deines Arguments aufgefallen, dass an der Stelle '...' noch ein Beleg fehlt, um es vollstÃ¤ndig zu untermauern."

### FELD 2: `feed_forward_text`
Formuliere EINEN klaren, umsetzbaren Tipp oder stelle EINE gezielte Frage, die dem SchÃ¼ler hilft, genau den im `feed_back_text` genannten Punkt zu verbessern. SchlieÃŸe mit einer Ermutigung.
Beispiel: "Welche Textstelle kÃ¶nntest du zitieren, um deine Behauptung zu untermauern? Ich bin gespannt auf deine nÃ¤chste Version!"
```

## 7. Performance-Betrachtung und Optimierungspotenziale

Die gewÃ¤hlte "Atomare Analyse"-Architektur priorisiert ZuverlÃ¤ssigkeit und Robustheit Ã¼ber rohe Geschwindigkeit. Statt eines groÃŸen LLM-Aufrufs werden N+1 kleinere Aufrufe getÃ¤tigt (N = Anzahl der Kriterien). Dies fÃ¼hrt zu einem erhÃ¶hten Rechenaufwand, da der Kontext (SchÃ¼lerlÃ¶sung, Aufgabenstellung) mehrfach verarbeitet wird.

**AbwÃ¤gung:** Dieser hÃ¶here Aufwand ist ein bewusster Kompromiss. Ein System, das zuverlÃ¤ssig in 99% der FÃ¤lle ein Ergebnis liefert, ist fÃ¼r den Bildungskontext wertvoller als ein schnelleres System, das aufgrund von Syntaxfehlern hÃ¤ufiger versagt.

**MaÃŸnahmen zur Performance-Optimierung (SpÃ¤tere Umsetzung):**

Die folgenden MaÃŸnahmen kÃ¶nnen in spÃ¤teren Entwicklungsphasen implementiert werden, um die Latenz zu reduzieren, ohne die Robustheit zu opfern:

*   **Asynchrone/Parallele AusfÃ¼hrung:** Die N Analyse-Aufrufe in der Schleife sind voneinander unabhÃ¤ngig. Sie kÃ¶nnen parallelisiert werden, sodass die Gesamtlatenz der Analysephase sich an der des langsamsten Einzelaufrufs orientiert, nicht an der Summe aller Aufrufe. Dies ist die wichtigste OptimierungsmaÃŸnahme.
*   **Intelligentes UI/UX-Design:** WÃ¤hrend die Analyse lÃ¤uft, kann dem Nutzer der Fortschritt angezeigt werden ("Analysiere Kriterium 2 von 5..."). Dies verbessert die wahrgenommene Geschwindigkeit und macht das Warten transparenter.
*   **Caching:** Ergebnisse von `AnalyseSingleCriterion` fÃ¼r eine identische Kombination aus SchÃ¼lerlÃ¶sung und Kriterium kÃ¶nnen zwischengespeichert werden, um wiederholte Berechnungen zu vermeiden.

## 8. Fazit

Dieses Implementierungskonzept skizziert eine robuste und pÃ¤dagogisch fundierte Architektur. Durch die **atomare Analyse** wird das Problem der ZuverlÃ¤ssigkeit kleiner LLMs adressiert. Durch die **Trennung von Feed-Back und Feed-Forward in separate Outputs** und ein **regelbasiertes Prompt-Design** wird sichergestellt, dass das generierte Feedback den wissenschaftlichen Kriterien entspricht und im Frontend optimal dargestellt werden kann. Die nÃ¤chsten Schritte umfassen die konkrete Implementierung der DSPy-Module und den Aufbau eines "Gold-Standard"-Datensatzes, um das System kontinuierlich zu optimieren.

## 9. Template-basiertes Parsing (Update 2025-08-01)

### 9.1. Problemstellung

In der Praxis zeigte sich, dass viele lokale LLMs (insbesondere gemma3:12b) erhebliche Schwierigkeiten haben, konsistent valides JSON zu generieren. Dies fÃ¼hrte dazu, dass die atomare Analyse regelmÃ¤ÃŸig fehlschlug, obwohl das LLM die Aufgabe inhaltlich verstanden hatte.

### 9.2. LÃ¶sung: Strukturierte Text-Templates

Anstatt JSON zu verlangen, nutzen wir nun ein einfaches, fÃ¼r Menschen und Maschinen lesbares Template-Format:

```
STATUS: erfÃ¼llt
ZITAT: "Die deutsche Verfassung (Grundgesetz) hat ein besonderes Gesetz, das sogenannte Â§21."
ANALYSE: Der SchÃ¼ler nennt korrekt den relevanten Paragraphen des Grundgesetzes.
```

### 9.3. Template-Parser

Der Parser verwendet robuste Regex-Patterns, um die drei Felder zu extrahieren:
- **STATUS**: Sucht nach dem Label und einem der drei erlaubten Werte
- **ZITAT**: Extrahiert Text zwischen AnfÃ¼hrungszeichen (mit Fallbacks)
- **ANALYSE**: Nimmt allen Text nach dem Label bis zum Ende oder nÃ¤chsten Label

### 9.4. Vorteile

1. **Robustheit**: Funktioniert zuverlÃ¤ssig mit allen LLMs
2. **Transparenz**: Einfach zu debuggen und zu verstehen
3. **FlexibilitÃ¤t**: Teilweise Ergebnisse sind mÃ¶glich (z.B. nur Status)
4. **Wartbarkeit**: Parser kann leicht angepasst werden

### 9.5. ZukÃ¼nftige Erweiterung: DSPy TypedPredictor

Als zukÃ¼nftige Alternative kÃ¶nnte DSPy's TypedPredictor mit Pydantic-Modellen evaluiert werden:

```python
from pydantic import BaseModel
from dspy.functional import TypedPredictor

class CriterionAnalysis(BaseModel):
    status: Literal["erfÃ¼llt", "nicht erfÃ¼llt", "teilweise erfÃ¼llt"]
    quote: str
    analysis: str

# WÃ¼rde automatisch verschiedene Parsing-Strategien versuchen
analyzer = TypedPredictor(output_type=CriterionAnalysis)
```

Diese Option bleibt als Fallback fÃ¼r spÃ¤tere Iterationen, wenn sich die LLM-FÃ¤higkeiten verbessern.

## 10. SpÃ¤tere Ideen
- Spezifische Prompts fÃ¼r unterschiedliche Aufgabentypen
- MÃ¶glichkeit, Feed-Up generieren zu lassen
- Feed-Back und Feed-Forward in verschiedenfarbigen Boxen darstellen
- Optimierung in DSPy
- Gewichtung von Bewertungskriterien (z.B. Hauptkriterium 40%, Nebenkriterien je 20%)
- Migration zu TypedPredictor wenn LLMs besser werden
